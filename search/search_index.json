{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b # \u672c\u5de5\u7a0b\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a docs\uff1a\u63cf\u8ff0 data structure \u7684\u7406\u8bba\u77e5\u8bc6\uff0c\u8fd9\u90e8\u5206\u7684\u76ee\u5f55\u7ec4\u7ec7\u7ed3\u6784\u662f\u57fa\u4e8e Wikipedia Data structures \u7684\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u6211\u7684\u5b9e\u8df5\uff0c\u8fdb\u884c\u4e86\u4e00\u5b9a\u7684\u6269\u5145\u3002 \u9996\u5148\u4ece Structure \u8c08\u8d77\uff0c\u7136\u540e\u8ba8\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684 data structure \uff1b \u7136\u540e\u8ba8\u8bba\u5404\u79cd\u5177\u4f53\u7684data structure\uff0c\u5982 tree \uff0c graph \u7b49\uff0c\u5bf9\u4e8e\u6bcf\u79cddata structure\uff0c\u4f1a\u5bf9\u5b83\u7684\u5404\u65b9\u9762\uff08\u63cf\u8ff0\u3001\u8868\u793a\u3001\u5b9e\u73b0\u3001\u64cd\u4f5c\uff09\u8fdb\u884c\u603b\u7ed3\u3002 \u6700\u540e\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4\u4ee5\u7a81\u51fa\u5176\u7279\u6027\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002 src\uff1a\u63d0\u4f9b\u5404\u79cddata structure\u7684\u5b9e\u73b0\uff0c\u8fd9\u90e8\u5206\u7684\u76ee\u5f55\u7ed3\u6784\u548cdocs\u7684\u76ee\u5f55\u7ed3\u6784\u4e4b\u95f4\u57fa\u672c\u4e0a\u662f\u4e00\u4e00\u5bf9\u5e94\uff0c\u5bf9\u4e8e\u6bcf\u79cd\u7ed3\u6784\uff0c\u5c3d\u53ef\u80fd\u5730\u63d0\u4f9bpython\u3001 c++ \u3001c\u5b9e\u73b0\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u4e2a\u5de5\u7a0b\u4f5c\u4e3a data structure \u7684\u77e5\u8bc6\u5e93\uff0c\u63d0\u4f9b\u6587\u6863\u4e0e\u5b9e\u73b0\u3002","title":"Home"},{"location":"#_1","text":"\u672c\u5de5\u7a0b\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a docs\uff1a\u63cf\u8ff0 data structure \u7684\u7406\u8bba\u77e5\u8bc6\uff0c\u8fd9\u90e8\u5206\u7684\u76ee\u5f55\u7ec4\u7ec7\u7ed3\u6784\u662f\u57fa\u4e8e Wikipedia Data structures \u7684\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u6211\u7684\u5b9e\u8df5\uff0c\u8fdb\u884c\u4e86\u4e00\u5b9a\u7684\u6269\u5145\u3002 \u9996\u5148\u4ece Structure \u8c08\u8d77\uff0c\u7136\u540e\u8ba8\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684 data structure \uff1b \u7136\u540e\u8ba8\u8bba\u5404\u79cd\u5177\u4f53\u7684data structure\uff0c\u5982 tree \uff0c graph \u7b49\uff0c\u5bf9\u4e8e\u6bcf\u79cddata structure\uff0c\u4f1a\u5bf9\u5b83\u7684\u5404\u65b9\u9762\uff08\u63cf\u8ff0\u3001\u8868\u793a\u3001\u5b9e\u73b0\u3001\u64cd\u4f5c\uff09\u8fdb\u884c\u603b\u7ed3\u3002 \u6700\u540e\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4\u4ee5\u7a81\u51fa\u5176\u7279\u6027\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002 src\uff1a\u63d0\u4f9b\u5404\u79cddata structure\u7684\u5b9e\u73b0\uff0c\u8fd9\u90e8\u5206\u7684\u76ee\u5f55\u7ed3\u6784\u548cdocs\u7684\u76ee\u5f55\u7ed3\u6784\u4e4b\u95f4\u57fa\u672c\u4e0a\u662f\u4e00\u4e00\u5bf9\u5e94\uff0c\u5bf9\u4e8e\u6bcf\u79cd\u7ed3\u6784\uff0c\u5c3d\u53ef\u80fd\u5730\u63d0\u4f9bpython\u3001 c++ \u3001c\u5b9e\u73b0\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u4e2a\u5de5\u7a0b\u4f5c\u4e3a data structure \u7684\u77e5\u8bc6\u5e93\uff0c\u63d0\u4f9b\u6587\u6863\u4e0e\u5b9e\u73b0\u3002","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"TODO/","text":"TODO # algorithms use memory to gain performance improvement # \u4f7f\u7528\u7a7a\u95f4\u6362\u53d6\u65f6\u95f4 https://www.sciencedirect.com/topics/computer-science/performance-gain https://stackoverflow.com/questions/1898161/memory-vs-performance lintcode # https://www.zhihu.com/question/31218682 hash # Hash table \u76f8\u5173\uff1ahttps://www.infoq.com/articles/redis-time-series Hash functions Hash list What exactly (and precisely) is \u201chash?\u201d AVL tree # \u76f8\u5173\uff1ahttps://opensourceforu.com/2016/03/the-life-of-a-process/ https://en.wikipedia.org/wiki/AVL_tree Bit field # https://en.wikipedia.org/wiki/Bit_field \u5173\u4e8ebit field\uff0c \u5728Unix OS\u4e2d\u4f7f\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u6211\u4e4b\u524d\u5df2\u7ecf\u9047\u5230\u4e86\u975e\u5e38\u591a\uff0c\u53ea\u662f\u6ca1\u6709\u610f\u8bc6\u5230\u5b83\u4eec\u5c31\u662fbit field\uff1b\u4eca\u5929\uff0820190518\uff09\u5728\u9605\u8bfb Unix file types \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 Internally, ls obtains the stat structure[ 2] associated with the file and transforms the mode_t field into a human-readable format. Note that mode_t is actually a bit field with two parts; the file type is stored within the S_IFMT mask . It can be tested with some macros like S_ISDIR (for the S_IFDIR value with mask S_IFMT ) to get the file type flags. \u9664\u6b64\u4e4b\u5916\uff0cUnix OS\u5f88\u591asystem call\u90fd\u63a5\u6536\u8fd9\u79cd\u901a\u8fc7bit or\u6765\u7ec4\u5408\u5404\u79cd\u6807\u5fd7\u7684\u65b9\u6cd5\uff1b https://github.com/lattera/glibc/blob/master/bits/fcntl.h https://en.wikipedia.org/wiki/Bitwise_operation \u6709\u5e8fdata structure # multiset # std::multiset https://en.wikipedia.org/wiki/Multiset priority queue # std::priority_queue Skip list # https://en.wikipedia.org/wiki/Skip_list \u76f8\u5173\uff1ahttps://www.infoq.com/articles/redis-time-series redis\u4f7f\u7528skip list\u6765\u5b9e\u73b0\u6709\u5e8f\u96c6\u5408 https://jameshfisher.com/2018/04/22/redis-sorted-set/ http://ticki.github.io/blog/skip-lists-done-right/ std::multiset vs. std::priority_queue speed comparision","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#algorithms_use_memory_to_gain_performance_improvement","text":"\u4f7f\u7528\u7a7a\u95f4\u6362\u53d6\u65f6\u95f4 https://www.sciencedirect.com/topics/computer-science/performance-gain https://stackoverflow.com/questions/1898161/memory-vs-performance","title":"algorithms use memory to gain performance improvement"},{"location":"TODO/#lintcode","text":"https://www.zhihu.com/question/31218682","title":"lintcode"},{"location":"TODO/#hash","text":"Hash table \u76f8\u5173\uff1ahttps://www.infoq.com/articles/redis-time-series Hash functions Hash list What exactly (and precisely) is \u201chash?\u201d","title":"hash"},{"location":"TODO/#avl_tree","text":"\u76f8\u5173\uff1ahttps://opensourceforu.com/2016/03/the-life-of-a-process/ https://en.wikipedia.org/wiki/AVL_tree","title":"AVL tree"},{"location":"TODO/#bit_field","text":"https://en.wikipedia.org/wiki/Bit_field \u5173\u4e8ebit field\uff0c \u5728Unix OS\u4e2d\u4f7f\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u6211\u4e4b\u524d\u5df2\u7ecf\u9047\u5230\u4e86\u975e\u5e38\u591a\uff0c\u53ea\u662f\u6ca1\u6709\u610f\u8bc6\u5230\u5b83\u4eec\u5c31\u662fbit field\uff1b\u4eca\u5929\uff0820190518\uff09\u5728\u9605\u8bfb Unix file types \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 Internally, ls obtains the stat structure[ 2] associated with the file and transforms the mode_t field into a human-readable format. Note that mode_t is actually a bit field with two parts; the file type is stored within the S_IFMT mask . It can be tested with some macros like S_ISDIR (for the S_IFDIR value with mask S_IFMT ) to get the file type flags. \u9664\u6b64\u4e4b\u5916\uff0cUnix OS\u5f88\u591asystem call\u90fd\u63a5\u6536\u8fd9\u79cd\u901a\u8fc7bit or\u6765\u7ec4\u5408\u5404\u79cd\u6807\u5fd7\u7684\u65b9\u6cd5\uff1b https://github.com/lattera/glibc/blob/master/bits/fcntl.h https://en.wikipedia.org/wiki/Bitwise_operation","title":"Bit field"},{"location":"TODO/#data_structure","text":"","title":"\u6709\u5e8fdata structure"},{"location":"TODO/#multiset","text":"std::multiset https://en.wikipedia.org/wiki/Multiset","title":"multiset"},{"location":"TODO/#priority_queue","text":"std::priority_queue","title":"priority queue"},{"location":"TODO/#skip_list","text":"https://en.wikipedia.org/wiki/Skip_list \u76f8\u5173\uff1ahttps://www.infoq.com/articles/redis-time-series redis\u4f7f\u7528skip list\u6765\u5b9e\u73b0\u6709\u5e8f\u96c6\u5408 https://jameshfisher.com/2018/04/22/redis-sorted-set/ http://ticki.github.io/blog/skip-lists-done-right/ std::multiset vs. std::priority_queue speed comparision","title":"Skip list"},{"location":"Array/","text":"\u5173\u4e8e\u672c\u7ae0 # Array \uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6700\u5148\u8ba4\u8bc6\u7684data structure\uff0c\u5b83\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u5f88\u591a\u7b80\u5355\u4f46\u662f\u9ad8\u6548\u7684\u529f\u80fd\u3002\u672c\u7ae0\u8fd8\u4f1a\u4ecb\u7ecd\u4e00\u4e9barray\u7684\u9ad8\u7ea7\u4e00\u4e9b\u7684\u73a9\u6cd5\u3002","title":"Introduction"},{"location":"Array/#_1","text":"Array \uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6700\u5148\u8ba4\u8bc6\u7684data structure\uff0c\u5b83\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u5f88\u591a\u7b80\u5355\u4f46\u662f\u9ad8\u6548\u7684\u529f\u80fd\u3002\u672c\u7ae0\u8fd8\u4f1a\u4ecb\u7ecd\u4e00\u4e9barray\u7684\u9ad8\u7ea7\u4e00\u4e9b\u7684\u73a9\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Array/Array-data-structure/","text":"Array data structure #","title":"Array-data-structure"},{"location":"Array/Array-data-structure/#array_data_structure","text":"","title":"Array data structure"},{"location":"Array/bit-array/Bit-array/","text":"Bit array # A bit array (also known as bit map , bit set , bit string , or bit vector ) is an array data structure that compactly stores bits . It can be used to implement a simple set data structure . A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores kw bits, where w is the number of bits in the unit of storage, such as a byte or word , and k is some nonnegative integer. If w does not divide the number of bits to be stored, some space is wasted due to internal fragmentation . NOTE: \u6709 \u5982\u4e0b\u95ee\u9898\uff1a - \u5982\u4f55\u4f7f\u7528binary literal\u6765\u521d\u59cb\u5316bit array\uff1f - \u5982\u4f55index bit array\uff1f - \u4f7f\u7528\u4ec0\u4e48\u7c7b\u578b\u6765\u8868\u793abit array\uff1f Definition # A bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. As with other arrays, the access to a single bit can be managed by applying an index to the array . Assuming its size (or length) to be n bits, the array can be used to specify a subset of the domain (e.g. {0, 1, 2, ..., n \u22121}), where a 1-bit indicates the presence and a 0-bit the absence of a number in the set. This set data structure uses about n / w words of space, where w is the number of bits in each machine word . Whether the least significant bit (of the word) or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred (on little-endian machines). Basic operations # Although most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using bitwise operations . In particular: OR can be used to set a bit to one: 11101010 OR 00000100 = 11101110 AND can be used to set a bit to zero: 11101010 AND 11111101 = 11101000 AND together with zero-testing can be used to determine if a bit is set: 11101010 AND 00000001 = 00000000 = 0 11101010 AND 00000010 = 00000010 \u2260 0 XOR can be used to invert or toggle a bit: 11101010 XOR 00000100 = 11101110 11101110 XOR 00000100 = 11101010 NOT can be used to invert all bits. NOT 10110010 = 01001101 To obtain the bit mask needed for these operations, we can use a bit shift operator to shift the number 1 to the left by the appropriate number of places, as well as bitwise negation if necessary. Given two bit arrays of the same size representing sets, we can compute their union , intersection , and set-theoretic difference using n / w simple bit operations each (2 n / w for difference), as well as the complement of either: for i from 0 to n/w-1 complement_a[i] := not a[i] union[i] := a[i] or b[i] intersection[i] := a[i] and b[i] difference[i] := a[i] and (not b[i]) NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u6b21\u64cd\u4f5c\u4e00\u4e2abyte\uff0c\u800c\u4e0d\u662fbit If we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only n / w memory accesses are required: for i from 0 to n/w-1 index := 0 // if needed word := a[i] for b from 0 to w-1 value := word and 1 \u2260 0 word := word shift right 1 // do something with value index := index + 1 // if needed Both of these code samples exhibit ideal locality of reference , which will subsequently receive large performance boost from a data cache. If a cache line is k words, only about n / wk cache misses will occur. More complex operations # As with character strings it is straightforward to define length , substring , lexicographical compare , concatenation , reverse operations. The implementation of some of these operations is sensitive to endianness . Population / Hamming weight # If we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the Hamming weight article for examples of an efficient implementation. Inversion # Vertical flipping of a one-bit-per-pixel image, or some FFT algorithms, requires flipping the bits of individual words (so b31 b30 ... b0 becomes b0 ... b30 b31 ). When this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits: exchange two 16bit halfwords exchange bytes by pairs (0xddccbbaa -> 0xccddaabb) ... swap bits by pairs swap bits (b31 b30 ... b1 b0 -> b30 b31 ... b0 b1) The last operation can be written ((x&0x55555555)<<1) | (x&0xaaaaaaaa)>>1)). Find first one # The find first set or find first one operation identifies the index or position of the 1-bit with the smallest index in an array, and has widespread hardware support (for arrays not larger than a word) and efficient algorithms for its computation. When a priority queue is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size find first one to longer arrays, one can find the first nonzero word and then run find first one on that word. The related operations find first zero , count leading zeros , count leading ones , count trailing zeros , count trailing ones , and log base 2 (see find first set ) can also be extended to a bit array in a straightforward manner. \u5f00\u6e90\u5e93 # BitArray","title":"Bit-array"},{"location":"Array/bit-array/Bit-array/#bit_array","text":"A bit array (also known as bit map , bit set , bit string , or bit vector ) is an array data structure that compactly stores bits . It can be used to implement a simple set data structure . A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores kw bits, where w is the number of bits in the unit of storage, such as a byte or word , and k is some nonnegative integer. If w does not divide the number of bits to be stored, some space is wasted due to internal fragmentation . NOTE: \u6709 \u5982\u4e0b\u95ee\u9898\uff1a - \u5982\u4f55\u4f7f\u7528binary literal\u6765\u521d\u59cb\u5316bit array\uff1f - \u5982\u4f55index bit array\uff1f - \u4f7f\u7528\u4ec0\u4e48\u7c7b\u578b\u6765\u8868\u793abit array\uff1f","title":"Bit array"},{"location":"Array/bit-array/Bit-array/#definition","text":"A bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. As with other arrays, the access to a single bit can be managed by applying an index to the array . Assuming its size (or length) to be n bits, the array can be used to specify a subset of the domain (e.g. {0, 1, 2, ..., n \u22121}), where a 1-bit indicates the presence and a 0-bit the absence of a number in the set. This set data structure uses about n / w words of space, where w is the number of bits in each machine word . Whether the least significant bit (of the word) or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred (on little-endian machines).","title":"Definition"},{"location":"Array/bit-array/Bit-array/#basic_operations","text":"Although most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using bitwise operations . In particular: OR can be used to set a bit to one: 11101010 OR 00000100 = 11101110 AND can be used to set a bit to zero: 11101010 AND 11111101 = 11101000 AND together with zero-testing can be used to determine if a bit is set: 11101010 AND 00000001 = 00000000 = 0 11101010 AND 00000010 = 00000010 \u2260 0 XOR can be used to invert or toggle a bit: 11101010 XOR 00000100 = 11101110 11101110 XOR 00000100 = 11101010 NOT can be used to invert all bits. NOT 10110010 = 01001101 To obtain the bit mask needed for these operations, we can use a bit shift operator to shift the number 1 to the left by the appropriate number of places, as well as bitwise negation if necessary. Given two bit arrays of the same size representing sets, we can compute their union , intersection , and set-theoretic difference using n / w simple bit operations each (2 n / w for difference), as well as the complement of either: for i from 0 to n/w-1 complement_a[i] := not a[i] union[i] := a[i] or b[i] intersection[i] := a[i] and b[i] difference[i] := a[i] and (not b[i]) NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u6b21\u64cd\u4f5c\u4e00\u4e2abyte\uff0c\u800c\u4e0d\u662fbit If we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only n / w memory accesses are required: for i from 0 to n/w-1 index := 0 // if needed word := a[i] for b from 0 to w-1 value := word and 1 \u2260 0 word := word shift right 1 // do something with value index := index + 1 // if needed Both of these code samples exhibit ideal locality of reference , which will subsequently receive large performance boost from a data cache. If a cache line is k words, only about n / wk cache misses will occur.","title":"Basic operations"},{"location":"Array/bit-array/Bit-array/#more_complex_operations","text":"As with character strings it is straightforward to define length , substring , lexicographical compare , concatenation , reverse operations. The implementation of some of these operations is sensitive to endianness .","title":"More complex operations"},{"location":"Array/bit-array/Bit-array/#population_hamming_weight","text":"If we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the Hamming weight article for examples of an efficient implementation.","title":"Population / Hamming weight"},{"location":"Array/bit-array/Bit-array/#inversion","text":"Vertical flipping of a one-bit-per-pixel image, or some FFT algorithms, requires flipping the bits of individual words (so b31 b30 ... b0 becomes b0 ... b30 b31 ). When this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits: exchange two 16bit halfwords exchange bytes by pairs (0xddccbbaa -> 0xccddaabb) ... swap bits by pairs swap bits (b31 b30 ... b1 b0 -> b30 b31 ... b0 b1) The last operation can be written ((x&0x55555555)<<1) | (x&0xaaaaaaaa)>>1)).","title":"Inversion"},{"location":"Array/bit-array/Bit-array/#find_first_one","text":"The find first set or find first one operation identifies the index or position of the 1-bit with the smallest index in an array, and has widespread hardware support (for arrays not larger than a word) and efficient algorithms for its computation. When a priority queue is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size find first one to longer arrays, one can find the first nonzero word and then run find first one on that word. The related operations find first zero , count leading zeros , count leading ones , count trailing zeros , count trailing ones , and log base 2 (see find first set ) can also be extended to a bit array in a straightforward manner.","title":"Find first one"},{"location":"Array/bit-array/Bit-array/#_1","text":"BitArray","title":"\u5f00\u6e90\u5e93"},{"location":"Array/circular-array/Circular-buffer/","text":"Circular buffer # A circular buffer , circular queue , cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams . Uses # The useful property of a circular buffer is that it does not need to have its elements shuffled around(\u79fb\u52a8) when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer. Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a linked list approach may be preferred instead. In some situations, overwriting(\u76d6\u5199) circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card ) is unable to momentarily keep up. Also, the LZ77 family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer. How it works # A circular buffer first starts empty and of some predefined length. For example, this is a 7-element buffer: Assume that a 1 is written into the middle of the buffer (exact starting location does not matter in a circular buffer): Then assume that two more elements are added \u2014 2 & 3 \u2014 which get appended after the 1: If two elements are then removed from the buffer, the oldest values inside the buffer are removed. The two elements removed, in this case, are 1 & 2, leaving the buffer with just a 3: If the buffer has 7 elements then it is completely full: A consequence of the circular buffer is that when it is full and a subsequent write is performed, then it starts overwriting the oldest data. In this case, two more elements \u2014 A & B \u2014 are added and they overwrite the 3 & 4: Alternatively, the routines that manage the buffer could prevent overwriting the data and return an error or raise an exception . Whether or not data is overwritten is up to the semantics of the buffer routines or the application using the circular buffer. Finally, if two elements are now removed then what would be returned is not 3 & 4 but 5 & 6 because A & B overwrote the 3 & the 4 yielding the buffer with: Circular buffer mechanics # A circular buffer can be implemented using four pointers , or two pointers and two integers: buffer start in memory buffer end in memory, or buffer capacity start of valid data (index or pointer) end of valid data (index or pointer), or amount of data currently in the buffer (integer) This image shows a partially full buffer: This image shows a full buffer with four elements (numbers 1 through 4) having been overwritten: CIRCULAR QUEUE IN C # Implementing a Queue using a circular array #","title":"Circular-buffer"},{"location":"Array/circular-array/Circular-buffer/#circular_buffer","text":"A circular buffer , circular queue , cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams .","title":"Circular buffer"},{"location":"Array/circular-array/Circular-buffer/#uses","text":"The useful property of a circular buffer is that it does not need to have its elements shuffled around(\u79fb\u52a8) when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer. Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a linked list approach may be preferred instead. In some situations, overwriting(\u76d6\u5199) circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card ) is unable to momentarily keep up. Also, the LZ77 family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer.","title":"Uses"},{"location":"Array/circular-array/Circular-buffer/#how_it_works","text":"A circular buffer first starts empty and of some predefined length. For example, this is a 7-element buffer: Assume that a 1 is written into the middle of the buffer (exact starting location does not matter in a circular buffer): Then assume that two more elements are added \u2014 2 & 3 \u2014 which get appended after the 1: If two elements are then removed from the buffer, the oldest values inside the buffer are removed. The two elements removed, in this case, are 1 & 2, leaving the buffer with just a 3: If the buffer has 7 elements then it is completely full: A consequence of the circular buffer is that when it is full and a subsequent write is performed, then it starts overwriting the oldest data. In this case, two more elements \u2014 A & B \u2014 are added and they overwrite the 3 & 4: Alternatively, the routines that manage the buffer could prevent overwriting the data and return an error or raise an exception . Whether or not data is overwritten is up to the semantics of the buffer routines or the application using the circular buffer. Finally, if two elements are now removed then what would be returned is not 3 & 4 but 5 & 6 because A & B overwrote the 3 & the 4 yielding the buffer with:","title":"How it works"},{"location":"Array/circular-array/Circular-buffer/#circular_buffer_mechanics","text":"A circular buffer can be implemented using four pointers , or two pointers and two integers: buffer start in memory buffer end in memory, or buffer capacity start of valid data (index or pointer) end of valid data (index or pointer), or amount of data currently in the buffer (integer) This image shows a partially full buffer: This image shows a full buffer with four elements (numbers 1 through 4) having been overwritten:","title":"Circular buffer mechanics"},{"location":"Array/circular-array/Circular-buffer/#circular_queue_in_c","text":"","title":"CIRCULAR QUEUE IN C"},{"location":"Array/circular-array/Circular-buffer/#implementing_a_queue_using_a_circular_array","text":"","title":"Implementing a Queue using a circular array"},{"location":"Array/circular-array/circular-array/","text":"\u5176\u5b9e\u6211\u60f3\u8981\u5b9e\u73b0\u7684DS\u4e25\u683c\u6765\u8bf4\u5e76\u4e0d\u662f\u4e00\u4e2a Circular Queue?Queue\u662fFIFO\u7684\uff0c\u800c\u6211\u7684\u9700\u6c42\u662f\u5bf9\u6570\u636e\u7684\u5b58\u50a8\uff0c\u6bcf\u6b21\u65b0\u6dfb\u52a0\u7684\u6570\u636e\u90fd\u662f\u52a0\u5165\u5230\u5934\uff0c\u5f53\u6570\u7ec4\u5df2\u7ecf\u6ee1\u4e86\u540e\uff0c\u5219\u65b0\u6dfb\u52a0\u7684\u6570\u636e\u76f4\u63a5overwrite\u5230\u6570\u7ec4\u7684end\uff1b \u76ee\u524d\u6211\u7684\u8fd9\u4e2a\u9700\u6c42\u662f\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2alinked list\u6765\u5b9e\u73b0\u7684\uff0c\u6bcf\u6b21\u65b0\u6dfb\u52a0\u4e00\u4e2a\u5143\u7d20\uff0c\u5219new \u4e00\u4e2anode\uff0c\u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u4e86\u6700\u5927\u957f\u5ea6\uff0c\u5219pop\u6700\u540e\u4e00\u4e2anode\uff1b \u6240\u4ee5\uff0c\u6bcf\u6b21\u6dfb\u52a0\u5143\u7d20\u90fd\u662f push_front | | | | | | start end \u90fd\u662finteger\u8fd8\u662fpointer\uff1f \u65e2\u7136\u662f\u73af\u5f62\u7684\uff0c\u6709\u4e00\u4e2a\u7279\u70b9\u5c31\u662f\u5b83\u6ca1\u6709\u56fa\u5b9a\u7684start\uff0cend \u5f53array is full\uff0c\u5219\u65b0\u6dfb\u52a0\u7684\u5143\u7d20\u76f4\u63a5","title":"Circular-array"},{"location":"Data-structure/","text":"\u5173\u4e8e\u672c\u7ae0 # \u8fd9\u4e00\u7ae0\u8282\u5bf9 data structure \u8fdb\u884c\u7efc\u8ff0\uff0c\u8bf4\u660edata structure\u9886\u57df\u7684\u4e00\u4e9b\u5e38\u89c1\u6982\u5ff5\u3002 \u4e0b\u9762\u8fd9\u4e9b\u662f\u5728\u540e\u9762\u8ba8\u8bba\u5404\u79cd\u5177\u4f53\u7684data structure\u90fd\u4f1a\u6d89\u53ca\u5230\u7684\u4e00\u4e9b\u516c\u5171\u7684\u8bae\u9898\uff0c\uff1a Formal description\uff1a\u5982\u4f55\u6765\u5f62\u5f0f\u5316\u5730\u63cf\u8ff0data structure\uff1f Recursion\uff1a\u4ece\u9012\u5f52\u7684\u89d2\u5ea6\u6765\u5206\u6790\u7ed3\u6784\uff0c\u53c2\u89c1 Recursive-data-type Representation\uff1a\u5982\u4f55\u8868\u793adata structure\uff1f Implicit implementation of data structure\uff1a\u5bf9\u4e8e\u5404\u79cddata structure\u7684\u5b9e\u73b0\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u90fd\u4e0d\u4f1a\u5ffd\u89c6\uff0c\u4f46\u662f\u4ed6\u4eec\u53ef\u80fd\u4f1a\u5ffd\u89c6\u7684\u662fimplicit implementation\u3002 \u540e\u7eed\u5728\u8ba8\u8bba\u5404\u79cd\u5177\u4f53\u7684data structure\u7684\u65f6\u5019\uff0c\u4f1a\u5bf9\u8fd9\u4e9b\u8bae\u9898\u8fdb\u884c\u5177\u4f53\u7684\u5206\u6790\u3002","title":"Introduction"},{"location":"Data-structure/#_1","text":"\u8fd9\u4e00\u7ae0\u8282\u5bf9 data structure \u8fdb\u884c\u7efc\u8ff0\uff0c\u8bf4\u660edata structure\u9886\u57df\u7684\u4e00\u4e9b\u5e38\u89c1\u6982\u5ff5\u3002 \u4e0b\u9762\u8fd9\u4e9b\u662f\u5728\u540e\u9762\u8ba8\u8bba\u5404\u79cd\u5177\u4f53\u7684data structure\u90fd\u4f1a\u6d89\u53ca\u5230\u7684\u4e00\u4e9b\u516c\u5171\u7684\u8bae\u9898\uff0c\uff1a Formal description\uff1a\u5982\u4f55\u6765\u5f62\u5f0f\u5316\u5730\u63cf\u8ff0data structure\uff1f Recursion\uff1a\u4ece\u9012\u5f52\u7684\u89d2\u5ea6\u6765\u5206\u6790\u7ed3\u6784\uff0c\u53c2\u89c1 Recursive-data-type Representation\uff1a\u5982\u4f55\u8868\u793adata structure\uff1f Implicit implementation of data structure\uff1a\u5bf9\u4e8e\u5404\u79cddata structure\u7684\u5b9e\u73b0\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u90fd\u4e0d\u4f1a\u5ffd\u89c6\uff0c\u4f46\u662f\u4ed6\u4eec\u53ef\u80fd\u4f1a\u5ffd\u89c6\u7684\u662fimplicit implementation\u3002 \u540e\u7eed\u5728\u8ba8\u8bba\u5404\u79cd\u5177\u4f53\u7684data structure\u7684\u65f6\u5019\uff0c\u4f1a\u5bf9\u8fd9\u4e9b\u8bae\u9898\u8fdb\u884c\u5177\u4f53\u7684\u5206\u6790\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Data-structure/Abstract-data-type/","text":"Abstract data type # \u7ef4\u57fa\u767e\u79d1\u5173\u4e8e Abstract data type \u7684\u8bb2\u89e3\u975e\u5e38\u6df1\u523b\uff0c\u6df1\u5965\uff0c\u6211\u5bf9ADT\u7684\u7b80\u5355\u7406\u89e3\u5c31\u662f\u5728\u5404\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u6807\u51c6\u5e93\u4e2d\u90fd\u4f1a\u63d0\u4f9b\u7684\u5404\u79cd\u5bb9\u5668\uff0c\u6bd4\u5982 C++ \u7684 STL \uff0cpython\u7684 collections \uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u4e8e\u5b83\u4eec\u975e\u5e38\u719f\u7cfb\uff0c\u5f80\u5f80\u76f4\u63a5\u4f7f\u7528\u800c\u65e0\u9700\u5173\u7cfb\u5b83\u4eec\u7684\u5b9e\u73b0\u7ec6\u8282\uff08\u62bd\u8c61\u7684\u597d\u5904\uff09\u3002","title":"Abstract-data-type"},{"location":"Data-structure/Abstract-data-type/#abstract_data_type","text":"\u7ef4\u57fa\u767e\u79d1\u5173\u4e8e Abstract data type \u7684\u8bb2\u89e3\u975e\u5e38\u6df1\u523b\uff0c\u6df1\u5965\uff0c\u6211\u5bf9ADT\u7684\u7b80\u5355\u7406\u89e3\u5c31\u662f\u5728\u5404\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u6807\u51c6\u5e93\u4e2d\u90fd\u4f1a\u63d0\u4f9b\u7684\u5404\u79cd\u5bb9\u5668\uff0c\u6bd4\u5982 C++ \u7684 STL \uff0cpython\u7684 collections \uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u4e8e\u5b83\u4eec\u975e\u5e38\u719f\u7cfb\uff0c\u5f80\u5f80\u76f4\u63a5\u4f7f\u7528\u800c\u65e0\u9700\u5173\u7cfb\u5b83\u4eec\u7684\u5b9e\u73b0\u7ec6\u8282\uff08\u62bd\u8c61\u7684\u597d\u5904\uff09\u3002","title":"Abstract data type"},{"location":"Data-structure/Data-structure/","text":"Data structure # A data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values , the relationships among them, and the functions or operations that can be applied to the data. NOTE: Efficient can be measured by time complexity and space complexity . Data structure VS abstract data types (ADT) # Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type. NOTE: \u53c2\u89c1 \u62bd\u8c61\u4e0e\u5b9e\u73b0 Usage # Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers. NOTE: So how to choose data structure ? It is an art and worth learning. Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services . Usually, efficient data structures are key to designing efficient algorithms . Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory . Implicit data structure # An Implicit Binary Tree # implicit stack # https://mp.weixin.qq.com/s/YCWRBCay8PvnjomVcEBOTA List of data structures # List of terms relating to algorithms and data structures #","title":"Data-structure"},{"location":"Data-structure/Data-structure/#data_structure","text":"A data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values , the relationships among them, and the functions or operations that can be applied to the data. NOTE: Efficient can be measured by time complexity and space complexity .","title":"Data structure"},{"location":"Data-structure/Data-structure/#data_structure_vs_abstract_data_types_adt","text":"Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type. NOTE: \u53c2\u89c1 \u62bd\u8c61\u4e0e\u5b9e\u73b0","title":"Data structure VS abstract data types (ADT)"},{"location":"Data-structure/Data-structure/#usage","text":"Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers. NOTE: So how to choose data structure ? It is an art and worth learning. Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services . Usually, efficient data structures are key to designing efficient algorithms . Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory .","title":"Usage"},{"location":"Data-structure/Data-structure/#implicit_data_structure","text":"","title":"Implicit data structure"},{"location":"Data-structure/Data-structure/#an_implicit_binary_tree","text":"","title":"An Implicit Binary Tree"},{"location":"Data-structure/Data-structure/#implicit_stack","text":"https://mp.weixin.qq.com/s/YCWRBCay8PvnjomVcEBOTA","title":"implicit stack"},{"location":"Data-structure/Data-structure/#list_of_data_structures","text":"","title":"List of data structures"},{"location":"Data-structure/Data-structure/#list_of_terms_relating_to_algorithms_and_data_structures","text":"","title":"List of terms relating to algorithms and data structures"},{"location":"Data-structure/Recursive-data-type/","text":"Recursive data type # \u672c\u6587\u662f\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684\u9605\u8bfb\u7b14\u8bb0\u3002 \u5982\u679c\u5c06\u5404\u79cddata structure\u770b\u505a\u662f\u4e00\u79cd\u7c7b\u578b\uff0c\u90a3\u4e9b\u5177\u5907 \u9012\u5f52\u6027 \u7684data structure\uff0c\u5c31\u662f\u672c\u8282\u6807\u9898\u6240\u8ff0\u201c recursive data type \u201d\uff0c\u8fd9\u4e2a\u672f\u8bed\u8868\u8fbe\u4e86\u6570\u636e\u7c7b\u578b\u7684\u9012\u5f52\u6027\u7279\u5f81\u3002\u5bf9\u4e8erecursive data type\uff0c\u90fd\u53ef\u4ee5\u7ed9\u51fa\u5b83\u7684 recursive definition \u3002 \u7ef4\u57fa\u767e\u79d1 Recursive data type \u4e2d\u5173\u4e8e\u5b83\u7684\u63cf\u8ff0\u662f\u4f7f\u7528\u7684\u7ef4\u57fa\u767e\u79d1 Recursive definition \u4e2d\u201crecursively defined set\u201d\u7684\u63cf\u8ff0\u65b9\u5f0f\u3002 \u6700\u6700\u5178\u578b\u7684recursive data type\u5c31\u662f\uff1a list tree \u5728\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684 Example \u8282\u7ed9\u51fa\u4e86\u4e0a\u8ff0\u4e24\u79cdrecursive data type\u7684 recursive definition \u3002 Recursive data type \u548c Structural induction \u7d27\u5bc6\u76f8\u5173\uff1b","title":"Recursive-data-type"},{"location":"Data-structure/Recursive-data-type/#recursive_data_type","text":"\u672c\u6587\u662f\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684\u9605\u8bfb\u7b14\u8bb0\u3002 \u5982\u679c\u5c06\u5404\u79cddata structure\u770b\u505a\u662f\u4e00\u79cd\u7c7b\u578b\uff0c\u90a3\u4e9b\u5177\u5907 \u9012\u5f52\u6027 \u7684data structure\uff0c\u5c31\u662f\u672c\u8282\u6807\u9898\u6240\u8ff0\u201c recursive data type \u201d\uff0c\u8fd9\u4e2a\u672f\u8bed\u8868\u8fbe\u4e86\u6570\u636e\u7c7b\u578b\u7684\u9012\u5f52\u6027\u7279\u5f81\u3002\u5bf9\u4e8erecursive data type\uff0c\u90fd\u53ef\u4ee5\u7ed9\u51fa\u5b83\u7684 recursive definition \u3002 \u7ef4\u57fa\u767e\u79d1 Recursive data type \u4e2d\u5173\u4e8e\u5b83\u7684\u63cf\u8ff0\u662f\u4f7f\u7528\u7684\u7ef4\u57fa\u767e\u79d1 Recursive definition \u4e2d\u201crecursively defined set\u201d\u7684\u63cf\u8ff0\u65b9\u5f0f\u3002 \u6700\u6700\u5178\u578b\u7684recursive data type\u5c31\u662f\uff1a list tree \u5728\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684 Example \u8282\u7ed9\u51fa\u4e86\u4e0a\u8ff0\u4e24\u79cdrecursive data type\u7684 recursive definition \u3002 Recursive data type \u548c Structural induction \u7d27\u5bc6\u76f8\u5173\uff1b","title":"Recursive data type"},{"location":"Graph/","text":"\u5173\u4e8e\u672c\u7ae0 # \u8ba8\u8bba\u4e86\u56fe\u7684\u5b9a\u4e49\uff0c\u6784\u9020\uff0c\u57fa\u672c\u64cd\u4f5c\uff0c\u4ee5\u53ca\u56fe\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u5404\u4e2a\u9886\u57df\u5404\u79cd\u5404\u6837\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u300a Discrete Mathematics and Its Applications \u300b\u548c\u7ef4\u57fa\u767e\u79d1\u3002 TODO # \u5982\u4f55\u5224\u65ad\u4e00\u4e2agraph\u662f\u5426\u662ftree\uff1f","title":"Introduction"},{"location":"Graph/#_1","text":"\u8ba8\u8bba\u4e86\u56fe\u7684\u5b9a\u4e49\uff0c\u6784\u9020\uff0c\u57fa\u672c\u64cd\u4f5c\uff0c\u4ee5\u53ca\u56fe\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u5404\u4e2a\u9886\u57df\u5404\u79cd\u5404\u6837\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u300a Discrete Mathematics and Its Applications \u300b\u548c\u7ef4\u57fa\u767e\u79d1\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Graph/#todo","text":"\u5982\u4f55\u5224\u65ad\u4e00\u4e2agraph\u662f\u5426\u662ftree\uff1f","title":"TODO"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/","text":"Graph (discrete mathematics) and Graph theory # graph terminology # undirected graph # First, we give some terminology that describes the vertices and edges of undirected graphs . DEFINITION # Two vertices u and v in an undirected graph G are called adjacent (or neighbors ) in G if u and v are endpoints of an edge e of G . Such an edge e is called incident with the vertices u and v and e is said to connect u and v . DEFINITION # The set of all neighbors of a vertex v of G = (V,E) , denoted by N(v) , is called the neighborhood of v . If A is a subset of V , we denote by N(A) the set of all vertices in G that are adjacent to at least one vertex in A . So, N(A) =U v\u2208A N(v) . NOTE: \u4e0a\u8bc9\u5b9a\u4e49\u5176\u5b9e\u5b9a\u4e49\u7684\u662f\u4e00\u4e2a\u70b9\u7684 neighborhood \u548c\u4e00\u4e2a\u70b9\u96c6\u7684 neighborhood \u3002 DEFINITION # The degree of a vertex in an undirected graph is the number of edges incident with it, except that a loop at a vertex contributes twice to the degree of that vertex. The degree of the vertex v is denoted by deg(v) . THE HANDSHAKING THEOREM # Let G = (V,E) be an undirected graph with m edges. Then $$ 2m = \\sum_{v \\in V} deg(v) $$ (Note that this applies even if multiple edges and loops are present.) THEOREM # An undirected graph has an even number of vertices of odd degree. NOTE: \u7ffb\u8bd1\u6210\u4e2d\u6587\u662f\uff1a\u4e00\u4e2a\u65e0\u5411\u56fe\u6709\u5076\u6570\u4e2a\u5947\u6570\u5ea6\u7684\u70b9\u3002 directed graph # Terminology for graphs with directed edges reflects the fact that edges in directed graphs have directions. DEFINITION # When (u,v) is an edge of the graph G with directed edges, u is said to be adjacent to v and v is said to be adjacent from u . The vertex u is called the initial vertex of (u,v) , and v is called the terminal or end vertex of (u,v) . The initial vertex and terminal vertex of a loop are the same. DEFINITION # In a graph with directed edges the in-degree of a vertex v , denoted by deg \u2212 (v) , is the number of edges with v as their terminal vertex. The out-degree of v , denoted by deg + (v) , is the number of edges with v as their initial vertex. (Note that a loop at a vertex contributes 1 to both the in-degree and the out-degree of this vertex.) THEOREM # Let G = (V,E) be a graph with directed edges. Then $$ \\sum_{v \\in V} deg^-(v) = \\sum_{v \\in V} deg^+(v)=|E| $$ There are many properties of a graph with directed edges that do not depend on the direction of its edges.Consequently, it is often useful to ignore these directions. The undirected graph that results from ignoring directions of edges is called the underlying undirected graph . A graph with directed edges and its underlying undirected graph have the same number of edges. NOTE: \u8fd9\u4e9b\u5c5e\u4e8e\u5728\u540e\u6587\u4e2d\u4f1a\u5927\u91cf\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5148\u719f\u6089\u719f\u6089\u3002 Some Special Simple Graphs # Complete Graphs # A complete graph on n vertices, denoted by $K_n$ , is a simple graph that contains exactly one edge between each pair of distinct vertices. The graphs $K_n$ , for n = 1,2,3,4,5,6, are displayed in Figure 3. A simple graph for which there is at least one pair of distinct vertex not connected by an edge is called noncomplete . Cycles # A cycle $C_n$ , n \u2265 3, consists of n vertices $v_1 ,v_2 ,...,v_n$ and edges ${v_1 ,v_2 }$, ${v_2 ,v_3 }$,...,${v_{n\u22121} ,v_n }$, and ${v_n ,v_1 }$. The cycles $C_3$ , $C_4$ , $C_5$ , and $C_6$ are displayed in Figure 4. Wheels # We obtain a wheel $W_n$ when we add an additional vertex to a cycle $C_n$ , for n \u2265 3, and connect this new vertex to each of the n vertices in $C_n$ , by new edges. The wheels $W_3$ , $W_4$ , $W_5$ , and $W_6$ are displayed in Figure 5. n-Cubes # An n-dimensional hypercube,or n-cube,denoted by $Q_n$ ,is a graph that has vertices representing the $2^n$ bit strings of length n . Two vertices are adjacent if and only if the bit strings that they represent differ in exactly one bit position. We display $Q_1$ ,$ Q_2$ , and $Q_3$ in Figure 6. Note that you can construct the (n + 1)-cube $Q_{n+1}$ from the n-cube $Q_n$ by making two copies of $Q_n$ , prefacing the labels on the vertices with a 0 in one copy of $Q_n$ and with a 1 in the other copy of $Q_n$ , and adding edges connecting two vertices that have labels differing only in the first bit. In Figure 6, $Q_ 3$ is constructed from $Q_2$ by drawing two copies of $Q_2$ as the top and bottom faces of $Q_3$ , adding 0 at the beginning of the label of each vertex in the bottom face and 1 at the beginning of the label of each vertex in the top face. (Here, by face we mean a face of a cube in three-dimensional space. Think of drawing the graph $Q_3$ in three-dimensional space with copies of $Q_2$ as the top and bottom faces of a cube and then drawing the projection of the resulting depiction in the plane.) NOTE: How about $Q_4$? see the wikipedia entry Hypercube . Bipartite Graphs # Sometimes a graph has the property that its vertex set can be divided into two disjoint subsets such that each edge connects a vertex in one of these subsets to a vertex in the other subset. For example, consider the graph representing marriages between men and women in a village, where each person is represented by a vertex and a marriage is represented by an edge. In this graph, each edge connects a vertex in the subset of vertices representing males and a vertex in the subset of vertices representing females. This leads us to Definition 5. DEFINITION # A simple graph G is called bipartite if its vertex set V can be partitioned into two disjoint sets $V_1$ and $V_2$ such that every edge in the graph connects a vertex in $V_1$ and a vertex in $V_2$ (so that no edge in G connects either two vertices in $V_1$ or two vertices in $V_2$ ). When this condition holds, we call the pair $(V_1 ,V_2 )$ a bipartition of the vertex set V of G. In Example 9 we will show that $C_6$ is bipartite, and in Example 10 we will show that $K_3$ is not bipartite. Theorem below provides a useful criterion for determining whether a graph is bipartite. THEOREM 4 # A simple graph is bipartite if and only if it is possible to assign one of two different colors to each vertex of the graph so that no two adjacent vertices are assigned the same color. Theorem 4 is an example of a result in the part of graph theory known as graph colorings . Graph colorings is an important part of graph theory with important applications. We will study graph colorings further in Section 10.8. Another useful criterion for determining whether a graph is bipartite is based on the notion of a path , a topic we study in Section 10.4. A graph is bipartite if and only if it is not possible to start at a vertex and return to this vertex by traversing an odd number of distinct edges. We will make this notion more precise when we discuss paths and circuits in graphs in Section 10.4 (see Exercise 63 in that section). Complete Bipartite Graphs # Bipartite Graphs and Matchings #","title":"Graph-and-Graph-theory.md"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#graph_discrete_mathematics_and_graph_theory","text":"","title":"Graph (discrete mathematics) and Graph theory"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#graph_terminology","text":"","title":"graph terminology"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#undirected_graph","text":"First, we give some terminology that describes the vertices and edges of undirected graphs .","title":"undirected graph"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#definition","text":"Two vertices u and v in an undirected graph G are called adjacent (or neighbors ) in G if u and v are endpoints of an edge e of G . Such an edge e is called incident with the vertices u and v and e is said to connect u and v .","title":"DEFINITION"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#definition_1","text":"The set of all neighbors of a vertex v of G = (V,E) , denoted by N(v) , is called the neighborhood of v . If A is a subset of V , we denote by N(A) the set of all vertices in G that are adjacent to at least one vertex in A . So, N(A) =U v\u2208A N(v) . NOTE: \u4e0a\u8bc9\u5b9a\u4e49\u5176\u5b9e\u5b9a\u4e49\u7684\u662f\u4e00\u4e2a\u70b9\u7684 neighborhood \u548c\u4e00\u4e2a\u70b9\u96c6\u7684 neighborhood \u3002","title":"DEFINITION"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#definition_2","text":"The degree of a vertex in an undirected graph is the number of edges incident with it, except that a loop at a vertex contributes twice to the degree of that vertex. The degree of the vertex v is denoted by deg(v) .","title":"DEFINITION"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#the_handshaking_theorem","text":"Let G = (V,E) be an undirected graph with m edges. Then $$ 2m = \\sum_{v \\in V} deg(v) $$ (Note that this applies even if multiple edges and loops are present.)","title":"THE HANDSHAKING THEOREM"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#theorem","text":"An undirected graph has an even number of vertices of odd degree. NOTE: \u7ffb\u8bd1\u6210\u4e2d\u6587\u662f\uff1a\u4e00\u4e2a\u65e0\u5411\u56fe\u6709\u5076\u6570\u4e2a\u5947\u6570\u5ea6\u7684\u70b9\u3002","title":"THEOREM"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#directed_graph","text":"Terminology for graphs with directed edges reflects the fact that edges in directed graphs have directions.","title":"directed graph"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#definition_3","text":"When (u,v) is an edge of the graph G with directed edges, u is said to be adjacent to v and v is said to be adjacent from u . The vertex u is called the initial vertex of (u,v) , and v is called the terminal or end vertex of (u,v) . The initial vertex and terminal vertex of a loop are the same.","title":"DEFINITION"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#definition_4","text":"In a graph with directed edges the in-degree of a vertex v , denoted by deg \u2212 (v) , is the number of edges with v as their terminal vertex. The out-degree of v , denoted by deg + (v) , is the number of edges with v as their initial vertex. (Note that a loop at a vertex contributes 1 to both the in-degree and the out-degree of this vertex.)","title":"DEFINITION"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#theorem_1","text":"Let G = (V,E) be a graph with directed edges. Then $$ \\sum_{v \\in V} deg^-(v) = \\sum_{v \\in V} deg^+(v)=|E| $$ There are many properties of a graph with directed edges that do not depend on the direction of its edges.Consequently, it is often useful to ignore these directions. The undirected graph that results from ignoring directions of edges is called the underlying undirected graph . A graph with directed edges and its underlying undirected graph have the same number of edges. NOTE: \u8fd9\u4e9b\u5c5e\u4e8e\u5728\u540e\u6587\u4e2d\u4f1a\u5927\u91cf\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5148\u719f\u6089\u719f\u6089\u3002","title":"THEOREM"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#some_special_simple_graphs","text":"","title":"Some Special Simple Graphs"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#complete_graphs","text":"A complete graph on n vertices, denoted by $K_n$ , is a simple graph that contains exactly one edge between each pair of distinct vertices. The graphs $K_n$ , for n = 1,2,3,4,5,6, are displayed in Figure 3. A simple graph for which there is at least one pair of distinct vertex not connected by an edge is called noncomplete .","title":"Complete Graphs"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#cycles","text":"A cycle $C_n$ , n \u2265 3, consists of n vertices $v_1 ,v_2 ,...,v_n$ and edges ${v_1 ,v_2 }$, ${v_2 ,v_3 }$,...,${v_{n\u22121} ,v_n }$, and ${v_n ,v_1 }$. The cycles $C_3$ , $C_4$ , $C_5$ , and $C_6$ are displayed in Figure 4.","title":"Cycles"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#wheels","text":"We obtain a wheel $W_n$ when we add an additional vertex to a cycle $C_n$ , for n \u2265 3, and connect this new vertex to each of the n vertices in $C_n$ , by new edges. The wheels $W_3$ , $W_4$ , $W_5$ , and $W_6$ are displayed in Figure 5.","title":"Wheels"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#n-cubes","text":"An n-dimensional hypercube,or n-cube,denoted by $Q_n$ ,is a graph that has vertices representing the $2^n$ bit strings of length n . Two vertices are adjacent if and only if the bit strings that they represent differ in exactly one bit position. We display $Q_1$ ,$ Q_2$ , and $Q_3$ in Figure 6. Note that you can construct the (n + 1)-cube $Q_{n+1}$ from the n-cube $Q_n$ by making two copies of $Q_n$ , prefacing the labels on the vertices with a 0 in one copy of $Q_n$ and with a 1 in the other copy of $Q_n$ , and adding edges connecting two vertices that have labels differing only in the first bit. In Figure 6, $Q_ 3$ is constructed from $Q_2$ by drawing two copies of $Q_2$ as the top and bottom faces of $Q_3$ , adding 0 at the beginning of the label of each vertex in the bottom face and 1 at the beginning of the label of each vertex in the top face. (Here, by face we mean a face of a cube in three-dimensional space. Think of drawing the graph $Q_3$ in three-dimensional space with copies of $Q_2$ as the top and bottom faces of a cube and then drawing the projection of the resulting depiction in the plane.) NOTE: How about $Q_4$? see the wikipedia entry Hypercube .","title":"n-Cubes"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#bipartite_graphs","text":"Sometimes a graph has the property that its vertex set can be divided into two disjoint subsets such that each edge connects a vertex in one of these subsets to a vertex in the other subset. For example, consider the graph representing marriages between men and women in a village, where each person is represented by a vertex and a marriage is represented by an edge. In this graph, each edge connects a vertex in the subset of vertices representing males and a vertex in the subset of vertices representing females. This leads us to Definition 5.","title":"Bipartite Graphs"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#definition_5","text":"A simple graph G is called bipartite if its vertex set V can be partitioned into two disjoint sets $V_1$ and $V_2$ such that every edge in the graph connects a vertex in $V_1$ and a vertex in $V_2$ (so that no edge in G connects either two vertices in $V_1$ or two vertices in $V_2$ ). When this condition holds, we call the pair $(V_1 ,V_2 )$ a bipartition of the vertex set V of G. In Example 9 we will show that $C_6$ is bipartite, and in Example 10 we will show that $K_3$ is not bipartite. Theorem below provides a useful criterion for determining whether a graph is bipartite.","title":"DEFINITION"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#theorem_4","text":"A simple graph is bipartite if and only if it is possible to assign one of two different colors to each vertex of the graph so that no two adjacent vertices are assigned the same color. Theorem 4 is an example of a result in the part of graph theory known as graph colorings . Graph colorings is an important part of graph theory with important applications. We will study graph colorings further in Section 10.8. Another useful criterion for determining whether a graph is bipartite is based on the notion of a path , a topic we study in Section 10.4. A graph is bipartite if and only if it is not possible to start at a vertex and return to this vertex by traversing an odd number of distinct edges. We will make this notion more precise when we discuss paths and circuits in graphs in Section 10.4 (see Exercise 63 in that section).","title":"THEOREM 4"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#complete_bipartite_graphs","text":"","title":"Complete Bipartite Graphs"},{"location":"Graph/01-Graph-and-Graph-theory/01-Graph-and-Graph-theory/#bipartite_graphs_and_matchings","text":"","title":"Bipartite Graphs and Matchings"},{"location":"Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-graph/","text":"Directed graph #","title":"Directed-graph"},{"location":"Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-graph/#directed_graph","text":"","title":"Directed graph"},{"location":"Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-acyclic-graph/Directed-acyclic-graph/","text":"Directed acyclic graph Directed acyclic graph #","title":"Directed-acyclic-graph"},{"location":"Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-acyclic-graph/Directed-acyclic-graph/#directed_acyclic_graph","text":"","title":"Directed acyclic graph"},{"location":"Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/","text":"Representations # Representing weighted graphs # Representing graphs #","title":"Graph-representations"},{"location":"Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representations","text":"","title":"Representations"},{"location":"Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representing_weighted_graphs","text":"","title":"Representing weighted graphs"},{"location":"Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representing_graphs","text":"","title":"Representing graphs"},{"location":"Graph/02-Graph-Model/Application-of-graph/","text":"directed acyclic word graph # jieba \u7684\u8bcd\u56fe Control-flow graph # \u5728\u7f16\u8bd1\u539f\u7406\u4e2d\u4f7f\u7528\u7684\u63a7\u5236\u6d41\u56fe","title":"Application-of-graph"},{"location":"Graph/02-Graph-Model/Application-of-graph/#directed_acyclic_word_graph","text":"jieba \u7684\u8bcd\u56fe","title":"directed acyclic word graph"},{"location":"Graph/02-Graph-Model/Application-of-graph/#control-flow_graph","text":"\u5728\u7f16\u8bd1\u539f\u7406\u4e2d\u4f7f\u7528\u7684\u63a7\u5236\u6d41\u56fe","title":"Control-flow graph"},{"location":"Graph/02-Graph-Model/Graph-Model/","text":"Graph Models # This mainly includes application-specific graphs. COMMUNICATION NETWORKS # We can model different communications networks using vertices to represent devices and edges to represent the particular type of communications links of interest. SOFTWARE DESIGN APPLICATIONS # NOTE: graph\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5176\u4ed6\u7684\u5b66\u79d1\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u6211\u5c06\u6b64\u7bc7\u4f5c\u4e3a\u6211\u603b\u7ed3\u5404\u79cd\u5404\u6837\u7684graph model\u3002\u4e3a\u4ec0\u4e48\u4f7f\u7528graph\u6765\u63cf\u8ff0\u95ee\u9898\u5462\uff1f\u6211\u89c9\u5f97\u56fe\u6709\u5982\u4e0b\u4f18\u52bf\uff1a\u56fe\u80fd\u591f\u975e\u5e38\u597d\u5730\u63cf\u8ff0\u5b9e\u4f53(vertex)\u548c\u5b9e\u4f53\u4e4b\u95f4\u5173\u7cfb(edge)\u3002 \u8fd9\u79cd\u5173\u7cfb\u53ef\u80fd\u662f\u62bd\u8c61\u7684dependency\uff0cprecedence\uff0c\u4e5f\u53ef\u80fd\u8868\u793a\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff08\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb\uff09\u3002 \u56fe\u80fd\u591f\u63cf\u8ff0\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff0c\u5176\u4e2d\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb \u4f7f\u7528\u56fe\u6765\u5efa\u6a21\u540e\uff0c\u5f88\u591a\u95ee\u9898\u7684\u89e3\u51b3\u5c31\u53d8\u5f97\u7b80\u5355\u4e86\u3002 Graph models are useful tools in the design of software. We will briefly describe two of these models here. Dependency Graph # Module Dependency Graphs One of the most important tasks in designing software is how to structure a program into different parts,or modules. Understanding how the different modules of a program interact is essential not only for program design, but also for testing and maintenance of the resulting software. A module dependency graph provides a useful tool for understanding how different modules of a program interact. In a program dependency graph, each module is represented by a vertex. There is a directed edge from a module to a second module if the second module depends on the first. An example of a program dependency graph for a web browser is shown in Figure 9. NOTE: \u4f7f\u7528edge\u6765\u8868\u793adependency\u5173\u7cfb\uff0c\u5982\u4e0b\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5173\u4e8e\u8868\u8fbe\u4f9d\u8d56\u5173\u7cfb\u7684\u56fe\u7684\u6587\u7ae0\uff1a Dependency graph Wait-for graph Precedence graph # Precedence Graphs and Concurrent Processing Computer programs can be executed more rapidly by executing certain statements concurrently. It is important not to execute a statement that requires results of statements not yet executed. The dependence of statements on previous statements can be represented by a directed graph. Each statement is represented by a vertex, and there is an edge from one statement to a second statement if the second statement cannot be executed before the first statement. This resulting graph is called a precedence graph . A computer program and its graph are displayed in Figure 10. For instance, the graph shows that statement S5 cannot be executed before statements S1 , S2 , and S4 are executed. NOTE: \u4f7f\u7528edge\u6765\u8868\u793aprecedence\u5173\u7cfb\u3002precedence\u5173\u7cfb VS dependency\u5173\u7cfb\uff1f Precedence graph Concurrency control Serializability Serialization Conflict Serializability in DBMS NOTE: Wait-for graph vs Precedence graph ?\u4e24\u8005\u90fd\u5728concurrent control\u76f8\u5173\u95ee\u9898\u4e2d\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u6bd4\u8f83\u4e00\u4e0b\u5b83\u4eec\u3002 Control-flow graph # NOTE: \u4f7f\u7528edge\u6765\u8868\u793acontrol\u7684flow Dataflow # NOTE:\u4f7f\u7528edge\u6765\u8868\u793adata\u7684flow Dataflow programming Data-flow analysis TensorFlow : A machine-learning library based on dataflow programming. \u5728tensorflow\u4e2d\uff0cnode\u8868\u793aoperation\uff0cedge\u8868\u793atensor\uff0c\u4e0e\u6b64\u7c7b\u4f3c\u7684\u6709\uff0c computational graph # https://www.codingame.com/playgrounds/9487/deep-learning-from-scratch---theory-and-implementation/computational-graphs http://colah.github.io/posts/2015-08-Backprop/ http://www.cs.columbia.edu/~mcollins/ff2.pdf Knowledge Graph # word graph # In jieba , based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations. probabilistic graphical model # Tree diagram (probability theory) Automata theory # Finite-state machine # TOURNAMENTS # We now give some examples that show how graphs can also be used to model different kinds of tournaments. Round-Robin Tournaments # A tournament where each team plays every other team exactly once and no ties are allowed is called a round-robin tournament . Such tournaments can be modeled using directed graphs where each team is represented by a vertex. Note that (a,b) is an edge if team a beats team b . This graph is a simple directed graph, containing no loops or multiple directed edges (because no two teams play each other more than once). Such a directed graph model is presented in Figure 13. We see that Team 1 is undefeated in this tournament, and Team 3 is winless. Single-Elimination Tournaments # A tournament where each contestant is eliminated after one loss is called a single-elimination tournament. Single-elimination tournaments are often used in sports, including tennis championships and the yearly NCAA basketball championship. We can model such a tournament using a vertex to represent each game and a directed edge to connect a game to the next game the winner of this game played in. The graph in Figure 14 represents the games played by the final 16 teams in the 2010 NCAA women\u2019s basketball tournament. NOTE: Bracket (tournament) More # \u66f4\u591a\u56fe\u6a21\u578b\u53c2\u89c1 Application-specific graphs","title":"Graph-Model"},{"location":"Graph/02-Graph-Model/Graph-Model/#graph_models","text":"This mainly includes application-specific graphs.","title":"Graph Models"},{"location":"Graph/02-Graph-Model/Graph-Model/#communication_networks","text":"We can model different communications networks using vertices to represent devices and edges to represent the particular type of communications links of interest.","title":"COMMUNICATION NETWORKS"},{"location":"Graph/02-Graph-Model/Graph-Model/#software_design_applications","text":"NOTE: graph\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5176\u4ed6\u7684\u5b66\u79d1\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u6211\u5c06\u6b64\u7bc7\u4f5c\u4e3a\u6211\u603b\u7ed3\u5404\u79cd\u5404\u6837\u7684graph model\u3002\u4e3a\u4ec0\u4e48\u4f7f\u7528graph\u6765\u63cf\u8ff0\u95ee\u9898\u5462\uff1f\u6211\u89c9\u5f97\u56fe\u6709\u5982\u4e0b\u4f18\u52bf\uff1a\u56fe\u80fd\u591f\u975e\u5e38\u597d\u5730\u63cf\u8ff0\u5b9e\u4f53(vertex)\u548c\u5b9e\u4f53\u4e4b\u95f4\u5173\u7cfb(edge)\u3002 \u8fd9\u79cd\u5173\u7cfb\u53ef\u80fd\u662f\u62bd\u8c61\u7684dependency\uff0cprecedence\uff0c\u4e5f\u53ef\u80fd\u8868\u793a\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff08\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb\uff09\u3002 \u56fe\u80fd\u591f\u63cf\u8ff0\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff0c\u5176\u4e2d\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb \u4f7f\u7528\u56fe\u6765\u5efa\u6a21\u540e\uff0c\u5f88\u591a\u95ee\u9898\u7684\u89e3\u51b3\u5c31\u53d8\u5f97\u7b80\u5355\u4e86\u3002 Graph models are useful tools in the design of software. We will briefly describe two of these models here.","title":"SOFTWARE DESIGN APPLICATIONS"},{"location":"Graph/02-Graph-Model/Graph-Model/#dependency_graph","text":"Module Dependency Graphs One of the most important tasks in designing software is how to structure a program into different parts,or modules. Understanding how the different modules of a program interact is essential not only for program design, but also for testing and maintenance of the resulting software. A module dependency graph provides a useful tool for understanding how different modules of a program interact. In a program dependency graph, each module is represented by a vertex. There is a directed edge from a module to a second module if the second module depends on the first. An example of a program dependency graph for a web browser is shown in Figure 9. NOTE: \u4f7f\u7528edge\u6765\u8868\u793adependency\u5173\u7cfb\uff0c\u5982\u4e0b\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5173\u4e8e\u8868\u8fbe\u4f9d\u8d56\u5173\u7cfb\u7684\u56fe\u7684\u6587\u7ae0\uff1a Dependency graph Wait-for graph","title":"Dependency Graph"},{"location":"Graph/02-Graph-Model/Graph-Model/#precedence_graph","text":"Precedence Graphs and Concurrent Processing Computer programs can be executed more rapidly by executing certain statements concurrently. It is important not to execute a statement that requires results of statements not yet executed. The dependence of statements on previous statements can be represented by a directed graph. Each statement is represented by a vertex, and there is an edge from one statement to a second statement if the second statement cannot be executed before the first statement. This resulting graph is called a precedence graph . A computer program and its graph are displayed in Figure 10. For instance, the graph shows that statement S5 cannot be executed before statements S1 , S2 , and S4 are executed. NOTE: \u4f7f\u7528edge\u6765\u8868\u793aprecedence\u5173\u7cfb\u3002precedence\u5173\u7cfb VS dependency\u5173\u7cfb\uff1f Precedence graph Concurrency control Serializability Serialization Conflict Serializability in DBMS NOTE: Wait-for graph vs Precedence graph ?\u4e24\u8005\u90fd\u5728concurrent control\u76f8\u5173\u95ee\u9898\u4e2d\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u6bd4\u8f83\u4e00\u4e0b\u5b83\u4eec\u3002","title":"Precedence graph"},{"location":"Graph/02-Graph-Model/Graph-Model/#control-flow_graph","text":"NOTE: \u4f7f\u7528edge\u6765\u8868\u793acontrol\u7684flow","title":"Control-flow graph"},{"location":"Graph/02-Graph-Model/Graph-Model/#dataflow","text":"NOTE:\u4f7f\u7528edge\u6765\u8868\u793adata\u7684flow Dataflow programming Data-flow analysis TensorFlow : A machine-learning library based on dataflow programming. \u5728tensorflow\u4e2d\uff0cnode\u8868\u793aoperation\uff0cedge\u8868\u793atensor\uff0c\u4e0e\u6b64\u7c7b\u4f3c\u7684\u6709\uff0c","title":"Dataflow"},{"location":"Graph/02-Graph-Model/Graph-Model/#computational_graph","text":"https://www.codingame.com/playgrounds/9487/deep-learning-from-scratch---theory-and-implementation/computational-graphs http://colah.github.io/posts/2015-08-Backprop/ http://www.cs.columbia.edu/~mcollins/ff2.pdf","title":"computational graph"},{"location":"Graph/02-Graph-Model/Graph-Model/#knowledge_graph","text":"","title":"Knowledge Graph"},{"location":"Graph/02-Graph-Model/Graph-Model/#word_graph","text":"In jieba , based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations.","title":"word graph"},{"location":"Graph/02-Graph-Model/Graph-Model/#probabilistic_graphical_model","text":"Tree diagram (probability theory)","title":"probabilistic graphical model"},{"location":"Graph/02-Graph-Model/Graph-Model/#automata_theory","text":"","title":"Automata theory"},{"location":"Graph/02-Graph-Model/Graph-Model/#finite-state_machine","text":"","title":"Finite-state machine"},{"location":"Graph/02-Graph-Model/Graph-Model/#tournaments","text":"We now give some examples that show how graphs can also be used to model different kinds of tournaments.","title":"TOURNAMENTS"},{"location":"Graph/02-Graph-Model/Graph-Model/#round-robin_tournaments","text":"A tournament where each team plays every other team exactly once and no ties are allowed is called a round-robin tournament . Such tournaments can be modeled using directed graphs where each team is represented by a vertex. Note that (a,b) is an edge if team a beats team b . This graph is a simple directed graph, containing no loops or multiple directed edges (because no two teams play each other more than once). Such a directed graph model is presented in Figure 13. We see that Team 1 is undefeated in this tournament, and Team 3 is winless.","title":"Round-Robin Tournaments"},{"location":"Graph/02-Graph-Model/Graph-Model/#single-elimination_tournaments","text":"A tournament where each contestant is eliminated after one loss is called a single-elimination tournament. Single-elimination tournaments are often used in sports, including tennis championships and the yearly NCAA basketball championship. We can model such a tournament using a vertex to represent each game and a directed edge to connect a game to the next game the winner of this game played in. The graph in Figure 14 represents the games played by the final 16 teams in the 2010 NCAA women\u2019s basketball tournament. NOTE: Bracket (tournament)","title":"Single-Elimination Tournaments"},{"location":"Graph/02-Graph-Model/Graph-Model/#more","text":"\u66f4\u591a\u56fe\u6a21\u578b\u53c2\u89c1 Application-specific graphs","title":"More"},{"location":"Graph/02-Graph-Model/Dependency-graph/Dependency-graph/","text":"Dependency graph Dependency graph #","title":"Dependency-graph"},{"location":"Graph/02-Graph-Model/Dependency-graph/Dependency-graph/#dependency_graph","text":"","title":"Dependency graph"},{"location":"Graph/02-Graph-Model/Dependency-graph/Dependency-resolving-algorithm/","text":"Dependency Resolving Algorithm # Algorithm for dependency resolution #","title":"Dependency-resolving-algorithm"},{"location":"Graph/02-Graph-Model/Dependency-graph/Dependency-resolving-algorithm/#dependency_resolving_algorithm","text":"","title":"Dependency Resolving Algorithm"},{"location":"Graph/02-Graph-Model/Dependency-graph/Dependency-resolving-algorithm/#algorithm_for_dependency_resolution","text":"","title":"Algorithm for dependency resolution"},{"location":"Graph/02-Graph-Model/Dependency-graph/Precedence-graph/","text":"Precedence graph Precedence graph #","title":"Precedence-graph"},{"location":"Graph/02-Graph-Model/Dependency-graph/Precedence-graph/#precedence_graph","text":"","title":"Precedence graph"},{"location":"Graph/03-Graph-operation/Graph-operation/","text":"Graph operations # Graph operations produce new graphs from initial ones. They may be separated into the following major categories. Unary operations # Unary operations create a new graph from one initial one. Elementary operations # Elementary operations or editing operations create a new graph from one initial one by a simple local change, such as addition or deletion of a vertex or of an edge, merging and splitting of vertices, edge contraction , etc. The graph edit distance between a pair of graphs is the minimum number of elementary operations required to transform one graph into the other. Advanced operations # Advanced operations create a new graph from one initial one by a complex changes, such as: transpose graph ; complement graph ; line graph ; graph minor ; graph rewriting ; power of graph ; dual graph ; medial graph ; quotient graph ; Y-\u0394 transform ; Mycielskian . Binary operations # Binary operations create a new graph from two initial ones G 1 = ( V 1, E 1) and G 2 = ( V 2, E 2), such as: graph union: G 1 \u222a G 2. There are two definitions. In the most common one, the disjoint union of graphs , the union is assumed to be disjoint. Less commonly (though more consistent with the general definition of union in mathematics) the union of two graphs is defined as the graph ( V 1 \u222a V 2, E 1 \u222a E 2). \u56fe\u64cd\u4f5c\uff1a serialize/topological sorting\uff0c\u53c2\u89c1 Topological sorting C3 linearization shortest path Spanning tree see more: Computational problems in graph theory","title":"Graph-operation"},{"location":"Graph/03-Graph-operation/Graph-operation/#graph_operations","text":"Graph operations produce new graphs from initial ones. They may be separated into the following major categories.","title":"Graph operations"},{"location":"Graph/03-Graph-operation/Graph-operation/#unary_operations","text":"Unary operations create a new graph from one initial one.","title":"Unary operations"},{"location":"Graph/03-Graph-operation/Graph-operation/#elementary_operations","text":"Elementary operations or editing operations create a new graph from one initial one by a simple local change, such as addition or deletion of a vertex or of an edge, merging and splitting of vertices, edge contraction , etc. The graph edit distance between a pair of graphs is the minimum number of elementary operations required to transform one graph into the other.","title":"Elementary operations"},{"location":"Graph/03-Graph-operation/Graph-operation/#advanced_operations","text":"Advanced operations create a new graph from one initial one by a complex changes, such as: transpose graph ; complement graph ; line graph ; graph minor ; graph rewriting ; power of graph ; dual graph ; medial graph ; quotient graph ; Y-\u0394 transform ; Mycielskian .","title":"Advanced operations"},{"location":"Graph/03-Graph-operation/Graph-operation/#binary_operations","text":"Binary operations create a new graph from two initial ones G 1 = ( V 1, E 1) and G 2 = ( V 2, E 2), such as: graph union: G 1 \u222a G 2. There are two definitions. In the most common one, the disjoint union of graphs , the union is assumed to be disjoint. Less commonly (though more consistent with the general definition of union in mathematics) the union of two graphs is defined as the graph ( V 1 \u222a V 2, E 1 \u222a E 2). \u56fe\u64cd\u4f5c\uff1a serialize/topological sorting\uff0c\u53c2\u89c1 Topological sorting C3 linearization shortest path Spanning tree see more: Computational problems in graph theory","title":"Binary operations"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/","text":"Computational problems in graph theory Spanning tree Topological sorting Pre-topological order C3 linearization Graph coloring path related problem Eulerian path Hamiltonian path Shortest path problem Longest path problem Tree decomposition Graph algorithms Reachability Connectivity (graph theory) TODO Computational problems in graph theory # Spanning tree # Topological sorting # Pre-topological order # C3 linearization # Graph coloring # path related problem # Eulerian path # Hamiltonian path # Hamiltonian path problem Hamiltonian cycle polynomial Shortest path problem # Longest path problem # Tree decomposition # Junction tree algorithm Graph algorithms # Graph algorithms solve problems related to graph theory . Reachability # Connectivity (graph theory) # TODO # Directed graph traversal, orderings and applications to data-flow analysis Control-flow graph","title":"Computational-problems-in-graph-theory"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#computational_problems_in_graph_theory","text":"","title":"Computational problems in graph theory"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#spanning_tree","text":"","title":"Spanning tree"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#topological_sorting","text":"","title":"Topological sorting"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#pre-topological_order","text":"","title":"Pre-topological order"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#c3_linearization","text":"","title":"C3 linearization"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#graph_coloring","text":"","title":"Graph coloring"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#path_related_problem","text":"","title":"path related problem"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#eulerian_path","text":"","title":"Eulerian path"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#hamiltonian_path","text":"Hamiltonian path problem Hamiltonian cycle polynomial","title":"Hamiltonian path"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#shortest_path_problem","text":"","title":"Shortest path problem"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#longest_path_problem","text":"","title":"Longest path problem"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#tree_decomposition","text":"Junction tree algorithm","title":"Tree decomposition"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#graph_algorithms","text":"Graph algorithms solve problems related to graph theory .","title":"Graph algorithms"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#reachability","text":"","title":"Reachability"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#connectivity_graph_theory","text":"","title":"Connectivity (graph theory)"},{"location":"Graph/04-Computational-problems-in-graph-theory/Computational-problems-in-graph-theory/#todo","text":"Directed graph traversal, orderings and applications to data-flow analysis Control-flow graph","title":"TODO"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/","text":"Circular dependency Acyclic dependencies principle Resolve build errors due to circular dependency amongst classes Circular dependencies in C++ Circular dependency # Acyclic dependencies principle # Resolve build errors due to circular dependency amongst classes # Circular dependencies in C++ #","title":"Circular-dependency"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#circular_dependency","text":"","title":"Circular dependency"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#acyclic_dependencies_principle","text":"","title":"Acyclic dependencies principle"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#resolve_build_errors_due_to_circular_dependency_amongst_classes","text":"","title":"Resolve build errors due to circular dependency amongst classes"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#circular_dependencies_in_c","text":"","title":"Circular dependencies in C++"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Cycle-detection/","text":"Cycle detection # Detect Cycle in a Directed Graph #","title":"Cycle-detection"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Cycle-detection/#cycle_detection","text":"","title":"Cycle detection"},{"location":"Graph/04-Computational-problems-in-graph-theory/Circular/Cycle-detection/#detect_cycle_in_a_directed_graph","text":"","title":"Detect Cycle in a Directed Graph"},{"location":"Graph/04-Computational-problems-in-graph-theory/Dominator/Dominator(graph-theory)/","text":"Dominator (graph theory) #","title":"Dominator"},{"location":"Graph/04-Computational-problems-in-graph-theory/Dominator/Dominator(graph-theory)/#dominator_graph_theory","text":"","title":"Dominator (graph theory)"},{"location":"Graph/04-Computational-problems-in-graph-theory/Sorting/Topological-sorting/","text":"Topological sorting #","title":"Topological-sorting"},{"location":"Graph/04-Computational-problems-in-graph-theory/Sorting/Topological-sorting/#topological_sorting","text":"","title":"Topological sorting"},{"location":"Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Bor\u016fvka's-algorithm/","text":"","title":"Bor\u016fvka's-algorithm"},{"location":"Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Minimum-spanning-tree/","text":"","title":"Minimum-spanning-tree"},{"location":"Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Spanning-tree/","text":"Spanning tree #","title":"Spanning-tree"},{"location":"Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Spanning-tree/#spanning_tree","text":"","title":"Spanning tree"},{"location":"Hash/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u63cf\u8ff0hash\u76f8\u5173\u5185\u5bb9\uff0c\u5185\u5bb9\u8f83\u591a\uff0c\u6b63\u5728\u6574\u7406\u4e2d\u3002","title":"Introduction"},{"location":"Hash/#_1","text":"\u672c\u7ae0\u63cf\u8ff0hash\u76f8\u5173\u5185\u5bb9\uff0c\u5185\u5bb9\u8f83\u591a\uff0c\u6b63\u5728\u6574\u7406\u4e2d\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Hash/application-of-hash/","text":"liblex Symbol table \u5728python\u4e2d\u6709hashable\u7684\u6982\u5ff5 TODO # https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/ hash\u7528\u4e8e\u53bb\u91cd https://stackoverflow.com/questions/3435616/c-string-compare-vs-hash-compare https://stackoverflow.com/questions/10534937/comparing-long-strings-by-their-hashes https://en.wikipedia.org/wiki/Rolling_hash https://cs.stackexchange.com/questions/51933/string-comparison-vs-hashing https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm Hashing - The Greatest Idea In Programming # https://www.i-programmer.info/babbages-bag/479-hashing.html https://www.i-programmer.info/babbages-bag/479-hashing.html?start=1 Applications of Hashing # https://www.geeksforgeeks.org/applications-of-hashing/ python\u4e2d\u7684symbol table\u662f\u4f7f\u7528\u7684hash \u5417\uff1f #","title":"Application of hash"},{"location":"Hash/application-of-hash/#todo","text":"https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/ hash\u7528\u4e8e\u53bb\u91cd https://stackoverflow.com/questions/3435616/c-string-compare-vs-hash-compare https://stackoverflow.com/questions/10534937/comparing-long-strings-by-their-hashes https://en.wikipedia.org/wiki/Rolling_hash https://cs.stackexchange.com/questions/51933/string-comparison-vs-hashing https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm","title":"TODO"},{"location":"Hash/application-of-hash/#hashing_-_the_greatest_idea_in_programming","text":"https://www.i-programmer.info/babbages-bag/479-hashing.html https://www.i-programmer.info/babbages-bag/479-hashing.html?start=1","title":"Hashing - The Greatest Idea In Programming"},{"location":"Hash/application-of-hash/#applications_of_hashing","text":"https://www.geeksforgeeks.org/applications-of-hashing/","title":"Applications of Hashing"},{"location":"Hash/application-of-hash/#pythonsymbol_tablehash","text":"","title":"python\u4e2d\u7684symbol table\u662f\u4f7f\u7528\u7684hash \u5417\uff1f"},{"location":"Hash/geeksforgeeks-Top-20-Hashing-Technique-based-Interview-Questions/","text":"","title":"geeksforgeeks Top 20 Hashing Technique based Interview Questions"},{"location":"Hash/Checksum/VS-check-sum-VS-cryptographic-hash/","text":"","title":"VS check sum VS cryptographic hash"},{"location":"Hash/Checksum/wikipedia-Checksum/","text":"Checksum # A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors that may have been introduced during its transmission or storage . By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity . The procedure which generates this checksum is called a checksum function or checksum algorithm . Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions , which may be used to detect many data corruption errors and verify overall data integrity ; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted. NOTE: Hash code can be used as checksum. Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers , bank account numbers, computer words , single bytes , etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases. \u8865\u5145 # What Is a Checksum (and Why Should You Care)? # Git Has Integrity # \u5728 Pro Git book \u7684 1.3 Getting Started - What is Git? \u8282\u4e2d\u63cf\u8ff0\u4e86git\u4e2dchecksum\u7684\u4f7f\u7528\u60c5\u51b5\u3002 Practical Application of Cryptographic Checksums # \u603b\u7ed3\u7684\u975e\u5e38\u597d","title":"[Checksum](https://en.wikipedia.org/wiki/Checksum)"},{"location":"Hash/Checksum/wikipedia-Checksum/#checksum","text":"A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors that may have been introduced during its transmission or storage . By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity . The procedure which generates this checksum is called a checksum function or checksum algorithm . Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions , which may be used to detect many data corruption errors and verify overall data integrity ; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted. NOTE: Hash code can be used as checksum. Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers , bank account numbers, computer words , single bytes , etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.","title":"Checksum"},{"location":"Hash/Checksum/wikipedia-Checksum/#_1","text":"","title":"\u8865\u5145"},{"location":"Hash/Checksum/wikipedia-Checksum/#what_is_a_checksum_and_why_should_you_care","text":"","title":"What Is a Checksum (and Why Should You Care)?"},{"location":"Hash/Checksum/wikipedia-Checksum/#git_has_integrity","text":"\u5728 Pro Git book \u7684 1.3 Getting Started - What is Git? \u8282\u4e2d\u63cf\u8ff0\u4e86git\u4e2dchecksum\u7684\u4f7f\u7528\u60c5\u51b5\u3002","title":"Git Has Integrity"},{"location":"Hash/Checksum/wikipedia-Checksum/#practical_application_of_cryptographic_checksums","text":"\u603b\u7ed3\u7684\u975e\u5e38\u597d","title":"Practical Application of Cryptographic Checksums"},{"location":"Hash/Error-detection-and-correction/Error-detection-and-correction/","text":"Error detection and correction #","title":"[Error detection and correction](https://en.wikipedia.org/wiki/Category:Error_detection_and_correction)"},{"location":"Hash/Error-detection-and-correction/Error-detection-and-correction/#error_detection_and_correction","text":"","title":"Error detection and correction"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/","text":"Hashing Algorithms to Find Duplicates # Finding duplicate files is a hectic task when you have millions of files spread all over your computer. To check if two files are duplicates of each other, we should do a one-to-one check for the suspect files. Even though this is simple for smaller files, performing a one-to-one (byte-to-byte) comparison for large files is a massively time-consuming task. Imagine comparing two gigantic files to check whether they are duplicates, the time and the effort you need to spare for undertaking such a task is probably enough to simply make you give up. An easy way to remove duplicates would be to utilize an all-purpose duplicate cleaner such as the Clone Files Checker . It works on the basis of highly advanced algorithms and saves your time in organizing files. Let\u2019s imagine that we are downloading a large file over the Internet and we need to make sure that the file was not modified by a third-party in the midst of the transmission, which may cause a man-in-the-middle attack. Checking each byte over the Internet would take years to accomplish and also eat up a significant portion of your Internet bandwidth. So, how can we easily compare two files in such manner that is credible yet not at all laborious? There are multiple ways to do so, including Hash Comparison which is turning out to be a highly popular and reliable. What is a hash? # Hash is a mathematical function which takes objects as inputs and produces as output a number or a string. These input objects can vary from numbers and strings (text) to large byte streams (files or network data). The output is usually known as the hash of the input. The main properties of a hash function are: Easy to compute Relatively small output Example We can define a hash function to get the hash of the numbers between 0 to 9999. h(x) = x mod 100 In the above-mentioned hash function, you can see that there is a significant probability of getting the same hash (collision) for two different inputs. (i.e. 1 and 1001). So, it is always good to have a better hash function with fewer collisions, which makes it difficult to find two inputs which give the same output. While hashing is used in many applications such as hash-tables (data structure), compression and encryption, it is also used to generate checksums of files, network data and many other input types. Checksum # The checksum is a small sized datum, generated by applying a hash function on a large chunk of data. This hash function should have a minimum rate of collisions such that the hashes for different inputs are almost unique. That means, getting the same hash for different inputs is nearly impossible in practice. These checksums are used to verify if a segment of data has been modified. The checksum (from a known hash function) of received data can be compared with the checksum provided by the sender to verify the purity of the segment. That is how all the data is verified in TCP/IP protocols. In this way, if we generate two checksums for two files, we can declare that the two files aren\u2019t duplicates if the checksums are different. If the checksums are equal, we can claim that the files are identical, considering the fact that getting the same hash for two different files is almost impossible. And many websites provide hashes of the files at the download pages, especially when the files are located on different servers. In such a scenario, the user can verify the authenticity of a file by comparing the provided hash with the one he generated using the downloaded file. There are various hashing functions that are used to generate checksums. Here are some popular ones. Name Output size MD5 128bits SHA-1 160bits SHA-256 256bits SHA-512 512bits MD5 # MD5 is a popular hashing function which was initially used for cryptographic purposes. Even though this generates 128-bit output, many people no longer use it due to a host of vulnerabilities that later surfaced. However, it still can be used as a checksum to verify the authenticity of a file (or a data segment) to detect unintentional changes/corruptions. SHA-1 # SHA 1 ( Secure Hash Algorithm 1 ) is a cryptographic hashing function which generates a 160-bit output. This is no longer considered as secure after many attacks, web browsers already have stopped accepting SLL Certificates based on SHA-1 . The later versions of SHA checksums ( SHA-256 and SHA-512 ) are more secure as they incorporate vast improvements and hence don\u2019t contain any vulnerabilities as oft he time this article was published. Features of a strong hash function # Should be open \u2013 Everybody should know how the hashing is performed Easy to generate \u2013 Should not take too much time to generate the output Fewer collisions \u2013 The probability of getting the same hash for two inputs should be near to zero Hard to back-trace \u2013 Given a hash, recovering the original input should not be possible Hash MD5 Check \u2013 The Best Method to Find Duplicate Files # One of the biggest problems of every computer user is the piling up of junk in their system. This happens due to many factors, and the creation of duplicate files is one of them. Duplicate files decrease the performance of a system drastically. They aren\u2019t created by the user intentionally, but nevertheless, they will clutter the hard drive and cause disorganization of data without the user even knowing about them. Luckily enough, some genius people have worked endlessly to come up with unique state-of-the-art methods that make scanning for and getting rid of duplicate data of all kinds a very straightforward matter. So let\u2019s jump into some detail of the mechanism that is employed to scan for duplicates and then we shall move on to a genius solution of its own kind. But let\u2019s just introduce it to you before we go into more details. Clone Files Checker is its name, and its job is to go after those pesky duplicates on Windows as well as Mac systems! Why Use MD5 Check to find Duplicate Files? # The history of MD5 starts from 1991 when the systems had already been upgraded to 128-bit data transfer rates and a security feature that could encrypt 128-bit data slot was very much needed. MD5 was initially used for cryptographic purposes only, but the discovery of several vulnerabilities meant it was put out of service soon after. However, the algorithm is still used to verify the authenticity of a file, and to know if it has been through an event of data corruption. The benefits of MD5 check over other cryptic algorithms is that it can be implemented faster than the other cryptic algorithms and provides an impressive performance increase for verifying data in comparison with SHA1 , SHA2 and SHA3 cryptic software. However, MD5 is vulnerable to collision resistance. But, the good point is that in finding duplicate files, collision resistance is not really an issue. Collision resistance refers to two inputs providing the same outputs when hashed. But A good MD5 hash program will work in unison with the file size, type, and the last byte value. This is because it is common for various parameters of some files to be similar/ identical/ different (size, name etc) but the hash value will always be the same (provided these files are duplicates). This absolutely does away with the chances of deleting a file which isn\u2019t a duplicate while cleaning up duplicate data. Therefore, MD5 is a huge blessing while on the lookout for duplicates. Now let\u2019s take a brief look at a software that uses MD5 to weed out duplicates swiftly and accurately. Clone Files Checker # This software is simply put, a death sentence for duplicate data. Be it duplicate data of any kind, located on your local computer, external hard drive or even your cloud account, this software will dive into great details and make exceptional use of MD5 Check to bring up a list of all the duplicate data that had eaten into your previous hard drive and was beginning to slow down your system. From then onwards, it is pretty simple for you to choose between deleting the duplicates permanently or moving them to a separate folder. But it is all due to the goodness offered by MD5 Check and the genius brains behind Clone Files Checker that you can be assured to a swift and accurate scan that will help free up a big quantum of your hard drive. C++ Coding Example, Using Hash Algorithm to Remove Duplicate Number by O(n) #","title":"[Hashing Algorithms to Find Duplicates](https://www.clonefileschecker.com/blog/hashing-algorithms-to-find-duplicates/)"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#hashing_algorithms_to_find_duplicates","text":"Finding duplicate files is a hectic task when you have millions of files spread all over your computer. To check if two files are duplicates of each other, we should do a one-to-one check for the suspect files. Even though this is simple for smaller files, performing a one-to-one (byte-to-byte) comparison for large files is a massively time-consuming task. Imagine comparing two gigantic files to check whether they are duplicates, the time and the effort you need to spare for undertaking such a task is probably enough to simply make you give up. An easy way to remove duplicates would be to utilize an all-purpose duplicate cleaner such as the Clone Files Checker . It works on the basis of highly advanced algorithms and saves your time in organizing files. Let\u2019s imagine that we are downloading a large file over the Internet and we need to make sure that the file was not modified by a third-party in the midst of the transmission, which may cause a man-in-the-middle attack. Checking each byte over the Internet would take years to accomplish and also eat up a significant portion of your Internet bandwidth. So, how can we easily compare two files in such manner that is credible yet not at all laborious? There are multiple ways to do so, including Hash Comparison which is turning out to be a highly popular and reliable.","title":"Hashing Algorithms to Find Duplicates"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#what_is_a_hash","text":"Hash is a mathematical function which takes objects as inputs and produces as output a number or a string. These input objects can vary from numbers and strings (text) to large byte streams (files or network data). The output is usually known as the hash of the input. The main properties of a hash function are: Easy to compute Relatively small output Example We can define a hash function to get the hash of the numbers between 0 to 9999. h(x) = x mod 100 In the above-mentioned hash function, you can see that there is a significant probability of getting the same hash (collision) for two different inputs. (i.e. 1 and 1001). So, it is always good to have a better hash function with fewer collisions, which makes it difficult to find two inputs which give the same output. While hashing is used in many applications such as hash-tables (data structure), compression and encryption, it is also used to generate checksums of files, network data and many other input types.","title":"What is a hash?"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#checksum","text":"The checksum is a small sized datum, generated by applying a hash function on a large chunk of data. This hash function should have a minimum rate of collisions such that the hashes for different inputs are almost unique. That means, getting the same hash for different inputs is nearly impossible in practice. These checksums are used to verify if a segment of data has been modified. The checksum (from a known hash function) of received data can be compared with the checksum provided by the sender to verify the purity of the segment. That is how all the data is verified in TCP/IP protocols. In this way, if we generate two checksums for two files, we can declare that the two files aren\u2019t duplicates if the checksums are different. If the checksums are equal, we can claim that the files are identical, considering the fact that getting the same hash for two different files is almost impossible. And many websites provide hashes of the files at the download pages, especially when the files are located on different servers. In such a scenario, the user can verify the authenticity of a file by comparing the provided hash with the one he generated using the downloaded file. There are various hashing functions that are used to generate checksums. Here are some popular ones. Name Output size MD5 128bits SHA-1 160bits SHA-256 256bits SHA-512 512bits","title":"Checksum"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#md5","text":"MD5 is a popular hashing function which was initially used for cryptographic purposes. Even though this generates 128-bit output, many people no longer use it due to a host of vulnerabilities that later surfaced. However, it still can be used as a checksum to verify the authenticity of a file (or a data segment) to detect unintentional changes/corruptions.","title":"MD5"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#sha-1","text":"SHA 1 ( Secure Hash Algorithm 1 ) is a cryptographic hashing function which generates a 160-bit output. This is no longer considered as secure after many attacks, web browsers already have stopped accepting SLL Certificates based on SHA-1 . The later versions of SHA checksums ( SHA-256 and SHA-512 ) are more secure as they incorporate vast improvements and hence don\u2019t contain any vulnerabilities as oft he time this article was published.","title":"SHA-1"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#features_of_a_strong_hash_function","text":"Should be open \u2013 Everybody should know how the hashing is performed Easy to generate \u2013 Should not take too much time to generate the output Fewer collisions \u2013 The probability of getting the same hash for two inputs should be near to zero Hard to back-trace \u2013 Given a hash, recovering the original input should not be possible","title":"Features of a strong hash function"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#hash_md5_check_the_best_method_to_find_duplicate_files","text":"One of the biggest problems of every computer user is the piling up of junk in their system. This happens due to many factors, and the creation of duplicate files is one of them. Duplicate files decrease the performance of a system drastically. They aren\u2019t created by the user intentionally, but nevertheless, they will clutter the hard drive and cause disorganization of data without the user even knowing about them. Luckily enough, some genius people have worked endlessly to come up with unique state-of-the-art methods that make scanning for and getting rid of duplicate data of all kinds a very straightforward matter. So let\u2019s jump into some detail of the mechanism that is employed to scan for duplicates and then we shall move on to a genius solution of its own kind. But let\u2019s just introduce it to you before we go into more details. Clone Files Checker is its name, and its job is to go after those pesky duplicates on Windows as well as Mac systems!","title":"Hash MD5 Check \u2013 The Best Method to Find Duplicate Files"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#why_use_md5_check_to_find_duplicate_files","text":"The history of MD5 starts from 1991 when the systems had already been upgraded to 128-bit data transfer rates and a security feature that could encrypt 128-bit data slot was very much needed. MD5 was initially used for cryptographic purposes only, but the discovery of several vulnerabilities meant it was put out of service soon after. However, the algorithm is still used to verify the authenticity of a file, and to know if it has been through an event of data corruption. The benefits of MD5 check over other cryptic algorithms is that it can be implemented faster than the other cryptic algorithms and provides an impressive performance increase for verifying data in comparison with SHA1 , SHA2 and SHA3 cryptic software. However, MD5 is vulnerable to collision resistance. But, the good point is that in finding duplicate files, collision resistance is not really an issue. Collision resistance refers to two inputs providing the same outputs when hashed. But A good MD5 hash program will work in unison with the file size, type, and the last byte value. This is because it is common for various parameters of some files to be similar/ identical/ different (size, name etc) but the hash value will always be the same (provided these files are duplicates). This absolutely does away with the chances of deleting a file which isn\u2019t a duplicate while cleaning up duplicate data. Therefore, MD5 is a huge blessing while on the lookout for duplicates. Now let\u2019s take a brief look at a software that uses MD5 to weed out duplicates swiftly and accurately.","title":"Why Use MD5 Check to find Duplicate Files?"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#clone_files_checker","text":"This software is simply put, a death sentence for duplicate data. Be it duplicate data of any kind, located on your local computer, external hard drive or even your cloud account, this software will dive into great details and make exceptional use of MD5 Check to bring up a list of all the duplicate data that had eaten into your previous hard drive and was beginning to slow down your system. From then onwards, it is pretty simple for you to choose between deleting the duplicates permanently or moving them to a separate folder. But it is all due to the goodness offered by MD5 Check and the genius brains behind Clone Files Checker that you can be assured to a swift and accurate scan that will help free up a big quantum of your hard drive.","title":"Clone Files Checker"},{"location":"Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#c_coding_example_using_hash_algorithm_to_remove_duplicate_number_by_on","text":"","title":"C++ Coding Example, Using Hash Algorithm to Remove Duplicate Number by O(n)"},{"location":"Hash/Fingerprint/wikipedia-Fingerprint(computing)/","text":"","title":"wikipedia Fingerprint(computing)"},{"location":"Hash/Fingerprint/wikipedia-Rabin-fingerprint/","text":"Rabin fingerprint #","title":"[Rabin fingerprint](https://en.wikipedia.org/wiki/Rabin_fingerprint)"},{"location":"Hash/Fingerprint/wikipedia-Rabin-fingerprint/#rabin_fingerprint","text":"","title":"Rabin fingerprint"},{"location":"Hash/Hash-based-data-structures/Category-Hash-based-data-structures/","text":"Category:Hash based data structures #","title":"[Category:Hash based data structures](https://en.wikipedia.org/wiki/Category:Hash_based_data_structures)"},{"location":"Hash/Hash-based-data-structures/Category-Hash-based-data-structures/#categoryhash_based_data_structures","text":"","title":"Category:Hash based data structures"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/VS-consistent-hash-VS-rendezvous-hash/","text":"","title":"VS consistent hash VS rendezvous hash"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/consistent-hash-in-redis/","text":"redis partition # https://redis.io/topics/partitioning redis cluster # https://redis.io/topics/cluster-tutorial Redis Cluster does not use consistent hashing , but a different form of sharding where every key is conceptually part of what we call an hash slot.","title":"redis partition"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/consistent-hash-in-redis/#redis_partition","text":"https://redis.io/topics/partitioning","title":"redis partition"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/consistent-hash-in-redis/#redis_cluster","text":"https://redis.io/topics/cluster-tutorial Redis Cluster does not use consistent hashing , but a different form of sharding where every key is conceptually part of what we call an hash slot.","title":"redis cluster"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-cache/","text":"Distributed cache Examples Distributed cache # In computing , a distributed cache is an extension of the traditional concept of cache used in a single locale . A distributed cache may span multiple servers so that it can grow in size and in transactional capacity. It is mainly used to store application data residing in database and web session data. The idea of distributed caching[ 1] has become feasible now because main memory has become very cheap and network cards have become very fast, with 1 Gbit now standard everywhere and 10 Gbit gaining traction. Also, a distributed cache works well on lower cost machines usually employed for web servers as opposed to database servers which require expensive hardware.[ 2] An emerging internet architecture known as Information-centric networking (ICN) is one of the best examples of a distributed cache network. The ICN is a network level solution hence the existing distributed network cache management schemes are not well suited for ICN.[ 3] In the supercomputer environment, distributed cache is typically implemented in the form of burst buffer . Examples # Aerospike Apache Ignite Couchbase Ehcache GigaSpaces GridGain Systems Hazelcast Infinispan Memcached NCache[ 4] Oracle Coherence Riak Redis SafePeak Tarantool Velocity / AppFabric","title":"wikipedia Distributed cache"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-cache/#distributed_cache","text":"In computing , a distributed cache is an extension of the traditional concept of cache used in a single locale . A distributed cache may span multiple servers so that it can grow in size and in transactional capacity. It is mainly used to store application data residing in database and web session data. The idea of distributed caching[ 1] has become feasible now because main memory has become very cheap and network cards have become very fast, with 1 Gbit now standard everywhere and 10 Gbit gaining traction. Also, a distributed cache works well on lower cost machines usually employed for web servers as opposed to database servers which require expensive hardware.[ 2] An emerging internet architecture known as Information-centric networking (ICN) is one of the best examples of a distributed cache network. The ICN is a network level solution hence the existing distributed network cache management schemes are not well suited for ICN.[ 3] In the supercomputer environment, distributed cache is typically implemented in the form of burst buffer .","title":"Distributed cache"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-cache/#examples","text":"Aerospike Apache Ignite Couchbase Ehcache GigaSpaces GridGain Systems Hazelcast Infinispan Memcached NCache[ 4] Oracle Coherence Riak Redis SafePeak Tarantool Velocity / AppFabric","title":"Examples"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/","text":"Distributed hash table History Properties Structure Keyspace partitioning Consistent hashing Rendezvous hashing Overlay network Algorithms for overlay networks DHT implementations Examples Distributed hash table # NOTE : redis cluster\u53ef\u4ee5\u770b\u505a\u662f Distributed hash table \uff1b A distributed hash table ( DHT ) is a class of a decentralized distributed system that provides a lookup service similar to a hash table : ( key , value ) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Keys are unique identifiers which map to particular values , which in turn can be anything from addresses, to documents , to arbitrary data .[ 1] Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption. This allows a DHT to scale to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures. DHTs form an infrastructure that can be used to build more complex services, such as anycast , cooperative Web caching , distributed file systems , domain name services , instant messaging , multicast , and also peer-to-peer file sharing and content distribution systems. Notable distributed networks that use DHTs include BitTorrent 's distributed tracker, the Coral Content Distribution Network , the Kad network , the Storm botnet , the Tox instant messenger , Freenet , the YaCy search engine, and the InterPlanetary File System . Distributed hash tables History # DHT research was originally motivated, in part, by peer-to-peer systems such as Freenet , gnutella , BitTorrent and Napster , which took advantage of resources distributed across the Internet to provide a single useful application. In particular, they took advantage of increased bandwidth and hard disk capacity to provide a file-sharing service.[ 2] DHT\u7814\u7a76\u6700\u521d\u7684\u52a8\u673a\u90e8\u5206\u662f\u7531Freenet\uff0cgnutella\uff0cBitTorrent\u548cNapster\u7b49\u5bf9\u7b49\u7cfb\u7edf\u63a8\u52a8\u7684\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5229\u7528\u5206\u5e03\u5728Internet\u4e0a\u7684\u8d44\u6e90\u6765\u63d0\u4f9b\u5355\u4e00\u6709\u7528\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u7279\u522b\u662f\uff0c\u4ed6\u4eec\u5229\u7528\u589e\u52a0\u7684\u5e26\u5bbd\u548c\u786c\u76d8\u5bb9\u91cf\u6765\u63d0\u4f9b\u6587\u4ef6\u5171\u4eab\u670d\u52a1\u3002 These systems differed in how they located the data offered by their peers. Napster, the first large-scale P2P content delivery system, required a central index server : each node, upon joining, would send a list of locally held files to the server, which would perform searches and refer the queries to the nodes that held the results. This central component left the system vulnerable to attacks and lawsuits. \u8fd9\u4e9b\u7cfb\u7edf\u5728\u5b9a\u4f4d\u540c\u884c\u63d0\u4f9b\u7684\u6570\u636e\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002 Napster\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21P2P\u5185\u5bb9\u4f20\u9001\u7cfb\u7edf\uff0c\u9700\u8981\u4e00\u4e2a\u4e2d\u592e\u7d22\u5f15\u670d\u52a1\u5668\uff1a\u6bcf\u4e2a\u8282\u70b9\u5728\u52a0\u5165\u65f6\uff0c\u4f1a\u5411\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u672c\u5730\u4fdd\u5b58\u6587\u4ef6\u5217\u8868\uff0c\u8fd9\u5c06\u6267\u884c\u641c\u7d22\u5e76\u5c06\u67e5\u8be2\u5f15\u7528\u5230\u6301\u6709\u7ed3\u679c\u3002\u8fd9\u4e00\u6838\u5fc3\u7ec4\u4ef6\u4f7f\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u548c\u8bc9\u8bbc Gnutella and similar networks moved to a flooding query model \u2013 in essence, each search would result in a message being broadcast to every other machine in the network. While avoiding a single point of failure , this method was significantly less efficient than Napster. Later versions of Gnutella clients moved to a dynamic querying model which vastly improved efficiency.[ 3] Gnutella\u548c\u7c7b\u4f3c\u7684\u7f51\u7edc\u8f6c\u79fb\u5230\u6d2a\u6cdb\u67e5\u8be2\u6a21\u578b - \u5b9e\u8d28\u4e0a\uff0c\u6bcf\u6b21\u641c\u7d22\u90fd\u4f1a\u5bfc\u81f4\u6d88\u606f\u88ab\u5e7f\u64ad\u5230\u7f51\u7edc\u4e2d\u7684\u6bcf\u53f0\u5176\u4ed6\u673a\u5668\u3002\u867d\u7136\u907f\u514d\u4e86\u5355\u70b9\u6545\u969c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u6548\u7387\u660e\u663e\u4f4e\u4e8eNapster\u3002\u66f4\u9ad8\u7248\u672c\u7684Gnutella\u5ba2\u6237\u8f6c\u5411\u52a8\u6001\u67e5\u8be2\u6a21\u578b\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u6548\u7387\u3002 Freenet is fully distributed, but employs a heuristic key-based routing in which each file is associated with a key, and files with similar keys tend to cluster on a similar set of nodes. Queries are likely to be routed through the network to such a cluster without needing to visit many peers.[ 4] However, Freenet does not guarantee that data will be found. Freenet\u662f\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\uff0c\u4f46\u91c7\u7528\u57fa\u4e8e\u5bc6\u94a5\u7684\u542f\u53d1\u5f0f\u8def\u7531\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6587\u4ef6\u4e0e\u5bc6\u94a5\u76f8\u5173\u8054\uff0c\u5177\u6709\u76f8\u4f3c\u5bc6\u94a5\u7684\u6587\u4ef6\u503e\u5411\u4e8e\u5728\u7c7b\u4f3c\u7684\u8282\u70b9\u96c6\u4e0a\u96c6\u7fa4\u3002\u67e5\u8be2\u5f88\u53ef\u80fd\u901a\u8fc7\u7f51\u7edc\u8def\u7531\u5230\u8fd9\u6837\u7684\u96c6\u7fa4\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u8bb8\u591a\u5bf9\u7b49\u4f53\u3002[4]\u4f46\u662f\uff0cFreenet\u4e0d\u4fdd\u8bc1\u4f1a\u627e\u5230\u6570\u636e\u3002 Distributed hash table s use a more structured key-based routing in order to attain both the decentralization of Freenet and gnutella, and the efficiency and guaranteed results of Napster. One drawback is that, like Freenet, DHTs only directly support exact-match search , rather than keyword search , although Freenet's routing algorithm can be generalized to any key type where a closeness operation can be defined.[ 5] \u5206\u5e03\u5f0f\u54c8\u5e0c\u8868\u4f7f\u7528\u66f4\u52a0\u7ed3\u6784\u5316\u7684\u57fa\u4e8e\u5bc6\u94a5\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0Freenet\u548cgnutella\u7684\u5206\u6563\u5316\uff0c\u4ee5\u53caNapster\u7684\u6548\u7387\u548c\u4fdd\u8bc1\u7ed3\u679c\u3002 \u4e00\u4e2a\u7f3a\u70b9\u662f\uff0c\u50cfFreenet\u4e00\u6837\uff0cDHT\u53ea\u76f4\u63a5\u652f\u6301\u7cbe\u786e\u5339\u914d\u641c\u7d22\uff0c\u800c\u4e0d\u662f\u5173\u952e\u5b57\u641c\u7d22\uff0c\u5c3d\u7ba1Freenet\u7684\u8def\u7531\u7b97\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u4efb\u4f55\u53ef\u4ee5\u5b9a\u4e49\u63a5\u8fd1\u5ea6\u64cd\u4f5c\u7684\u5bc6\u94a5\u7c7b\u578b\u3002 In 2001, four systems\u2014 CAN ,[ 6] Chord ,[ 7] Pastry , and Tapestry \u2014ignited\uff08\u6fc0\u8d77\uff09 DHTs as a popular research topic. A project called the Infrastructure for Resilient Internet Systems (Iris) was funded by a $12 million grant from the US National Science Foundation in 2002.[ 8] Researchers included Sylvia Ratnasamy , Ion Stoica , Hari Balakrishnan and Scott Shenker .[ 9] Outside academia, DHT technology has been adopted as a component of BitTorrent and in the Coral Content Distribution Network . 2001\u5e74\uff0c\u56db\u4e2a\u7cfb\u7edf-CAN\uff0c[6] Chord\uff0c[7] Pastry\u548cTapestry-\u70b9\u71c3\u4e86DHT\u4f5c\u4e3a\u4e00\u4e2a\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898\u3002 \u4e00\u4e2a\u540d\u4e3a\u201c\u5f39\u6027\u4e92\u8054\u7f51\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u201d\uff08Iris\uff09\u7684\u9879\u76ee\u75312002\u5e74\u7f8e\u56fd\u56fd\u5bb6\u79d1\u5b66\u57fa\u91d1\u4f1a\u62e8\u6b3e1200\u4e07\u7f8e\u5143\u8d44\u52a9\u3002[8] \u7814\u7a76\u4eba\u5458\u5305\u62ecSylvia Ratnasamy\uff0cIon Stoica\uff0cHari Balakrishnan\u548cScott Shenker\u3002[9] \u5728\u5b66\u672f\u754c\u4e4b\u5916\uff0cDHT\u6280\u672f\u5df2\u88ab\u91c7\u7eb3\u4e3aBitTorrent\u548cCoral\u5185\u5bb9\u5206\u53d1\u7f51\u7edc\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002 Properties # DHTs characteristically emphasize the following properties: Autonomy\uff08\u81ea\u6cbb\u7684\uff09 and decentralization : the nodes collectively form the system without any central coordination . Fault tolerance : the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing. Scalability : the system should function efficiently even with thousands or millions of nodes. SUMMARY : redis cluster\u4e5f\u5177\u6709\u4e0a\u8ff0\u7684property A key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system \u2013 most commonly, O (log n ) of the $ n $ participants (see below) \u2013 so that only a limited amount of work needs to be done for each change in membership. Some DHT designs seek to be secure against malicious\uff08\u6076\u610f\u7684\uff09 participants[ 10] and to allow participants to remain anonymous , though this is less common than in many other peer-to-peer (especially file sharing ) systems; see anonymous P2P . Finally, DHTs must deal with more traditional distributed systems issues such as load balancing , data integrity , and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly). Structure # The structure of a DHT can be decomposed into several main components.[ 11] [ 12] The foundation is an abstract keyspace , such as the set of 160-bit strings . A keyspace partitioning scheme splits ownership of this keyspace among the participating nodes. An overlay network then connects the nodes, allowing them to find the owner of any given key in the keyspace . Once these components are in place, a typical use of the DHT for storage and retrieval might proceed as follows. Suppose the keyspace is the set of 160-bit strings. To index a file with given filename and data in the DHT, the SHA-1 hash of filename is generated, producing a 160-bit key k , and a message put ( k, data ) is sent to any node participating in the DHT. The message is forwarded from node to node through the overlay network until it reaches the single node responsible for key k as specified by the keyspace partitioning . That node then stores the key and the data. Any other client can then retrieve the contents of the file by again hashing filename to produce k and asking any DHT node to find the data associated with k with a message get ( k ). The message will again be routed through the overlay to the node responsible for k , which will reply with the stored data . SUMMARY : \u4e0a\u8ff0\u662f\u4f7f\u7528 SHA-1 hash \u6765\u83b7\u5f97abstract keyspace\uff1b\u4f7f\u7528 Consistent hashing \u3001 Rendezvous hashing \u6765\u8fdb\u884ckeyspace partition\uff1b SUMMARY : redis cluster \u662f\u5426\u5b58\u5728\u4e0a\u8ff0forward\u7684\u8fc7\u7a0b\uff1f The keyspace partitioning and overlay network components are described below with the goal of capturing the principal ideas common to most DHTs; many designs differ in the details. Keyspace partitioning # Most DHTs use some variant of consistent hashing or rendezvous hashing to map keys to nodes. The two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem. Both consistent hashing and rendezvous hashing have the essential property that removal or addition of one node changes only the set of keys owned by the nodes with adjacent IDs , and leaves all other nodes unaffected. Contrast this with a traditional hash table in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to bandwidth -intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure). Consistent hashing # Further information: Consistent hashing Consistent hashing employs a function $ \\delta (k_{1},k_{2}) $ that defines an abstract notion of the distance between the keys $ k_{1} $ and $ k_{2} $, which is unrelated to geographical distance or network latency . Each node is assigned a single key called its identifier (ID). A node with ID $ i_{x} $owns all the keys $ k_{m} $ for which $ i_{x} $ is the closest ID, measured according to $ \\delta (k_{m},i_{x}) $. For example, the Chord DHT uses consistent hashing , which treats nodes as points on a circle, and $ \\delta (k_{1},k_{2}) $ is the distance traveling clockwise around the circle from $ k_{1} $ to $ k_{2} $. Thus, the circular keyspace is split into contiguous segments whose endpoints are the node identifiers . If $ i_{1} $ and $ i_{2} $ are two adjacent IDs, with a shorter clockwise distance from $ i_{1} $ to $ i_{2} $, then the node with ID $ i_{2} $owns all the keys that fall between $ i_{1} $ and $ i_{2} $. Rendezvous hashing # Further information: Rendezvous hashing In rendezvous hashing, also called highest random weight hashing, all clients use the same hash function h() (chosen ahead of time) to associate a key to one of the n available servers. Each client has the same list of identifiers { S 1, S 2, ..., S n }, one for each server. Given some key k , a client computes n hash weights w 1 = h ( S 1, k ), w 2 = h ( S 2, k ), ..., w n = h ( S n , k ). The client associates that key with the server corresponding to the highest hash weight for that key. A server with ID $ S_{x} $ owns all the keys $ k_{m} $ for which the hash weight $ h(S_{x},k_{m}) $ is higher than the hash weight of any other node for that key. Overlay network # Each node maintains a set of links to other nodes (its neighbors or routing table ). Together, these links form the overlay network . A node picks its neighbors according to a certain structure, called the network's topology . All DHT topologies share some variant of the most essential property: for any key k , each node either has a node ID that owns k or has a link to a node whose node ID is closer to k , in terms of the keyspace distance defined above. It is then easy to route a message to the owner of any key k using the following greedy algorithm (that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest to k . When there is no such neighbor, then we must have arrived at the closest node, which is the owner of k as defined above. This style of routing is sometimes called key-based routing . SUMMARY : keyspace distance \u5728consistent hashing\u4e2d\u5b9a\u4e49\u7684\uff1b Beyond basic routing correctness, two important constraints on the topology are to guarantee that the maximum number of hops \uff08\u8df3\uff09 in any route (route length) is low, so that requests complete quickly; and that the maximum number of neighbors of any node (maximum node degree ) is low, so that maintenance overhead is not excessive. Of course, having shorter routes requires higher maximum degree . Some common choices for maximum degree and route length are as follows, where n is the number of nodes in the DHT, using Big O notation : Max. degree Max route length Used in Note $ O(1) $ $ O(n) $ Worst lookup lengths, with likely much slower lookups times $ O(\\log n) $ $ O(\\log n) $ Chord Kademlia Pastry Tapestry Most common, but not optimal (degree/route length). Chord is the most basic version, with Kademlia seeming the most popular optimized variant (should have improved average lookup) $ O(\\log n) $ $ O(\\log n/\\log(\\log n)) $ Koorde Likely would be more complex to implement, but lookups might be faster (have a lower worst case bound) $ O({\\sqrt {n}}) $ $ O(1) $ Worst local storage needs, with lots of communication after any node connects or disconnects Algorithms for overlay networks # Aside from routing, there exist many algorithms that exploit the structure of the overlay network for sending a message to all nodes, or a subset of nodes, in a DHT.[ 16] These algorithms are used by applications to do overlay multicast , range queries, or to collect statistics. Two systems that are based on this approach are Structella,[ 17] which implements flooding and random walks on a Pastry overlay, and DQ-DHT,[ 18] which implements a dynamic querying search algorithm over a Chord network. DHT implementations # Most notable differences encountered in practical instances of DHT implementations include at least the following: The address space is a parameter of DHT. Several real world DHTs use 128-bit or 160-bit key space Some real-world DHTs use hash functions other than SHA-1. In the real world the key $ k $ could be a hash of a file's content rather than a hash of a file's name to provide content-addressable storage , so that renaming of the file does not prevent users from finding it. Some DHTs may also publish objects of different types. For example, key $ k $ could be the node $ ID $ and associated data could describe how to contact this node. This allows publication-of-presence information and often used in IM applications, etc. In the simplest case, $ ID $ is just a random number that is directly used as key $ k $ (so in a 160-bit DHT $ ID $ will be a 160-bit number, usually randomly chosen). In some DHTs, publishing of nodes' IDs is also used to optimize DHT operations. Redundancy can be added to improve reliability. The $ (k,data) $ key pair can be stored in more than one node corresponding to the key. Usually, rather than selecting just one node, real world DHT algorithms select $ i $ suitable nodes, with $ i $ being an implementation-specific parameter of the DHT. In some DHT designs, nodes agree to handle a certain keyspace range, the size of which may be chosen dynamically, rather than hard-coded. Some advanced DHTs like Kademlia perform iterative lookups through the DHT first in order to select a set of suitable nodes and send $ put(k,data) $ messages only to those nodes, thus drastically reducing useless traffic, since published messages are only sent to nodes that seem suitable for storing the key $ k $; and iterative lookups cover just a small set of nodes rather than the entire DHT, reducing useless forwarding. In such DHTs, forwarding of $ put(k,data) $ messages may only occur as part of a self-healing algorithm: if a target node receives a $ put(k,data) $message, but believes that $ k $ is out of its handled range and a closer node (in terms of DHT keyspace) is known, the message is forwarded to that node. Otherwise, data are indexed locally. This leads to a somewhat self-balancing DHT behavior. Of course, such an algorithm requires nodes to publish their presence data in the DHT so the iterative lookups can be performed. Examples # DHT protocols and implementations Apache Cassandra BATON Overlay Mainline DHT - Standard DHT used by BitTorrent (based on Kademlia as provided by Khashmir[ 24] ) CAN (Content Addressable Network) Chord Koorde Kademlia Pastry P-Grid Riak Tapestry TomP2P Voldemort Applications employing DHTs BTDigg : BitTorrent DHT search engine CloudSNAP : a decentralized web application deployment platform Codeen : web caching Coral Content Distribution Network FAROO : peer-to-peer Web search engine Freenet : a censorship-resistant anonymous network GlusterFS : a distributed file system used for storage virtualization GNUnet : Freenet-like distribution network including a DHT implementation I2P : An open-source anonymous peer-to-peer network. I2P-Bote : serverless secure anonymous e-mail. IPFS : A content-addressable, peer-to-peer hypermedia distribution protocol JXTA : open-source P2P platform LBRY : a free, open and community-run digital marketplace Oracle Coherence : an in-memory data grid built on top of a Java DHT implementation Perfect Dark : a peer-to-peer file-sharing application from Japan Retroshare : a Friend-to-friend network[ 25] Ring : a privacy-preserving voice, video and chat communication platform, based on a Kademlia-like DHT Tox : an instant messaging system intended to function as a Skype replacement Twister : a microblogging peer-to-peer platform YaCy : a distributed search engine","title":"wikipedia Distributed hash table"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#distributed_hash_table","text":"NOTE : redis cluster\u53ef\u4ee5\u770b\u505a\u662f Distributed hash table \uff1b A distributed hash table ( DHT ) is a class of a decentralized distributed system that provides a lookup service similar to a hash table : ( key , value ) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Keys are unique identifiers which map to particular values , which in turn can be anything from addresses, to documents , to arbitrary data .[ 1] Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption. This allows a DHT to scale to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures. DHTs form an infrastructure that can be used to build more complex services, such as anycast , cooperative Web caching , distributed file systems , domain name services , instant messaging , multicast , and also peer-to-peer file sharing and content distribution systems. Notable distributed networks that use DHTs include BitTorrent 's distributed tracker, the Coral Content Distribution Network , the Kad network , the Storm botnet , the Tox instant messenger , Freenet , the YaCy search engine, and the InterPlanetary File System . Distributed hash tables","title":"Distributed hash table"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#history","text":"DHT research was originally motivated, in part, by peer-to-peer systems such as Freenet , gnutella , BitTorrent and Napster , which took advantage of resources distributed across the Internet to provide a single useful application. In particular, they took advantage of increased bandwidth and hard disk capacity to provide a file-sharing service.[ 2] DHT\u7814\u7a76\u6700\u521d\u7684\u52a8\u673a\u90e8\u5206\u662f\u7531Freenet\uff0cgnutella\uff0cBitTorrent\u548cNapster\u7b49\u5bf9\u7b49\u7cfb\u7edf\u63a8\u52a8\u7684\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5229\u7528\u5206\u5e03\u5728Internet\u4e0a\u7684\u8d44\u6e90\u6765\u63d0\u4f9b\u5355\u4e00\u6709\u7528\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u7279\u522b\u662f\uff0c\u4ed6\u4eec\u5229\u7528\u589e\u52a0\u7684\u5e26\u5bbd\u548c\u786c\u76d8\u5bb9\u91cf\u6765\u63d0\u4f9b\u6587\u4ef6\u5171\u4eab\u670d\u52a1\u3002 These systems differed in how they located the data offered by their peers. Napster, the first large-scale P2P content delivery system, required a central index server : each node, upon joining, would send a list of locally held files to the server, which would perform searches and refer the queries to the nodes that held the results. This central component left the system vulnerable to attacks and lawsuits. \u8fd9\u4e9b\u7cfb\u7edf\u5728\u5b9a\u4f4d\u540c\u884c\u63d0\u4f9b\u7684\u6570\u636e\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002 Napster\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21P2P\u5185\u5bb9\u4f20\u9001\u7cfb\u7edf\uff0c\u9700\u8981\u4e00\u4e2a\u4e2d\u592e\u7d22\u5f15\u670d\u52a1\u5668\uff1a\u6bcf\u4e2a\u8282\u70b9\u5728\u52a0\u5165\u65f6\uff0c\u4f1a\u5411\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u672c\u5730\u4fdd\u5b58\u6587\u4ef6\u5217\u8868\uff0c\u8fd9\u5c06\u6267\u884c\u641c\u7d22\u5e76\u5c06\u67e5\u8be2\u5f15\u7528\u5230\u6301\u6709\u7ed3\u679c\u3002\u8fd9\u4e00\u6838\u5fc3\u7ec4\u4ef6\u4f7f\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u548c\u8bc9\u8bbc Gnutella and similar networks moved to a flooding query model \u2013 in essence, each search would result in a message being broadcast to every other machine in the network. While avoiding a single point of failure , this method was significantly less efficient than Napster. Later versions of Gnutella clients moved to a dynamic querying model which vastly improved efficiency.[ 3] Gnutella\u548c\u7c7b\u4f3c\u7684\u7f51\u7edc\u8f6c\u79fb\u5230\u6d2a\u6cdb\u67e5\u8be2\u6a21\u578b - \u5b9e\u8d28\u4e0a\uff0c\u6bcf\u6b21\u641c\u7d22\u90fd\u4f1a\u5bfc\u81f4\u6d88\u606f\u88ab\u5e7f\u64ad\u5230\u7f51\u7edc\u4e2d\u7684\u6bcf\u53f0\u5176\u4ed6\u673a\u5668\u3002\u867d\u7136\u907f\u514d\u4e86\u5355\u70b9\u6545\u969c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u6548\u7387\u660e\u663e\u4f4e\u4e8eNapster\u3002\u66f4\u9ad8\u7248\u672c\u7684Gnutella\u5ba2\u6237\u8f6c\u5411\u52a8\u6001\u67e5\u8be2\u6a21\u578b\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u6548\u7387\u3002 Freenet is fully distributed, but employs a heuristic key-based routing in which each file is associated with a key, and files with similar keys tend to cluster on a similar set of nodes. Queries are likely to be routed through the network to such a cluster without needing to visit many peers.[ 4] However, Freenet does not guarantee that data will be found. Freenet\u662f\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\uff0c\u4f46\u91c7\u7528\u57fa\u4e8e\u5bc6\u94a5\u7684\u542f\u53d1\u5f0f\u8def\u7531\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6587\u4ef6\u4e0e\u5bc6\u94a5\u76f8\u5173\u8054\uff0c\u5177\u6709\u76f8\u4f3c\u5bc6\u94a5\u7684\u6587\u4ef6\u503e\u5411\u4e8e\u5728\u7c7b\u4f3c\u7684\u8282\u70b9\u96c6\u4e0a\u96c6\u7fa4\u3002\u67e5\u8be2\u5f88\u53ef\u80fd\u901a\u8fc7\u7f51\u7edc\u8def\u7531\u5230\u8fd9\u6837\u7684\u96c6\u7fa4\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u8bb8\u591a\u5bf9\u7b49\u4f53\u3002[4]\u4f46\u662f\uff0cFreenet\u4e0d\u4fdd\u8bc1\u4f1a\u627e\u5230\u6570\u636e\u3002 Distributed hash table s use a more structured key-based routing in order to attain both the decentralization of Freenet and gnutella, and the efficiency and guaranteed results of Napster. One drawback is that, like Freenet, DHTs only directly support exact-match search , rather than keyword search , although Freenet's routing algorithm can be generalized to any key type where a closeness operation can be defined.[ 5] \u5206\u5e03\u5f0f\u54c8\u5e0c\u8868\u4f7f\u7528\u66f4\u52a0\u7ed3\u6784\u5316\u7684\u57fa\u4e8e\u5bc6\u94a5\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0Freenet\u548cgnutella\u7684\u5206\u6563\u5316\uff0c\u4ee5\u53caNapster\u7684\u6548\u7387\u548c\u4fdd\u8bc1\u7ed3\u679c\u3002 \u4e00\u4e2a\u7f3a\u70b9\u662f\uff0c\u50cfFreenet\u4e00\u6837\uff0cDHT\u53ea\u76f4\u63a5\u652f\u6301\u7cbe\u786e\u5339\u914d\u641c\u7d22\uff0c\u800c\u4e0d\u662f\u5173\u952e\u5b57\u641c\u7d22\uff0c\u5c3d\u7ba1Freenet\u7684\u8def\u7531\u7b97\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u4efb\u4f55\u53ef\u4ee5\u5b9a\u4e49\u63a5\u8fd1\u5ea6\u64cd\u4f5c\u7684\u5bc6\u94a5\u7c7b\u578b\u3002 In 2001, four systems\u2014 CAN ,[ 6] Chord ,[ 7] Pastry , and Tapestry \u2014ignited\uff08\u6fc0\u8d77\uff09 DHTs as a popular research topic. A project called the Infrastructure for Resilient Internet Systems (Iris) was funded by a $12 million grant from the US National Science Foundation in 2002.[ 8] Researchers included Sylvia Ratnasamy , Ion Stoica , Hari Balakrishnan and Scott Shenker .[ 9] Outside academia, DHT technology has been adopted as a component of BitTorrent and in the Coral Content Distribution Network . 2001\u5e74\uff0c\u56db\u4e2a\u7cfb\u7edf-CAN\uff0c[6] Chord\uff0c[7] Pastry\u548cTapestry-\u70b9\u71c3\u4e86DHT\u4f5c\u4e3a\u4e00\u4e2a\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898\u3002 \u4e00\u4e2a\u540d\u4e3a\u201c\u5f39\u6027\u4e92\u8054\u7f51\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u201d\uff08Iris\uff09\u7684\u9879\u76ee\u75312002\u5e74\u7f8e\u56fd\u56fd\u5bb6\u79d1\u5b66\u57fa\u91d1\u4f1a\u62e8\u6b3e1200\u4e07\u7f8e\u5143\u8d44\u52a9\u3002[8] \u7814\u7a76\u4eba\u5458\u5305\u62ecSylvia Ratnasamy\uff0cIon Stoica\uff0cHari Balakrishnan\u548cScott Shenker\u3002[9] \u5728\u5b66\u672f\u754c\u4e4b\u5916\uff0cDHT\u6280\u672f\u5df2\u88ab\u91c7\u7eb3\u4e3aBitTorrent\u548cCoral\u5185\u5bb9\u5206\u53d1\u7f51\u7edc\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002","title":"History"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#properties","text":"DHTs characteristically emphasize the following properties: Autonomy\uff08\u81ea\u6cbb\u7684\uff09 and decentralization : the nodes collectively form the system without any central coordination . Fault tolerance : the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing. Scalability : the system should function efficiently even with thousands or millions of nodes. SUMMARY : redis cluster\u4e5f\u5177\u6709\u4e0a\u8ff0\u7684property A key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system \u2013 most commonly, O (log n ) of the $ n $ participants (see below) \u2013 so that only a limited amount of work needs to be done for each change in membership. Some DHT designs seek to be secure against malicious\uff08\u6076\u610f\u7684\uff09 participants[ 10] and to allow participants to remain anonymous , though this is less common than in many other peer-to-peer (especially file sharing ) systems; see anonymous P2P . Finally, DHTs must deal with more traditional distributed systems issues such as load balancing , data integrity , and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly).","title":"Properties"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#structure","text":"The structure of a DHT can be decomposed into several main components.[ 11] [ 12] The foundation is an abstract keyspace , such as the set of 160-bit strings . A keyspace partitioning scheme splits ownership of this keyspace among the participating nodes. An overlay network then connects the nodes, allowing them to find the owner of any given key in the keyspace . Once these components are in place, a typical use of the DHT for storage and retrieval might proceed as follows. Suppose the keyspace is the set of 160-bit strings. To index a file with given filename and data in the DHT, the SHA-1 hash of filename is generated, producing a 160-bit key k , and a message put ( k, data ) is sent to any node participating in the DHT. The message is forwarded from node to node through the overlay network until it reaches the single node responsible for key k as specified by the keyspace partitioning . That node then stores the key and the data. Any other client can then retrieve the contents of the file by again hashing filename to produce k and asking any DHT node to find the data associated with k with a message get ( k ). The message will again be routed through the overlay to the node responsible for k , which will reply with the stored data . SUMMARY : \u4e0a\u8ff0\u662f\u4f7f\u7528 SHA-1 hash \u6765\u83b7\u5f97abstract keyspace\uff1b\u4f7f\u7528 Consistent hashing \u3001 Rendezvous hashing \u6765\u8fdb\u884ckeyspace partition\uff1b SUMMARY : redis cluster \u662f\u5426\u5b58\u5728\u4e0a\u8ff0forward\u7684\u8fc7\u7a0b\uff1f The keyspace partitioning and overlay network components are described below with the goal of capturing the principal ideas common to most DHTs; many designs differ in the details.","title":"Structure"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#keyspace_partitioning","text":"Most DHTs use some variant of consistent hashing or rendezvous hashing to map keys to nodes. The two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem. Both consistent hashing and rendezvous hashing have the essential property that removal or addition of one node changes only the set of keys owned by the nodes with adjacent IDs , and leaves all other nodes unaffected. Contrast this with a traditional hash table in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to bandwidth -intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure).","title":"Keyspace partitioning"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#consistent_hashing","text":"Further information: Consistent hashing Consistent hashing employs a function $ \\delta (k_{1},k_{2}) $ that defines an abstract notion of the distance between the keys $ k_{1} $ and $ k_{2} $, which is unrelated to geographical distance or network latency . Each node is assigned a single key called its identifier (ID). A node with ID $ i_{x} $owns all the keys $ k_{m} $ for which $ i_{x} $ is the closest ID, measured according to $ \\delta (k_{m},i_{x}) $. For example, the Chord DHT uses consistent hashing , which treats nodes as points on a circle, and $ \\delta (k_{1},k_{2}) $ is the distance traveling clockwise around the circle from $ k_{1} $ to $ k_{2} $. Thus, the circular keyspace is split into contiguous segments whose endpoints are the node identifiers . If $ i_{1} $ and $ i_{2} $ are two adjacent IDs, with a shorter clockwise distance from $ i_{1} $ to $ i_{2} $, then the node with ID $ i_{2} $owns all the keys that fall between $ i_{1} $ and $ i_{2} $.","title":"Consistent hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#rendezvous_hashing","text":"Further information: Rendezvous hashing In rendezvous hashing, also called highest random weight hashing, all clients use the same hash function h() (chosen ahead of time) to associate a key to one of the n available servers. Each client has the same list of identifiers { S 1, S 2, ..., S n }, one for each server. Given some key k , a client computes n hash weights w 1 = h ( S 1, k ), w 2 = h ( S 2, k ), ..., w n = h ( S n , k ). The client associates that key with the server corresponding to the highest hash weight for that key. A server with ID $ S_{x} $ owns all the keys $ k_{m} $ for which the hash weight $ h(S_{x},k_{m}) $ is higher than the hash weight of any other node for that key.","title":"Rendezvous hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#overlay_network","text":"Each node maintains a set of links to other nodes (its neighbors or routing table ). Together, these links form the overlay network . A node picks its neighbors according to a certain structure, called the network's topology . All DHT topologies share some variant of the most essential property: for any key k , each node either has a node ID that owns k or has a link to a node whose node ID is closer to k , in terms of the keyspace distance defined above. It is then easy to route a message to the owner of any key k using the following greedy algorithm (that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest to k . When there is no such neighbor, then we must have arrived at the closest node, which is the owner of k as defined above. This style of routing is sometimes called key-based routing . SUMMARY : keyspace distance \u5728consistent hashing\u4e2d\u5b9a\u4e49\u7684\uff1b Beyond basic routing correctness, two important constraints on the topology are to guarantee that the maximum number of hops \uff08\u8df3\uff09 in any route (route length) is low, so that requests complete quickly; and that the maximum number of neighbors of any node (maximum node degree ) is low, so that maintenance overhead is not excessive. Of course, having shorter routes requires higher maximum degree . Some common choices for maximum degree and route length are as follows, where n is the number of nodes in the DHT, using Big O notation : Max. degree Max route length Used in Note $ O(1) $ $ O(n) $ Worst lookup lengths, with likely much slower lookups times $ O(\\log n) $ $ O(\\log n) $ Chord Kademlia Pastry Tapestry Most common, but not optimal (degree/route length). Chord is the most basic version, with Kademlia seeming the most popular optimized variant (should have improved average lookup) $ O(\\log n) $ $ O(\\log n/\\log(\\log n)) $ Koorde Likely would be more complex to implement, but lookups might be faster (have a lower worst case bound) $ O({\\sqrt {n}}) $ $ O(1) $ Worst local storage needs, with lots of communication after any node connects or disconnects","title":"Overlay network"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#algorithms_for_overlay_networks","text":"Aside from routing, there exist many algorithms that exploit the structure of the overlay network for sending a message to all nodes, or a subset of nodes, in a DHT.[ 16] These algorithms are used by applications to do overlay multicast , range queries, or to collect statistics. Two systems that are based on this approach are Structella,[ 17] which implements flooding and random walks on a Pastry overlay, and DQ-DHT,[ 18] which implements a dynamic querying search algorithm over a Chord network.","title":"Algorithms for overlay networks"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#dht_implementations","text":"Most notable differences encountered in practical instances of DHT implementations include at least the following: The address space is a parameter of DHT. Several real world DHTs use 128-bit or 160-bit key space Some real-world DHTs use hash functions other than SHA-1. In the real world the key $ k $ could be a hash of a file's content rather than a hash of a file's name to provide content-addressable storage , so that renaming of the file does not prevent users from finding it. Some DHTs may also publish objects of different types. For example, key $ k $ could be the node $ ID $ and associated data could describe how to contact this node. This allows publication-of-presence information and often used in IM applications, etc. In the simplest case, $ ID $ is just a random number that is directly used as key $ k $ (so in a 160-bit DHT $ ID $ will be a 160-bit number, usually randomly chosen). In some DHTs, publishing of nodes' IDs is also used to optimize DHT operations. Redundancy can be added to improve reliability. The $ (k,data) $ key pair can be stored in more than one node corresponding to the key. Usually, rather than selecting just one node, real world DHT algorithms select $ i $ suitable nodes, with $ i $ being an implementation-specific parameter of the DHT. In some DHT designs, nodes agree to handle a certain keyspace range, the size of which may be chosen dynamically, rather than hard-coded. Some advanced DHTs like Kademlia perform iterative lookups through the DHT first in order to select a set of suitable nodes and send $ put(k,data) $ messages only to those nodes, thus drastically reducing useless traffic, since published messages are only sent to nodes that seem suitable for storing the key $ k $; and iterative lookups cover just a small set of nodes rather than the entire DHT, reducing useless forwarding. In such DHTs, forwarding of $ put(k,data) $ messages may only occur as part of a self-healing algorithm: if a target node receives a $ put(k,data) $message, but believes that $ k $ is out of its handled range and a closer node (in terms of DHT keyspace) is known, the message is forwarded to that node. Otherwise, data are indexed locally. This leads to a somewhat self-balancing DHT behavior. Of course, such an algorithm requires nodes to publish their presence data in the DHT so the iterative lookups can be performed.","title":"DHT implementations"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/wikipedia-Distributed-hash-table/#examples","text":"DHT protocols and implementations Apache Cassandra BATON Overlay Mainline DHT - Standard DHT used by BitTorrent (based on Kademlia as provided by Khashmir[ 24] ) CAN (Content Addressable Network) Chord Koorde Kademlia Pastry P-Grid Riak Tapestry TomP2P Voldemort Applications employing DHTs BTDigg : BitTorrent DHT search engine CloudSNAP : a decentralized web application deployment platform Codeen : web caching Coral Content Distribution Network FAROO : peer-to-peer Web search engine Freenet : a censorship-resistant anonymous network GlusterFS : a distributed file system used for storage virtualization GNUnet : Freenet-like distribution network including a DHT implementation I2P : An open-source anonymous peer-to-peer network. I2P-Bote : serverless secure anonymous e-mail. IPFS : A content-addressable, peer-to-peer hypermedia distribution protocol JXTA : open-source P2P platform LBRY : a free, open and community-run digital marketplace Oracle Coherence : an in-memory data grid built on top of a Java DHT implementation Perfect Dark : a peer-to-peer file-sharing application from Japan Retroshare : a Friend-to-friend network[ 25] Ring : a privacy-preserving voice, video and chat communication platform, based on a Kademlia-like DHT Tox : an instant messaging system intended to function as a Skype replacement Twister : a microblogging peer-to-peer platform YaCy : a distributed search engine","title":"Examples"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/cnblogs-consistent hash/","text":"\u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5 01\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8def\u7531\u7b97\u6cd5 02\u4ec0\u4e48\u662f\u4e00\u81f4\u6027hash\u7b97\u6cd5 03\u4e00\u81f4\u6027hash\u5728\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528 04\u4e0d\u662f\u6240\u6709\u60c5\u51b5\u90fd\u9002\u5408\u4e00\u81f4\u6027hash \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5 # \u201c\u4e00\u81f4\u6027hash\u7684\u8bbe\u8ba1\u521d\u8877\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898\uff0c\u5b83\u4e0d\u4ec5\u80fd\u8d77\u5230hash\u4f5c\u7528\uff0c\u8fd8\u53ef\u4ee5\u5728\u670d\u52a1\u5668\u5b95\u673a\u65f6\uff0c\u5c3d\u91cf\u5c11\u5730\u8fc1\u79fb\u6570\u636e\u3002\u56e0\u6b64\u88ab\u5e7f\u6cdb\u7528\u4e8e \u72b6\u6001\u670d\u52a1 \u7684 \u8def\u7531\u529f\u80fd \u201d 01\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8def\u7531\u7b97\u6cd5 # \u5047\u8bbe\u6709\u4e00\u4e2a\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u5176\u7b80\u6613\u67b6\u6784\u5982\u4e0b ) \u8bbe\u5907\u63a5\u5165\u5c42 \u4e0d\u4ec5\u8981\u63a5\u6536\u8bbe\u5907\u7684\u767b\u5f55\u3001\u4e0b\u7ebf\u7b49\u72b6\u6001\u547d\u4ee4\uff0c\u8fd8\u8981\u628a\u5f00\u53d1\u8005\u7684\u6d88\u606f\u63a8\u9001\u7ed9\u8bbe\u5907\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u5c31\u9700\u8981\u7ef4\u62a4\u8bbe\u5907\u7684 \u72b6\u6001\u4fe1\u606f \uff08\u5f53\u7136\u53ef\u4ee5\u4e13\u95e8\u62c6\u4e00\u4e2a \u72b6\u6001\u670d\u52a1 \u53bb\u7ef4\u62a4\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8981\u6c42\u8fd9\u90e8\u5206\u5fc5\u987b\u5c11\u6709\u4ee3\u7801\u66f4\u65b0\uff0c\u5177\u4f53\u539f\u56e0\u81ea\u5df1\u53bb\u60f3\u54e6=_=\uff09\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u7684\u6bcf\u53f0server\u90fd\u4fdd\u7559\u4e00\u6279\u8bbe\u5907\u7684\u72b6\u6001\u4fe1\u606fcache\uff0c\u8bbe\u5907\u5e94\u8be5\u8fde\u63a5\u54ea\u53f0server\u53bb\u83b7\u53d6\u6570\u636e\uff0c\u540c\u65f6 \u4e2d\u95f4\u5c42 \u7684\u6d88\u606f\u53c8\u8be5\u53d1\u5f80\u54ea\u4e2aserver\u53bb\u63a8\u9001\u5462\uff1f\u8fd9\u5c31\u7528\u5230\u4e86\u4e00\u81f4\u6027hash\u7b97\u6cd5\u3002 02\u4ec0\u4e48\u662f\u4e00\u81f4\u6027hash\u7b97\u6cd5 # \u4e00\u81f4\u6027hash\u7531\u5bf9\u8c61\u3001\u8d44\u6e90\u3001\u7b97\u6cd5\u548c\u673a\u5668\u7ec4\u6210\u3002\u5b83\u8981\u505a\u7684\u662f\uff1a\u5bf9\u8c61\u901a\u8fc7\u7b97\u6cd5\u5224\u65ad\u8fde\u54ea\u53f0\u673a\u5668\u3002\u5728\u5982\u4e0a\u7cfb\u7edf\u4e2d\uff1a\u8bbe\u5907id\uff08userID\uff09\u4e3a\u5bf9\u8c61\uff1b\u5176\u5bf9\u5e94\u7684\u72b6\u6001\u6570\u636e(cache)\u4e3a\u8d44\u6e90\uff1b\u670d\u52a1\u5668\u4e3a\u673a\u5668\u3002 \u5728\u4e00\u81f4\u6027hash\u7b97\u6cd5\u4e2d\uff0c\u8fd9\u4e9b\u8d44\u6e90\u56f4\u6210\u4e86\u4e00\u4e2a\u95ed\u73af\uff0c\u6bcf\u53f0\u673a\u5668\u53c8\u4fdd\u5b58\u7740\u4e00\u4e2a\u8d44\u6e90\u6bb5\uff0c\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u4e00\u6279\u5bf9\u8c61/\u8bbe\u5907\uff1b\u8fd9\u6837\u5982\u679c\u67d0\u53f0\u673a\u5668\u6302\u4e86\uff0c\u90a3\u5b83\u5bf9\u5e94\u7684\u8d44\u6e90\u8f6c\u79fb\u5230\u79bb\u5b83\u8f83\u8fd1\u7684 \u673a\u5668x \uff0c\u8fd9\u53f0dead server\u5bf9\u5e94\u7684\u8bbe\u5907\u8fde\u63a5\u5230\u673a\u5668x\u5c31\u884c\u3002 \u73b0\u5728\u5047\u8bbe\u8fd9\u56db\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u7684\u8bbe\u5907\uff0c\u6d3b\u8dc3\u60c5\u51b5\u76f8\u5dee\u8f83\u5927\u3002\u6bd4\u5982\u8bf4\u8d44\u6e90\u6bb51\u30012\u5bf9\u5e94\u7684\u8bbe\u5907\u7279\u522b\u6d3b\u8dc3\uff0c\u800c\u8d44\u6e90\u6bb53\u548c4\u51e0\u4e4e\u6ca1\u6d3b\u52a8\u3002\u8fd9\u6837\u673a\u56681-2\u9700\u8981\u4fdd\u5b58\u5927\u91cf\u7684\u72b6\u6001\u6570\u636e\uff0c\u800c3-4\u5219\u6709\u5927\u91cf\u7684\u7a7a\u7f6e\uff0c\u663e\u7136\u662f\u4e0d\u5408\u7406\u7684\u3002\u6539\u8fdb\u7248\u7684\u4e00\u81f4\u6027hash\u7b97\u6cd5\u662f\u8fd9\u6837\u64cd\u4f5c\u7684\uff1a\u5b83\u4e0d\u518d\u662f\u6bcf\u53f0\u673a\u5668\u53bb\u4fdd\u5b58\u4e00\u4e2a\u8fde\u7eed\u7684\u8d44\u6e90\u6bb5\uff0c\u800c\u662f\u8ba9\u6bcf\u53f0\u673a\u5668\u90fd\u4fdd\u5b58\u591a\u4e2a\u533a\u57df\u7684\u90e8\u5206\u8d44\u6e90\u6bb5\u3002\u5982\u673a\u56681\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u76841/4\uff0c\u673a\u56682\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u76841/4\uff0c\u673a\u56683\u30014\u540c\u6837\u5982\u6b64\u3002\u8fd9\u6837\u5373\u4f7f\u4e2a\u522b\u53f7\u6bb5\u6709\u70ed\u70b9\uff0c\u4e5f\u4f1a \u5747\u644a \u5230\u4e0d\u540c\u7684\u673a\u5668\u3002 03\u4e00\u81f4\u6027hash\u5728\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528 # \u5982\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027hash\u7684\u6982\u5ff5\u548c\u6539\u8fdb\uff0c\u5728\u7cfb\u7edf\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u7528\u6237\u91cf\u975e\u5e38\u5927\uff0c\u5f80\u5f80\u4e0d\u53ea\u4e00\u4e2a\u96c6\u7fa4\u3002\u6211\u4eec\u662f\u5982\u6b64\u4f7f\u7528\u4e00\u81f4\u6027hash\uff1a \u9996\u5148\u6839\u636e\u4e0d\u540c\u53f7\u6bb5\u9009\u62e9\u5bf9\u5e94\u7684\u96c6\u7fa4\uff0c\u8fd9\u90e8\u5206\u662f\u53ef\u914d\u7f6e\u7684 \u786e\u5b9a\u96c6\u7fa4\u540e\uff0c\u6839\u636e\u4e00\u81f4\u6027hash\u628a\u8bbe\u5907\u5339\u914d\u5230server\u7684\u67d0\u4e2ainstance\u4e0a(\u6bcf\u53f0server\u90e8\u7f72\u591a\u4e2a\u8bbe\u5907\u63a5\u5165\u5c42\u5b9e\u4f8b\uff081.\u6bcf\u4e2ainstance\u4fdd\u5b58\u7684\u72b6\u6001\u4fe1\u606f\u66f4\u5206\u6563;2.\u670d\u52a1\u7684gc\u95ee\u9898\u4f1a\u6709\u7f13\u89e3\uff09 \u5efa\u7acb\u673a\u5668\u865a\u62df\u8282\u70b9\uff1a\u628auser\u9006\u5e8f(\u6253\u4e71\u4e4b\u524d\u8fde\u7eeduserId)\uff0c\u7ec4\u6210\u65b0\u7684\u8d44\u6e90\u6bb5;\u76f8\u5f53\u4e8e\u5efa\u7acb\u4e86server\u865a\u62df\u8282\u70b9 \u8bb0\u5f55\u6bcf\u53f0server\u9501\u670d\u52a1\u7684\u8bbe\u5907\u6570\uff0c\u5982\u679c\u673a\u5668A\u6302\u4e86\uff0c\u6311\u9009\u670d\u52a1\u8bbe\u5907\u6570\u6700\u5c11\u7684\u673a\u5668\u53bb\u627f\u63a5kicked-device 04\u4e0d\u662f\u6240\u6709\u60c5\u51b5\u90fd\u9002\u5408\u4e00\u81f4\u6027hash # \u4ee5\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5\uff0c\u4f46\u4e0d\u662f\u6240\u6709\u7684\u670d\u52a1\u90fd\u9002\u5408\u7528\u4e00\u81f4\u6027hash\u6765\u8def\u7531\u3002\u6bd4\u598201\u8282\u4e2d\u7684\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u4e2d\u95f4\u5c42\u662f\u65e0\u72b6\u6001\u7684\uff0c\u5f00\u53d1\u8005\u63a5\u5165\u5c42\u8bf7\u6c42cluter-A\u7684\u54ea\u53f0\u673a\u5668\u90fd\u884c\uff0c\u5b83\u53ea\u8981\u505a\u5b8c\u57fa\u672c\u6821\u9a8c\u540e\uff0c\u628a\u6d88\u606f\u5f02\u6b65\u53d1\u7ed9MQ\u5373\u53ef\uff0c\u65e0\u9700\u7b49\u5f85\u7ed3\u679c\u76f4\u63a5\u8fd4\u56de; \u800c\u8bbe\u5907\u63a5\u5165\u5c42\u662f\u6709\u72b6\u6001\u7684\uff0c\u4e14\u5bf9\u8f83\u9ad8\u65f6\u5ef6\u65e0\u6cd5\u5fcd\u53d7\uff0c\u66f4\u9002\u5408\u4e00\u81f4\u6027Hash\u9009\u62e9\u597dserver-instance\uff0c\u7136\u540e\u901a\u8fc7TCP/UDP\u6765\u901a\u4fe1\u3002 \u5982\u679c\u559c\u6b22\u6211\u7684\u6587\u7ae0\uff0c\u8bf7\u957f\u6309\u4e8c\u7ef4\u7801\uff0c\u5173\u6ce8\u9773\u521a\u540c\u5b66","title":"Cnblogs consistent hash"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/cnblogs-consistent hash/#hash","text":"\u201c\u4e00\u81f4\u6027hash\u7684\u8bbe\u8ba1\u521d\u8877\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898\uff0c\u5b83\u4e0d\u4ec5\u80fd\u8d77\u5230hash\u4f5c\u7528\uff0c\u8fd8\u53ef\u4ee5\u5728\u670d\u52a1\u5668\u5b95\u673a\u65f6\uff0c\u5c3d\u91cf\u5c11\u5730\u8fc1\u79fb\u6570\u636e\u3002\u56e0\u6b64\u88ab\u5e7f\u6cdb\u7528\u4e8e \u72b6\u6001\u670d\u52a1 \u7684 \u8def\u7531\u529f\u80fd \u201d","title":"\u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/cnblogs-consistent hash/#01","text":"\u5047\u8bbe\u6709\u4e00\u4e2a\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u5176\u7b80\u6613\u67b6\u6784\u5982\u4e0b ) \u8bbe\u5907\u63a5\u5165\u5c42 \u4e0d\u4ec5\u8981\u63a5\u6536\u8bbe\u5907\u7684\u767b\u5f55\u3001\u4e0b\u7ebf\u7b49\u72b6\u6001\u547d\u4ee4\uff0c\u8fd8\u8981\u628a\u5f00\u53d1\u8005\u7684\u6d88\u606f\u63a8\u9001\u7ed9\u8bbe\u5907\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u5c31\u9700\u8981\u7ef4\u62a4\u8bbe\u5907\u7684 \u72b6\u6001\u4fe1\u606f \uff08\u5f53\u7136\u53ef\u4ee5\u4e13\u95e8\u62c6\u4e00\u4e2a \u72b6\u6001\u670d\u52a1 \u53bb\u7ef4\u62a4\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8981\u6c42\u8fd9\u90e8\u5206\u5fc5\u987b\u5c11\u6709\u4ee3\u7801\u66f4\u65b0\uff0c\u5177\u4f53\u539f\u56e0\u81ea\u5df1\u53bb\u60f3\u54e6=_=\uff09\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u7684\u6bcf\u53f0server\u90fd\u4fdd\u7559\u4e00\u6279\u8bbe\u5907\u7684\u72b6\u6001\u4fe1\u606fcache\uff0c\u8bbe\u5907\u5e94\u8be5\u8fde\u63a5\u54ea\u53f0server\u53bb\u83b7\u53d6\u6570\u636e\uff0c\u540c\u65f6 \u4e2d\u95f4\u5c42 \u7684\u6d88\u606f\u53c8\u8be5\u53d1\u5f80\u54ea\u4e2aserver\u53bb\u63a8\u9001\u5462\uff1f\u8fd9\u5c31\u7528\u5230\u4e86\u4e00\u81f4\u6027hash\u7b97\u6cd5\u3002","title":"01\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8def\u7531\u7b97\u6cd5"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/cnblogs-consistent hash/#02hash","text":"\u4e00\u81f4\u6027hash\u7531\u5bf9\u8c61\u3001\u8d44\u6e90\u3001\u7b97\u6cd5\u548c\u673a\u5668\u7ec4\u6210\u3002\u5b83\u8981\u505a\u7684\u662f\uff1a\u5bf9\u8c61\u901a\u8fc7\u7b97\u6cd5\u5224\u65ad\u8fde\u54ea\u53f0\u673a\u5668\u3002\u5728\u5982\u4e0a\u7cfb\u7edf\u4e2d\uff1a\u8bbe\u5907id\uff08userID\uff09\u4e3a\u5bf9\u8c61\uff1b\u5176\u5bf9\u5e94\u7684\u72b6\u6001\u6570\u636e(cache)\u4e3a\u8d44\u6e90\uff1b\u670d\u52a1\u5668\u4e3a\u673a\u5668\u3002 \u5728\u4e00\u81f4\u6027hash\u7b97\u6cd5\u4e2d\uff0c\u8fd9\u4e9b\u8d44\u6e90\u56f4\u6210\u4e86\u4e00\u4e2a\u95ed\u73af\uff0c\u6bcf\u53f0\u673a\u5668\u53c8\u4fdd\u5b58\u7740\u4e00\u4e2a\u8d44\u6e90\u6bb5\uff0c\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u4e00\u6279\u5bf9\u8c61/\u8bbe\u5907\uff1b\u8fd9\u6837\u5982\u679c\u67d0\u53f0\u673a\u5668\u6302\u4e86\uff0c\u90a3\u5b83\u5bf9\u5e94\u7684\u8d44\u6e90\u8f6c\u79fb\u5230\u79bb\u5b83\u8f83\u8fd1\u7684 \u673a\u5668x \uff0c\u8fd9\u53f0dead server\u5bf9\u5e94\u7684\u8bbe\u5907\u8fde\u63a5\u5230\u673a\u5668x\u5c31\u884c\u3002 \u73b0\u5728\u5047\u8bbe\u8fd9\u56db\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u7684\u8bbe\u5907\uff0c\u6d3b\u8dc3\u60c5\u51b5\u76f8\u5dee\u8f83\u5927\u3002\u6bd4\u5982\u8bf4\u8d44\u6e90\u6bb51\u30012\u5bf9\u5e94\u7684\u8bbe\u5907\u7279\u522b\u6d3b\u8dc3\uff0c\u800c\u8d44\u6e90\u6bb53\u548c4\u51e0\u4e4e\u6ca1\u6d3b\u52a8\u3002\u8fd9\u6837\u673a\u56681-2\u9700\u8981\u4fdd\u5b58\u5927\u91cf\u7684\u72b6\u6001\u6570\u636e\uff0c\u800c3-4\u5219\u6709\u5927\u91cf\u7684\u7a7a\u7f6e\uff0c\u663e\u7136\u662f\u4e0d\u5408\u7406\u7684\u3002\u6539\u8fdb\u7248\u7684\u4e00\u81f4\u6027hash\u7b97\u6cd5\u662f\u8fd9\u6837\u64cd\u4f5c\u7684\uff1a\u5b83\u4e0d\u518d\u662f\u6bcf\u53f0\u673a\u5668\u53bb\u4fdd\u5b58\u4e00\u4e2a\u8fde\u7eed\u7684\u8d44\u6e90\u6bb5\uff0c\u800c\u662f\u8ba9\u6bcf\u53f0\u673a\u5668\u90fd\u4fdd\u5b58\u591a\u4e2a\u533a\u57df\u7684\u90e8\u5206\u8d44\u6e90\u6bb5\u3002\u5982\u673a\u56681\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u76841/4\uff0c\u673a\u56682\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u76841/4\uff0c\u673a\u56683\u30014\u540c\u6837\u5982\u6b64\u3002\u8fd9\u6837\u5373\u4f7f\u4e2a\u522b\u53f7\u6bb5\u6709\u70ed\u70b9\uff0c\u4e5f\u4f1a \u5747\u644a \u5230\u4e0d\u540c\u7684\u673a\u5668\u3002","title":"02\u4ec0\u4e48\u662f\u4e00\u81f4\u6027hash\u7b97\u6cd5"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/cnblogs-consistent hash/#03hash","text":"\u5982\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027hash\u7684\u6982\u5ff5\u548c\u6539\u8fdb\uff0c\u5728\u7cfb\u7edf\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u7528\u6237\u91cf\u975e\u5e38\u5927\uff0c\u5f80\u5f80\u4e0d\u53ea\u4e00\u4e2a\u96c6\u7fa4\u3002\u6211\u4eec\u662f\u5982\u6b64\u4f7f\u7528\u4e00\u81f4\u6027hash\uff1a \u9996\u5148\u6839\u636e\u4e0d\u540c\u53f7\u6bb5\u9009\u62e9\u5bf9\u5e94\u7684\u96c6\u7fa4\uff0c\u8fd9\u90e8\u5206\u662f\u53ef\u914d\u7f6e\u7684 \u786e\u5b9a\u96c6\u7fa4\u540e\uff0c\u6839\u636e\u4e00\u81f4\u6027hash\u628a\u8bbe\u5907\u5339\u914d\u5230server\u7684\u67d0\u4e2ainstance\u4e0a(\u6bcf\u53f0server\u90e8\u7f72\u591a\u4e2a\u8bbe\u5907\u63a5\u5165\u5c42\u5b9e\u4f8b\uff081.\u6bcf\u4e2ainstance\u4fdd\u5b58\u7684\u72b6\u6001\u4fe1\u606f\u66f4\u5206\u6563;2.\u670d\u52a1\u7684gc\u95ee\u9898\u4f1a\u6709\u7f13\u89e3\uff09 \u5efa\u7acb\u673a\u5668\u865a\u62df\u8282\u70b9\uff1a\u628auser\u9006\u5e8f(\u6253\u4e71\u4e4b\u524d\u8fde\u7eeduserId)\uff0c\u7ec4\u6210\u65b0\u7684\u8d44\u6e90\u6bb5;\u76f8\u5f53\u4e8e\u5efa\u7acb\u4e86server\u865a\u62df\u8282\u70b9 \u8bb0\u5f55\u6bcf\u53f0server\u9501\u670d\u52a1\u7684\u8bbe\u5907\u6570\uff0c\u5982\u679c\u673a\u5668A\u6302\u4e86\uff0c\u6311\u9009\u670d\u52a1\u8bbe\u5907\u6570\u6700\u5c11\u7684\u673a\u5668\u53bb\u627f\u63a5kicked-device","title":"03\u4e00\u81f4\u6027hash\u5728\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/cnblogs-consistent hash/#04hash","text":"\u4ee5\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5\uff0c\u4f46\u4e0d\u662f\u6240\u6709\u7684\u670d\u52a1\u90fd\u9002\u5408\u7528\u4e00\u81f4\u6027hash\u6765\u8def\u7531\u3002\u6bd4\u598201\u8282\u4e2d\u7684\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u4e2d\u95f4\u5c42\u662f\u65e0\u72b6\u6001\u7684\uff0c\u5f00\u53d1\u8005\u63a5\u5165\u5c42\u8bf7\u6c42cluter-A\u7684\u54ea\u53f0\u673a\u5668\u90fd\u884c\uff0c\u5b83\u53ea\u8981\u505a\u5b8c\u57fa\u672c\u6821\u9a8c\u540e\uff0c\u628a\u6d88\u606f\u5f02\u6b65\u53d1\u7ed9MQ\u5373\u53ef\uff0c\u65e0\u9700\u7b49\u5f85\u7ed3\u679c\u76f4\u63a5\u8fd4\u56de; \u800c\u8bbe\u5907\u63a5\u5165\u5c42\u662f\u6709\u72b6\u6001\u7684\uff0c\u4e14\u5bf9\u8f83\u9ad8\u65f6\u5ef6\u65e0\u6cd5\u5fcd\u53d7\uff0c\u66f4\u9002\u5408\u4e00\u81f4\u6027Hash\u9009\u62e9\u597dserver-instance\uff0c\u7136\u540e\u901a\u8fc7TCP/UDP\u6765\u901a\u4fe1\u3002 \u5982\u679c\u559c\u6b22\u6211\u7684\u6587\u7ae0\uff0c\u8bf7\u957f\u6309\u4e8c\u7ef4\u7801\uff0c\u5173\u6ce8\u9773\u521a\u540c\u5b66","title":"04\u4e0d\u662f\u6240\u6709\u60c5\u51b5\u90fd\u9002\u5408\u4e00\u81f4\u6027hash"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/","text":"A Guide to Consistent Hashing What Is Hashing? Introducing Hash Tables (Hash Maps) Scaling Out: Distributed Hashing The Rehashing Problem The Solution: Consistent Hashing What Next? A Guide to Consistent Hashing # In recent years, with the advent of concepts such as cloud computing and big data, distributed systems have gained popularity and relevance. One such type of system, distributed caches that power many high-traffic dynamic websites and web applications, typically consist of a particular case of distributed hashing . These take advantage of an algorithm known as consistent hashing . What is consistent hashing? What\u2019s the motivation behind it, and why should you care? In this article, I\u2019ll first review the general concept of hashing and its purpose, followed by a description of distributed hashing and the problems it entails. In turn, that will lead us to our title subject. What Is Hashing? # What is \u201chashing\u201d all about? Merriam-Webster defines the noun hash as \u201cchopped meat mixed with potatoes and browned,\u201d and the verb as \u201cto chop (as meat and potatoes) into small pieces.\u201d So, culinary details aside, hash roughly means \u201cchop and mix\u201d\u2014and that\u2019s precisely where the technical term comes from. \u54c8\u5e0c\u662f\u4ec0\u4e48\u610f\u601d?\u300a\u97e6\u6c0f\u8bcd\u5178\u300b\u5c06\u540d\u8bcd\u201chash\u201d\u5b9a\u4e49\u4e3a\u201c\u5c06\u788e\u8089\u4e0e\u571f\u8c46\u6df7\u5408\u5e76\u714e\u6210\u8910\u8272\u201d\uff0c\u52a8\u8bcd\u201c\u5c06(\u8089\u548c\u571f\u8c46)\u5207\u6210\u5c0f\u5757\u201d\u3002\u6240\u4ee5\uff0c\u6487\u5f00\u70f9\u996a\u7ec6\u8282\u4e0d\u8c08\uff0chash\u5927\u81f4\u7684\u610f\u601d\u662f\u201c\u5207\u788e\u548c\u6df7\u5408\u201d\u2014\u2014\u8fd9\u6b63\u662f\u8fd9\u4e2a\u6280\u672f\u672f\u8bed\u7684\u6765\u6e90\u3002 A hash function is a function that maps one piece of data\u2014typically describing some kind of object, often of arbitrary size\u2014to another piece of data, typically an integer, known as hash code , or simply hash . For instance, some hash function designed to hash strings, with an output range of 0 .. 100 , may map the string Hello to, say, the number 57 , Hasta la vista, baby to the number 33 , and any other possible string to some number within that range. Since there are way more possible inputs than outputs, any given number will have many different strings mapped to it, a phenomenon known as collision . Good hash functions should somehow \u201cchop and mix\u201d (hence the term) the input data in such a way that the outputs for different input values are spread as evenly as possible over the output range. Hash functions have many uses and for each one, different properties may be desired. There is a type of hash function known as cryptographic hash functions , which must meet a restrictive set of properties and are used for security purposes\u2014including applications such as password protection, integrity(\u5b8c\u6574\u6027) checking and fingerprinting of messages, and data corruption detection, among others, but those are outside the scope of this article. Non-cryptographic hash functions have several uses as well, the most common being their use in hash tables , which is the one that concerns us and which we\u2019ll explore in more detail. Introducing Hash Tables (Hash Maps) # Imagine we needed to keep a list of all the members of some club while being able to search for any specific member. We could handle it by keeping the list in an array (or linked list) and, to perform a search, iterate the elements until we find the desired one (we might be searching based on their name, for instance). In the worst case, that would mean checking all members (if the one we\u2019re searching for is last, or not present at all), or half of them on average. In complexity theory terms, the search would then have complexity O(n) , and it would be reasonably fast for a small list, but it would get slower and slower in direct proportion to the number of members. How could that be improved? Let\u2019s suppose all these club members had a member ID , which happened to be a sequential number reflecting the order in which they joined the club. Assuming that searching by ID were acceptable, we could place all members in an array, with their indexes matching their ID s (for example, a member with ID=10 would be at the index 10 in the array). This would allow us to access each member directly, with no search at all. That would be very efficient, in fact, as efficient as it can possibly be, corresponding to the lowest complexity possible, O(1) , also known as constant time . But, admittedly, our club member ID scenario is somewhat contrived. What if ID s were big, non-sequential or random numbers? Or, if searching by ID were not acceptable, and we needed to search by name (or some other field) instead? It would certainly be useful to keep our fast direct access (or something close) while at the same time being able to handle arbitrary datasets and less restrictive search criteria. Here\u2019s where hash functions come to the rescue. A suitable hash function can be used to map an arbitrary piece of data to an integer, which will play a similar role to that of our club member ID , albeit with a few important differences. First, a good hash function generally has a wide output range (typically, the whole range of a 32 or 64-bit integer), so building an array for all possible indices would be either impractical or plain impossible, and a colossal waste of memory. To overcome that, we can have a reasonably sized array (say, just twice the number of elements we expect to store) and perform a modulo operation on the hash to get the array index. So, the index would be index = hash(object) mod N , where N is the size of the array. Second, object hashes will not be unique (unless we\u2019re working with a fixed dataset and a custom-built perfect hash function , but we won\u2019t discuss that here). There will be collisions (further increased by the modulo operation), and therefore a simple direct index access won\u2019t work. There are several ways to handle this, but a typical one is to attach a list, commonly known as a bucket , to each array index to hold all the objects sharing a given index. So, we have an array of size N , with each entry pointing to an object bucket. To add a new object, we need to calculate its hash modulo N , and check the bucket at the resulting index, adding the object if it\u2019s not already there. To search for an object, we do the same, just looking into the bucket to check if the object is there. Such a structure is called a hash table , and although the searches within buckets are linear, a properly sized hash table should have a reasonably small number of objects per bucket, resulting in almost constant time access (an average complexity of O(N/k) , where k is the number of buckets). With complex objects, the hash function is typically not applied to the whole object, but to a key instead. In our club member example, each object might contain several fields (like name, age, address, email, phone), but we could pick, say, the email to act as the key so that the hash function would be applied to the email only. In fact, the key need not be part of the object; it is common to store key/value pairs, where the key is usually a relatively short string, and the value can be an arbitrary piece of data. In such cases, the hash table or hash map is used as a dictionary , and that\u2019s the way some high-level languages implement objects or associative arrays. Scaling Out: Distributed Hashing # Now that we have discussed hashing, we\u2019re ready to look into distributed hashing . In some situations, it may be necessary or desirable to split a hash table into several parts, hosted by different servers. One of the main motivations for this is to bypass the memory limitations of using a single computer, allowing for the construction of arbitrarily large hash tables (given enough servers). In such a scenario, the objects (and their keys) are distributed among several servers, hence the name. A typical use case for this is the implementation of in-memory caches, such as Memcached . Such setups consist of a pool of caching servers that host many key/value pairs and are used to provide fast access to data originally stored (or computed) elsewhere. For example, to reduce the load on a database server and at the same time improve performance, an application can be designed to first fetch data from the cache servers, and only if it\u2019s not present there\u2014a situation known as cache miss \u2014resort to the database, running the relevant query and caching the results with an appropriate key, so that it can be found next time it\u2019s needed. Now, how does distribution take place? What criteria are used to determine which keys to host in which servers? The simplest way is to take the hash modulo of the number of servers. That is, server = hash(key) mod N , where N is the size of the pool. To store or retrieve a key, the client first computes the hash, applies a modulo N operation, and uses the resulting index to contact the appropriate server (probably by using a lookup table of IP addresses). Note that the hash function used for key distribution must be the same one across all clients, but it need not be the same one used internally by the caching servers. Let\u2019s see an example. Say we have three servers, A , B and C , and we have some string keys with their hashes: KEY HASH HASH mod 3 \"john\" 1633428562 2 \"bill\" 7594634739 0 \"jane\" 5000799124 1 \"steve\" 9787173343 0 \"kate\" 3421657995 2 A client wants to retrieve the value for key john . Its hash modulo 3 is 2 , so it must contact server C . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this: A B C \"john\" Next another client (or the same one) wants to retrieve the value for key bill . Its hash modulo 3 is 0 , so it must contact server A . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this now: A B C \"bill\" \"john\" After the remaining keys are added, the pool looks like this: A B C \"bill\" \"jane\" \"john\" \"steve\" \"kate\" The Rehashing Problem # This distribution scheme is simple, intuitive, and works fine. That is, until the number of servers changes. What happens if one of the servers crashes or becomes unavailable? Keys need to be redistributed to account for the missing server\uff08key\u9700\u8981\u91cd\u65b0\u5206\u53d1\uff0c\u4ee5\u5f25\u8865\u4e22\u5931\u7684\u670d\u52a1\u5668\uff09, of course. The same applies if one or more new servers are added to the pool;keys need to be redistributed to include the new servers. This is true for any distribution scheme, but the problem with our simple modulo distribution is that when the number of servers changes, most hashes modulo N will change, so most keys will need to be moved to a different server. So, even if a single server is removed or added, all keys will likely need to be rehashed into a different server. SUMMARY : \u4e0a\u8ff0\u5bf9\u95ee\u9898\u7684\u603b\u7ed3\u975e\u5e38\u7684\u597d\uff1b From our previous example, if we removed server C , we\u2019d have to rehash all the keys using hash modulo 2 instead of hash modulo 3 , and the new locations for the keys would become: KEY HASH HASH mod 2 \"john\" 1633428562 0 \"bill\" 7594634739 1 \"jane\" 5000799124 0 \"steve\" 9787173343 1 \"kate\" 3421657995 1 A B \"john\" \"bill\" \"jane\" \"steve\" \"kate\" Note that all key locations changed, not only the ones from server C . In the typical use case we mentioned before (caching), this would mean that, all of a sudden, the keys won\u2019t be found because they won\u2019t yet be present at their new location. So, most queries will result in misses, and the original data will likely need retrieving again from the source to be rehashed, thus placing a heavy load on the origin server(s) (typically a database). This may very well degrade performance severely and possibly crash the origin servers. The Solution: Consistent Hashing # So, how can this problem be solved? We need a distribution scheme that does not depend directly on the number of servers , so that, when adding or removing servers, the number of keys that need to be relocated is minimized. One such scheme\u2014a clever, yet surprisingly simple one\u2014is called consistent hashing , and was first described by Karger et al. at MIT in an academic paper from 1997 (according to Wikipedia). Consistent Hashing is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract circle , or hash ring . This allows servers and objects to scale without affecting the overall system. Imagine we mapped the hash output range onto the edge of a circle\uff08\u5c06hash\u8f93\u51fa\u503c\u7684\u503c\u57df\u6620\u5c04\u5230circle\u4e0a\uff09. That means that the minimum possible hash value , zero, would correspond to an angle of zero , the maximum possible value (some big integer we\u2019ll call INT_MAX ) would correspond to an angle of 2\ud835\udf45 radians (or 360 degrees), and all other hash values would linearly fit somewhere in between. So, we could take a key, compute its hash, and find out where it lies on the circle\u2019s edge. Assuming an INT_MAX of 1010 (for example\u2019s sake), the keys from our previous example would look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 Now imagine we also placed the servers on the edge of the circle , by pseudo-randomly assigning them angles too. This should be done in a repeatable\uff08\u53ef\u91cd\u590d\u7684\uff09 way (or at least in such a way that all clients agree on the servers\u2019 angles). A convenient way of doing this is by hashing the server name (or IP address, or some ID)\u2014as we\u2019d do with any other key\u2014to come up with its angle. In our example, things might look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 \"A\" 5572014558 200.6 \"B\" 8077113362 290.8 \"C\" 2269549488 81.7 Since we have the keys for both the objects and the servers on the same circle , we may define a simple rule to associate the former with the latter: Each object key will belong in the server whose key is closest , in a counterclockwise direction (or clockwise, depending on the conventions used). In other words, to find out which server to ask for a given key, we need to locate the key on the circle and move in the ascending angle direction until we find a server. In our example: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.7 \"C\" 2269549488 81.7 \"kate\" 3421657995 123.1 \"jane\" 5000799124 180 \"A\" 5572014557 200.5 \"bill\" 7594634739 273.4 \"B\" 8077113361 290.7 \"steve\" 787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"C\" C \"kate\" 3421831276 123.1 \"A\" A \"jane\" 5000648311 180 \"A\" A \"bill\" 7594873884 273.4 \"B\" B \"steve\" 9786437450 352.3 \"C\" C From a programming perspective, what we would do is keep a sorted list of server values (which could be angles or numbers in any real interval), and walk this list (or use a binary search) to find the first server with a value greater than, or equal to, that of the desired key. If no such value is found, we need to wrap around, taking the first one from the list. To ensure object keys are evenly distributed among servers, we need to apply a simple trick: To assign not one, but many labels (angles) to each server . So instead of having labels A , B and C , we could have, say, A0 .. A9 , B0 .. B9 and C0 .. C9 , all interspersed along the circle. The factor by which to increase the number of labels (server keys), known as weight , depends on the situation (and may even be different for each server) to adjust the probability of keys ending up on each. For example, if server B were twice as powerful as the rest, it could be assigned twice as many labels, and as a result, it would end up holding twice as many objects (on average). For our example we\u2019ll assume all three servers have an equal weight of 10 (this works well for three servers, for 10 to 50 servers, a weight in the range 100 to 500 would work better, and bigger pools may need even higher weights): KEY HASH ANGLE (DEG) \"C6\" 408965526 14.7 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"C4\" 1493080938 53.7 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"C0\" 1982701318 71.3 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"C9\" 3359725419 120.9 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"C1\" 3672205973 132.1 \"C8\" 3750588567 135 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"C7\" 5014097839 180.5 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"C3\" 7330467663 263.8 \"C5\" 7502566333 270 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"C2\" 8605012288 309.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"C7\" C \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"C6\" C So, what\u2019s the benefit of all this circle approach? Imagine server C is removed. To account for\uff08\u5f25\u8865\uff09 this, we must remove labels C0 .. C9 from the circle. This results in the object keys formerly adjacent to the deleted labels now being randomly labeled Ax and Bx , reassigning them to servers A and B . But what happens with the other object keys, the ones that originally belonged in A and B ? Nothing! That\u2019s the beauty of it: The absence of Cx labels does not affect those keys in any way. So, removing a server results in its object keys being randomly reassigned to the rest of the servers, leaving all other keys untouched : KEY HASH ANGLE (DEG) \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"A1\" A Something similar happens if, instead of removing a server, we add one. If we wanted to add server D to our example (say, as a replacement for C ), we would need to add labels D0 .. D9 . The result would be that roughly one-third of the existing keys (all belonging to A or B ) would be reassigned to D , and, again, the rest would stay the same: KEY HASH ANGLE (DEG) \"D2\" 439890723 15.8 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"D8\" 796709216 28.6 \"D1\" 1008580939 36.3 \"A3\" 1466730567 52.8 \"D5\" 1587548309 57.1 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"D4\" 2909395217 104.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"D7\" 3567129743 128.4 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"D6\" 5703092354 205.3 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"D0\" 8272587142 297.8 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"D3\" 9048608874 325.7 \"D9\" 9314459653 335.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"D2\" D This is how consistent hashing solves the rehashing problem. In general, only k/N keys need to be remapped when k is the number of keys and N is the number of servers (more specifically, the maximum of the initial and final number of servers). What Next? # We observed that when using distributed caching to optimize performance, it may happen that the number of caching servers changes (reasons for this may be a server crashing, or the need to add or remove a server to increase or decrease overall capacity). By using consistent hashing to distribute keys between the servers, we can rest assured that should that happen, the number of keys being rehashed\u2014and therefore, the impact on origin servers\u2014will be minimized, preventing potential downtime or performance issues. There are clients for several systems, such as Memcached and Redis, that include support for consistent hashing out of the box. Alternatively, you can implement the algorithm yourself, in your language of choice, and that should be relatively easy once the concept is understood. If data science interests you, Toptal has some of the best articles on the subject at the blog","title":"toptal A Guide to Consistent Hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#a_guide_to_consistent_hashing","text":"In recent years, with the advent of concepts such as cloud computing and big data, distributed systems have gained popularity and relevance. One such type of system, distributed caches that power many high-traffic dynamic websites and web applications, typically consist of a particular case of distributed hashing . These take advantage of an algorithm known as consistent hashing . What is consistent hashing? What\u2019s the motivation behind it, and why should you care? In this article, I\u2019ll first review the general concept of hashing and its purpose, followed by a description of distributed hashing and the problems it entails. In turn, that will lead us to our title subject.","title":"A Guide to Consistent Hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#what_is_hashing","text":"What is \u201chashing\u201d all about? Merriam-Webster defines the noun hash as \u201cchopped meat mixed with potatoes and browned,\u201d and the verb as \u201cto chop (as meat and potatoes) into small pieces.\u201d So, culinary details aside, hash roughly means \u201cchop and mix\u201d\u2014and that\u2019s precisely where the technical term comes from. \u54c8\u5e0c\u662f\u4ec0\u4e48\u610f\u601d?\u300a\u97e6\u6c0f\u8bcd\u5178\u300b\u5c06\u540d\u8bcd\u201chash\u201d\u5b9a\u4e49\u4e3a\u201c\u5c06\u788e\u8089\u4e0e\u571f\u8c46\u6df7\u5408\u5e76\u714e\u6210\u8910\u8272\u201d\uff0c\u52a8\u8bcd\u201c\u5c06(\u8089\u548c\u571f\u8c46)\u5207\u6210\u5c0f\u5757\u201d\u3002\u6240\u4ee5\uff0c\u6487\u5f00\u70f9\u996a\u7ec6\u8282\u4e0d\u8c08\uff0chash\u5927\u81f4\u7684\u610f\u601d\u662f\u201c\u5207\u788e\u548c\u6df7\u5408\u201d\u2014\u2014\u8fd9\u6b63\u662f\u8fd9\u4e2a\u6280\u672f\u672f\u8bed\u7684\u6765\u6e90\u3002 A hash function is a function that maps one piece of data\u2014typically describing some kind of object, often of arbitrary size\u2014to another piece of data, typically an integer, known as hash code , or simply hash . For instance, some hash function designed to hash strings, with an output range of 0 .. 100 , may map the string Hello to, say, the number 57 , Hasta la vista, baby to the number 33 , and any other possible string to some number within that range. Since there are way more possible inputs than outputs, any given number will have many different strings mapped to it, a phenomenon known as collision . Good hash functions should somehow \u201cchop and mix\u201d (hence the term) the input data in such a way that the outputs for different input values are spread as evenly as possible over the output range. Hash functions have many uses and for each one, different properties may be desired. There is a type of hash function known as cryptographic hash functions , which must meet a restrictive set of properties and are used for security purposes\u2014including applications such as password protection, integrity(\u5b8c\u6574\u6027) checking and fingerprinting of messages, and data corruption detection, among others, but those are outside the scope of this article. Non-cryptographic hash functions have several uses as well, the most common being their use in hash tables , which is the one that concerns us and which we\u2019ll explore in more detail.","title":"What Is Hashing?"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#introducing_hash_tables_hash_maps","text":"Imagine we needed to keep a list of all the members of some club while being able to search for any specific member. We could handle it by keeping the list in an array (or linked list) and, to perform a search, iterate the elements until we find the desired one (we might be searching based on their name, for instance). In the worst case, that would mean checking all members (if the one we\u2019re searching for is last, or not present at all), or half of them on average. In complexity theory terms, the search would then have complexity O(n) , and it would be reasonably fast for a small list, but it would get slower and slower in direct proportion to the number of members. How could that be improved? Let\u2019s suppose all these club members had a member ID , which happened to be a sequential number reflecting the order in which they joined the club. Assuming that searching by ID were acceptable, we could place all members in an array, with their indexes matching their ID s (for example, a member with ID=10 would be at the index 10 in the array). This would allow us to access each member directly, with no search at all. That would be very efficient, in fact, as efficient as it can possibly be, corresponding to the lowest complexity possible, O(1) , also known as constant time . But, admittedly, our club member ID scenario is somewhat contrived. What if ID s were big, non-sequential or random numbers? Or, if searching by ID were not acceptable, and we needed to search by name (or some other field) instead? It would certainly be useful to keep our fast direct access (or something close) while at the same time being able to handle arbitrary datasets and less restrictive search criteria. Here\u2019s where hash functions come to the rescue. A suitable hash function can be used to map an arbitrary piece of data to an integer, which will play a similar role to that of our club member ID , albeit with a few important differences. First, a good hash function generally has a wide output range (typically, the whole range of a 32 or 64-bit integer), so building an array for all possible indices would be either impractical or plain impossible, and a colossal waste of memory. To overcome that, we can have a reasonably sized array (say, just twice the number of elements we expect to store) and perform a modulo operation on the hash to get the array index. So, the index would be index = hash(object) mod N , where N is the size of the array. Second, object hashes will not be unique (unless we\u2019re working with a fixed dataset and a custom-built perfect hash function , but we won\u2019t discuss that here). There will be collisions (further increased by the modulo operation), and therefore a simple direct index access won\u2019t work. There are several ways to handle this, but a typical one is to attach a list, commonly known as a bucket , to each array index to hold all the objects sharing a given index. So, we have an array of size N , with each entry pointing to an object bucket. To add a new object, we need to calculate its hash modulo N , and check the bucket at the resulting index, adding the object if it\u2019s not already there. To search for an object, we do the same, just looking into the bucket to check if the object is there. Such a structure is called a hash table , and although the searches within buckets are linear, a properly sized hash table should have a reasonably small number of objects per bucket, resulting in almost constant time access (an average complexity of O(N/k) , where k is the number of buckets). With complex objects, the hash function is typically not applied to the whole object, but to a key instead. In our club member example, each object might contain several fields (like name, age, address, email, phone), but we could pick, say, the email to act as the key so that the hash function would be applied to the email only. In fact, the key need not be part of the object; it is common to store key/value pairs, where the key is usually a relatively short string, and the value can be an arbitrary piece of data. In such cases, the hash table or hash map is used as a dictionary , and that\u2019s the way some high-level languages implement objects or associative arrays.","title":"Introducing Hash Tables (Hash Maps)"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#scaling_out_distributed_hashing","text":"Now that we have discussed hashing, we\u2019re ready to look into distributed hashing . In some situations, it may be necessary or desirable to split a hash table into several parts, hosted by different servers. One of the main motivations for this is to bypass the memory limitations of using a single computer, allowing for the construction of arbitrarily large hash tables (given enough servers). In such a scenario, the objects (and their keys) are distributed among several servers, hence the name. A typical use case for this is the implementation of in-memory caches, such as Memcached . Such setups consist of a pool of caching servers that host many key/value pairs and are used to provide fast access to data originally stored (or computed) elsewhere. For example, to reduce the load on a database server and at the same time improve performance, an application can be designed to first fetch data from the cache servers, and only if it\u2019s not present there\u2014a situation known as cache miss \u2014resort to the database, running the relevant query and caching the results with an appropriate key, so that it can be found next time it\u2019s needed. Now, how does distribution take place? What criteria are used to determine which keys to host in which servers? The simplest way is to take the hash modulo of the number of servers. That is, server = hash(key) mod N , where N is the size of the pool. To store or retrieve a key, the client first computes the hash, applies a modulo N operation, and uses the resulting index to contact the appropriate server (probably by using a lookup table of IP addresses). Note that the hash function used for key distribution must be the same one across all clients, but it need not be the same one used internally by the caching servers. Let\u2019s see an example. Say we have three servers, A , B and C , and we have some string keys with their hashes: KEY HASH HASH mod 3 \"john\" 1633428562 2 \"bill\" 7594634739 0 \"jane\" 5000799124 1 \"steve\" 9787173343 0 \"kate\" 3421657995 2 A client wants to retrieve the value for key john . Its hash modulo 3 is 2 , so it must contact server C . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this: A B C \"john\" Next another client (or the same one) wants to retrieve the value for key bill . Its hash modulo 3 is 0 , so it must contact server A . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this now: A B C \"bill\" \"john\" After the remaining keys are added, the pool looks like this: A B C \"bill\" \"jane\" \"john\" \"steve\" \"kate\"","title":"Scaling Out: Distributed Hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#the_rehashing_problem","text":"This distribution scheme is simple, intuitive, and works fine. That is, until the number of servers changes. What happens if one of the servers crashes or becomes unavailable? Keys need to be redistributed to account for the missing server\uff08key\u9700\u8981\u91cd\u65b0\u5206\u53d1\uff0c\u4ee5\u5f25\u8865\u4e22\u5931\u7684\u670d\u52a1\u5668\uff09, of course. The same applies if one or more new servers are added to the pool;keys need to be redistributed to include the new servers. This is true for any distribution scheme, but the problem with our simple modulo distribution is that when the number of servers changes, most hashes modulo N will change, so most keys will need to be moved to a different server. So, even if a single server is removed or added, all keys will likely need to be rehashed into a different server. SUMMARY : \u4e0a\u8ff0\u5bf9\u95ee\u9898\u7684\u603b\u7ed3\u975e\u5e38\u7684\u597d\uff1b From our previous example, if we removed server C , we\u2019d have to rehash all the keys using hash modulo 2 instead of hash modulo 3 , and the new locations for the keys would become: KEY HASH HASH mod 2 \"john\" 1633428562 0 \"bill\" 7594634739 1 \"jane\" 5000799124 0 \"steve\" 9787173343 1 \"kate\" 3421657995 1 A B \"john\" \"bill\" \"jane\" \"steve\" \"kate\" Note that all key locations changed, not only the ones from server C . In the typical use case we mentioned before (caching), this would mean that, all of a sudden, the keys won\u2019t be found because they won\u2019t yet be present at their new location. So, most queries will result in misses, and the original data will likely need retrieving again from the source to be rehashed, thus placing a heavy load on the origin server(s) (typically a database). This may very well degrade performance severely and possibly crash the origin servers.","title":"The Rehashing Problem"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#the_solution_consistent_hashing","text":"So, how can this problem be solved? We need a distribution scheme that does not depend directly on the number of servers , so that, when adding or removing servers, the number of keys that need to be relocated is minimized. One such scheme\u2014a clever, yet surprisingly simple one\u2014is called consistent hashing , and was first described by Karger et al. at MIT in an academic paper from 1997 (according to Wikipedia). Consistent Hashing is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract circle , or hash ring . This allows servers and objects to scale without affecting the overall system. Imagine we mapped the hash output range onto the edge of a circle\uff08\u5c06hash\u8f93\u51fa\u503c\u7684\u503c\u57df\u6620\u5c04\u5230circle\u4e0a\uff09. That means that the minimum possible hash value , zero, would correspond to an angle of zero , the maximum possible value (some big integer we\u2019ll call INT_MAX ) would correspond to an angle of 2\ud835\udf45 radians (or 360 degrees), and all other hash values would linearly fit somewhere in between. So, we could take a key, compute its hash, and find out where it lies on the circle\u2019s edge. Assuming an INT_MAX of 1010 (for example\u2019s sake), the keys from our previous example would look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 Now imagine we also placed the servers on the edge of the circle , by pseudo-randomly assigning them angles too. This should be done in a repeatable\uff08\u53ef\u91cd\u590d\u7684\uff09 way (or at least in such a way that all clients agree on the servers\u2019 angles). A convenient way of doing this is by hashing the server name (or IP address, or some ID)\u2014as we\u2019d do with any other key\u2014to come up with its angle. In our example, things might look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 \"A\" 5572014558 200.6 \"B\" 8077113362 290.8 \"C\" 2269549488 81.7 Since we have the keys for both the objects and the servers on the same circle , we may define a simple rule to associate the former with the latter: Each object key will belong in the server whose key is closest , in a counterclockwise direction (or clockwise, depending on the conventions used). In other words, to find out which server to ask for a given key, we need to locate the key on the circle and move in the ascending angle direction until we find a server. In our example: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.7 \"C\" 2269549488 81.7 \"kate\" 3421657995 123.1 \"jane\" 5000799124 180 \"A\" 5572014557 200.5 \"bill\" 7594634739 273.4 \"B\" 8077113361 290.7 \"steve\" 787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"C\" C \"kate\" 3421831276 123.1 \"A\" A \"jane\" 5000648311 180 \"A\" A \"bill\" 7594873884 273.4 \"B\" B \"steve\" 9786437450 352.3 \"C\" C From a programming perspective, what we would do is keep a sorted list of server values (which could be angles or numbers in any real interval), and walk this list (or use a binary search) to find the first server with a value greater than, or equal to, that of the desired key. If no such value is found, we need to wrap around, taking the first one from the list. To ensure object keys are evenly distributed among servers, we need to apply a simple trick: To assign not one, but many labels (angles) to each server . So instead of having labels A , B and C , we could have, say, A0 .. A9 , B0 .. B9 and C0 .. C9 , all interspersed along the circle. The factor by which to increase the number of labels (server keys), known as weight , depends on the situation (and may even be different for each server) to adjust the probability of keys ending up on each. For example, if server B were twice as powerful as the rest, it could be assigned twice as many labels, and as a result, it would end up holding twice as many objects (on average). For our example we\u2019ll assume all three servers have an equal weight of 10 (this works well for three servers, for 10 to 50 servers, a weight in the range 100 to 500 would work better, and bigger pools may need even higher weights): KEY HASH ANGLE (DEG) \"C6\" 408965526 14.7 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"C4\" 1493080938 53.7 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"C0\" 1982701318 71.3 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"C9\" 3359725419 120.9 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"C1\" 3672205973 132.1 \"C8\" 3750588567 135 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"C7\" 5014097839 180.5 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"C3\" 7330467663 263.8 \"C5\" 7502566333 270 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"C2\" 8605012288 309.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"C7\" C \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"C6\" C So, what\u2019s the benefit of all this circle approach? Imagine server C is removed. To account for\uff08\u5f25\u8865\uff09 this, we must remove labels C0 .. C9 from the circle. This results in the object keys formerly adjacent to the deleted labels now being randomly labeled Ax and Bx , reassigning them to servers A and B . But what happens with the other object keys, the ones that originally belonged in A and B ? Nothing! That\u2019s the beauty of it: The absence of Cx labels does not affect those keys in any way. So, removing a server results in its object keys being randomly reassigned to the rest of the servers, leaving all other keys untouched : KEY HASH ANGLE (DEG) \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"A1\" A Something similar happens if, instead of removing a server, we add one. If we wanted to add server D to our example (say, as a replacement for C ), we would need to add labels D0 .. D9 . The result would be that roughly one-third of the existing keys (all belonging to A or B ) would be reassigned to D , and, again, the rest would stay the same: KEY HASH ANGLE (DEG) \"D2\" 439890723 15.8 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"D8\" 796709216 28.6 \"D1\" 1008580939 36.3 \"A3\" 1466730567 52.8 \"D5\" 1587548309 57.1 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"D4\" 2909395217 104.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"D7\" 3567129743 128.4 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"D6\" 5703092354 205.3 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"D0\" 8272587142 297.8 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"D3\" 9048608874 325.7 \"D9\" 9314459653 335.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"D2\" D This is how consistent hashing solves the rehashing problem. In general, only k/N keys need to be remapped when k is the number of keys and N is the number of servers (more specifically, the maximum of the initial and final number of servers).","title":"The Solution: Consistent Hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#what_next","text":"We observed that when using distributed caching to optimize performance, it may happen that the number of caching servers changes (reasons for this may be a server crashing, or the need to add or remove a server to increase or decrease overall capacity). By using consistent hashing to distribute keys between the servers, we can rest assured that should that happen, the number of keys being rehashed\u2014and therefore, the impact on origin servers\u2014will be minimized, preventing potential downtime or performance issues. There are clients for several systems, such as Memcached and Redis, that include support for consistent hashing out of the box. Alternatively, you can implement the algorithm yourself, in your language of choice, and that should be relatively easy once the concept is understood. If data science interests you, Toptal has some of the best articles on the subject at the blog","title":"What Next?"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/","text":"Consistent hashing History Need for consistent hashing Technique Complexity Examples of use Consistent hashing # In computer science , consistent hashing is a special kind of hashing such that when a hash table is resized, only $ K/n $ keys need to be remapped on average, where $ K $ is the number of keys, and $ n $ is the number of slots. In contrast, in most traditional hash tables , a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation . Consistent hashing achieves some of the goals of rendezvous hashing (also called HRW Hashing), which is more general, since consistent hashing has been shown to be a special case of rendezvous hashing . Rendezvous hashing was first described in 1996, while consistent hashing appeared in 1997. The two techniques use different algorithms. History # The term \" consistent hashing \" was introduced by Karger et al. at MIT for use in distributed caching . This academic paper from 1997 introduced the term \"consistent hashing\" as a way of distributing\uff08\u5206\u5e03\uff09 requests among a changing population of Web servers. Each slot is then represented by a node in a distributed system. The addition (joins) and removal (leaves/failures) of nodes only requires $ K/n $ items to be re-shuffled when the number of slots/nodes change. The authors mention Linear hashing and its ability to handle sequential addition and removal of nodes, while consistent hashing allows buckets to be added and removed in arbitrary order. [ 1] Teradata used this technique in their distributed database, released in 1986, although they did not use this term. Teradata still use the concept of a Hash table to fulfill exactly this purpose. Akamai Technologies was founded in 1998 by the scientists Daniel Lewin and F. Thomson Leighton (co-authors of the article coining \"consistent hashing\") to apply this algorithm, which gave birth to the content delivery network industry. Consistent hashing has also been used to reduce the impact of partial system failures in large Web applications as to allow for robust caches without incurring the system wide fallout of a failure.[ 2] The consistent hashing concept also applies to the design of distributed hash tables (DHTs). DHTs use consistent hashing to partition a keyspace among a distributed set of nodes, and additionally provide an overlay network that connects nodes such that the node responsible for any key can be efficiently located. Rendezvous hashing , designed at the same time as consistent hashing, achieves the same goals using the very different Highest Random Weight (HRW) algorithm. Need for consistent hashing # While running collections of caching machines some limitations are experienced. A common way of load balancing $ n $ cache machines is to put object $ o $ in cache machine number $ {\\text{hash}}(o)\\;\\left({\\text{mod }}n\\right) $. But this will not work if a cache machine is added or removed because $ n $ changes and every object is hashed to a new location. This can be disastrous since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping of servers. Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared among the remaining machines. The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed its interval is taken over by a cache with an adjacent interval. All the remaining caches are unchanged. Technique # Consistent hashing is based on mapping each object to a point on a circle (or equivalently, mapping each object to a real angle). The system maps each available machine (or other storage bucket) to many pseudo-randomly distributed points on the same circle. To find where an object should be placed, the system finds the location of that object's key on the circle; then walks around the circle until falling into the first bucket it encounters (or equivalently, the first available bucket with a higher angle). The result is that each bucket contains all the resources located between each one of its points and the previous points that belong to other buckets. If a bucket becomes unavailable (for example because the computer it resides on is not reachable), then the points it maps to will be removed. Requests for resources that would have mapped to each of those points now map to the next highest points. Since each bucket is associated with many pseudo-randomly distributed points, the resources that were held by that bucket will now map to many different buckets. The items that mapped to the lost bucket must be redistributed among the remaining ones, but values mapping to other buckets will still do so and do not need to be moved. A similar process occurs when a bucket is added. By adding new bucket points, we make any resources between those and the points corresponding to the next smaller angles map to the new bucket. These resources will no longer be associated with the previous buckets, and any value previously stored there will not be found by the selection method described above. The portion of the keys associated with each bucket can be altered by altering the number of angles that bucket maps to. Complexity # Asymptotic time complexities for N nodes (or slots) and K keys Classic hash table Consistent hashing add a node O(K) O(K/N + log(N)) remove a node O(K) O(K/N + log(N)) add a key O(1) O(log(N)) remove a key O(1) O(log(N)) The O(log(N)) complexity for consistent hashing comes from the fact that a binary search among nodes angles is required to find the next node on the ring. Examples of use # Some known instances where consistent hashing is used are: Couchbase automated data partitioning Openstack 's Object Storage Service Swift[ 3] Partitioning component of Amazon's storage system Dynamo [ 4] Data partitioning in Apache Cassandra [ 5] Data Partitioning in Voldemort [ 6] Akka 's consistent hashing router[ 7] Riak , a distributed key-value database[ 8] GlusterFS , a network-attached storage file system[ 9] Skylable , an open-source distributed object-storage system [ 10] Akamai Content Delivery Network [ 11] Discord chat application [ 12] Maglev: A Fast and Reliable Software Network Load Balancer [ 13]","title":"wikipedia Consistent hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/#consistent_hashing","text":"In computer science , consistent hashing is a special kind of hashing such that when a hash table is resized, only $ K/n $ keys need to be remapped on average, where $ K $ is the number of keys, and $ n $ is the number of slots. In contrast, in most traditional hash tables , a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation . Consistent hashing achieves some of the goals of rendezvous hashing (also called HRW Hashing), which is more general, since consistent hashing has been shown to be a special case of rendezvous hashing . Rendezvous hashing was first described in 1996, while consistent hashing appeared in 1997. The two techniques use different algorithms.","title":"Consistent hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/#history","text":"The term \" consistent hashing \" was introduced by Karger et al. at MIT for use in distributed caching . This academic paper from 1997 introduced the term \"consistent hashing\" as a way of distributing\uff08\u5206\u5e03\uff09 requests among a changing population of Web servers. Each slot is then represented by a node in a distributed system. The addition (joins) and removal (leaves/failures) of nodes only requires $ K/n $ items to be re-shuffled when the number of slots/nodes change. The authors mention Linear hashing and its ability to handle sequential addition and removal of nodes, while consistent hashing allows buckets to be added and removed in arbitrary order. [ 1] Teradata used this technique in their distributed database, released in 1986, although they did not use this term. Teradata still use the concept of a Hash table to fulfill exactly this purpose. Akamai Technologies was founded in 1998 by the scientists Daniel Lewin and F. Thomson Leighton (co-authors of the article coining \"consistent hashing\") to apply this algorithm, which gave birth to the content delivery network industry. Consistent hashing has also been used to reduce the impact of partial system failures in large Web applications as to allow for robust caches without incurring the system wide fallout of a failure.[ 2] The consistent hashing concept also applies to the design of distributed hash tables (DHTs). DHTs use consistent hashing to partition a keyspace among a distributed set of nodes, and additionally provide an overlay network that connects nodes such that the node responsible for any key can be efficiently located. Rendezvous hashing , designed at the same time as consistent hashing, achieves the same goals using the very different Highest Random Weight (HRW) algorithm.","title":"History"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/#need_for_consistent_hashing","text":"While running collections of caching machines some limitations are experienced. A common way of load balancing $ n $ cache machines is to put object $ o $ in cache machine number $ {\\text{hash}}(o)\\;\\left({\\text{mod }}n\\right) $. But this will not work if a cache machine is added or removed because $ n $ changes and every object is hashed to a new location. This can be disastrous since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping of servers. Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared among the remaining machines. The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed its interval is taken over by a cache with an adjacent interval. All the remaining caches are unchanged.","title":"Need for consistent hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/#technique","text":"Consistent hashing is based on mapping each object to a point on a circle (or equivalently, mapping each object to a real angle). The system maps each available machine (or other storage bucket) to many pseudo-randomly distributed points on the same circle. To find where an object should be placed, the system finds the location of that object's key on the circle; then walks around the circle until falling into the first bucket it encounters (or equivalently, the first available bucket with a higher angle). The result is that each bucket contains all the resources located between each one of its points and the previous points that belong to other buckets. If a bucket becomes unavailable (for example because the computer it resides on is not reachable), then the points it maps to will be removed. Requests for resources that would have mapped to each of those points now map to the next highest points. Since each bucket is associated with many pseudo-randomly distributed points, the resources that were held by that bucket will now map to many different buckets. The items that mapped to the lost bucket must be redistributed among the remaining ones, but values mapping to other buckets will still do so and do not need to be moved. A similar process occurs when a bucket is added. By adding new bucket points, we make any resources between those and the points corresponding to the next smaller angles map to the new bucket. These resources will no longer be associated with the previous buckets, and any value previously stored there will not be found by the selection method described above. The portion of the keys associated with each bucket can be altered by altering the number of angles that bucket maps to.","title":"Technique"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/#complexity","text":"Asymptotic time complexities for N nodes (or slots) and K keys Classic hash table Consistent hashing add a node O(K) O(K/N + log(N)) remove a node O(K) O(K/N + log(N)) add a key O(1) O(log(N)) remove a key O(1) O(log(N)) The O(log(N)) complexity for consistent hashing comes from the fact that a binary search among nodes angles is required to find the next node on the ring.","title":"Complexity"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Consistent-hashing/wikipedia-Consistent-hashing/#examples_of_use","text":"Some known instances where consistent hashing is used are: Couchbase automated data partitioning Openstack 's Object Storage Service Swift[ 3] Partitioning component of Amazon's storage system Dynamo [ 4] Data partitioning in Apache Cassandra [ 5] Data Partitioning in Voldemort [ 6] Akka 's consistent hashing router[ 7] Riak , a distributed key-value database[ 8] GlusterFS , a network-attached storage file system[ 9] Skylable , an open-source distributed object-storage system [ 10] Akamai Content Delivery Network [ 11] Discord chat application [ 12] Maglev: A Fast and Reliable Software Network Load Balancer [ 13]","title":"Examples of use"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/Rendezvous-hashing-reading-list/","text":"Google redis Rendezvous hashing # https://github.com/drainingsun/ared https://en.wikipedia.org/wiki/Distributed_hash_table https://medium.com/ @dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8 https://link.springer.com/chapter/10.1007/978-3-030-29859-3_3 https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/ rendezvous hashing vs consistent hashing # https://github.com/clohfink/RendezvousHash https://stackoverflow.com/questions/20790898/consistent-hashing-vs-rendezvous-hrw-hashing-what-are-the-tradeoffs https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/ https://www.pvk.ca/Blog/2017/09/24/rendezvous-hashing-my-baseline-consistent-distribution-method/","title":"Google redis Rendezvous hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/Rendezvous-hashing-reading-list/#google_redis_rendezvous_hashing","text":"https://github.com/drainingsun/ared https://en.wikipedia.org/wiki/Distributed_hash_table https://medium.com/ @dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8 https://link.springer.com/chapter/10.1007/978-3-030-29859-3_3 https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/","title":"Google redis Rendezvous hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/Rendezvous-hashing-reading-list/#rendezvous_hashing_vs_consistent_hashing","text":"https://github.com/clohfink/RendezvousHash https://stackoverflow.com/questions/20790898/consistent-hashing-vs-rendezvous-hrw-hashing-what-are-the-tradeoffs https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/ https://www.pvk.ca/Blog/2017/09/24/rendezvous-hashing-my-baseline-consistent-distribution-method/","title":"rendezvous hashing vs consistent hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/wikipedia-Rendezvous-hashing/","text":"Rendezvous hashing History The HRW algorithm for rendezvous hashing Rendezvous hashing # Rendezvous or highest random weight (HRW) hashing [ 1] [ 2] is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to. As shown below, Rendezvous hashing is more general than Consistent hashing , which becomes a special case (for k = 1) of Rendezvous hashing. \u96c6\u5408\u6216\u6700\u9ad8\u968f\u673a\u6743\u91cd\uff08HRW\uff09\u6563\u5217[1] [2]\u662f\u4e00\u79cd\u7b97\u6cd5\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u5728\u4e00\u7ec4\u53ef\u80fd\u7684n\u4e2a\u9009\u9879\u4e2d\u5b9e\u73b0\u5bf9\u4e00\u7ec4k\u4e2a\u9009\u9879\u7684\u5206\u5e03\u5f0f\u534f\u8bae\u3002 \u5178\u578b\u7684\u5e94\u7528\u662f\u5f53\u5ba2\u6237\u9700\u8981\u5c31\u5c06objects\u5206\u914d\u4e86\u54ea\u4e9b\u7ad9\u70b9\uff08\u6216\u4ee3\u7406\uff09\u8fbe\u6210\u4e00\u81f4\u3002 \u5982\u4e0b\u6240\u793a\uff0cRendezvous\u54c8\u5e0c\u6bd4\u4e00\u81f4\u54c8\u5e0c\u66f4\u901a\u7528\uff0c\u5b83Rendezvous\u54c8\u5e0c\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff08\u5bf9\u4e8ek = 1\uff09\u3002 History # Rendezvous hashing was invented by David Thaler and Chinya Ravishankar at the University of Michigan in 1996[ 1] . Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE ) to identify multicast rendezvous points in a distributed fashion.[ 3] [ 4] It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing.[ 5] [ 6] Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.[ 1] Rendezvous\u54c8\u5e0c\u662f1996\u5e74\u7531\u5bc6\u6b47\u6839\u5927\u5b66\u7684David Thaler\u548cChinya Ravishankar\u53d1\u660e\u7684\u3002 \u4e00\u5e74\u540e\uff0c\u6587\u732e\u4e2d\u51fa\u73b0\u4e86\u4e00\u81f4\u7684\u6563\u5217\u3002 \u96c6\u5408\u6563\u5217\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u4e4b\u4e00\u662f\u4f7fInternet\u4e0a\u7684\u591a\u64ad\u5ba2\u6237\u7aef\u80fd\u591f\u4ee5\u5206\u5e03\u5f0f\u65b9\u5f0f\u8bc6\u522b\u591a\u64ad\u96c6\u5408\u70b9\u3002 \u5b83\u4e8e1998\u5e74\u88ab\u5fae\u8f6f\u7684\u7f13\u5b58\u9635\u5217\u8def\u7531\u534f\u8bae\u7528\u4e8e\u5206\u5e03\u5f0f\u7f13\u5b58\u534f\u8c03\u548c\u8def\u7531\u3002 \u67d0\u4e9b\u534f\u8bae\u72ec\u7acb\u591a\u64ad\u8def\u7531\u534f\u8bae\u4f7f\u7528\u96c6\u5408\u6563\u5217\u6765\u9009\u62e9\u96c6\u5408\u70b9\u3002 Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching,[ 7] router design,[ 8] secure key establishment ,[ 9] and sharding and distributed databases.[ 10] SUMMARY : \u4e0a\u8ff0\u8fd9\u4e9b\u9886\u57df\u662f\u975e\u5e38\u91cd\u8981\u7684 The HRW algorithm for rendezvous hashing # Rendezvous hashing solves the distributed hash table problem: How can a set of clients, given an object O , agree on where in a set of n sites (servers, say) to place O ? Each client is to select a site independently, but all clients must end up picking the same site. This is non-trivial if we add a minimal disruption constraint, and require that only objects mapping to a removed site may be reassigned to other sites. The basic idea is to give each site Sj a score (a weight ) for each object Oi , and assign the object to the highest scoring site. All clients first agree on a hash function h() . For object Oi , the site Sj is defined to have weight wi,j = h(Oi, Sj) . HRW assigns Oi to the site Sm whose weight wi,m is the largest. Since h() is agreed upon, each client can independently compute the weights wi,1, wi,2, ..., wi,n and pick the largest. If the goal is distributed k -agreement, the clients can independently pick the sites with the k largest hash values. If a site S is added or removed, only the objects mapping to S are remapped to different sites, satisfying the minimal disruption constraint above. The HRW assignment can be computed independently by any client, since it depends only on the identifiers for the set of sites S1, S2, ..., Sn and the object being assigned. HRW easily accommodates different capacities among sites. If site Sk has twice the capacity of the other sites, we simply represent Sk twice in the list, say, as Sk,1 and Sk,2 . Clearly, twice as many objects will now map to Sk as to the other sites.","title":"wikipedia Rendezvous hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/wikipedia-Rendezvous-hashing/#rendezvous_hashing","text":"Rendezvous or highest random weight (HRW) hashing [ 1] [ 2] is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to. As shown below, Rendezvous hashing is more general than Consistent hashing , which becomes a special case (for k = 1) of Rendezvous hashing. \u96c6\u5408\u6216\u6700\u9ad8\u968f\u673a\u6743\u91cd\uff08HRW\uff09\u6563\u5217[1] [2]\u662f\u4e00\u79cd\u7b97\u6cd5\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u5728\u4e00\u7ec4\u53ef\u80fd\u7684n\u4e2a\u9009\u9879\u4e2d\u5b9e\u73b0\u5bf9\u4e00\u7ec4k\u4e2a\u9009\u9879\u7684\u5206\u5e03\u5f0f\u534f\u8bae\u3002 \u5178\u578b\u7684\u5e94\u7528\u662f\u5f53\u5ba2\u6237\u9700\u8981\u5c31\u5c06objects\u5206\u914d\u4e86\u54ea\u4e9b\u7ad9\u70b9\uff08\u6216\u4ee3\u7406\uff09\u8fbe\u6210\u4e00\u81f4\u3002 \u5982\u4e0b\u6240\u793a\uff0cRendezvous\u54c8\u5e0c\u6bd4\u4e00\u81f4\u54c8\u5e0c\u66f4\u901a\u7528\uff0c\u5b83Rendezvous\u54c8\u5e0c\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff08\u5bf9\u4e8ek = 1\uff09\u3002","title":"Rendezvous hashing"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/wikipedia-Rendezvous-hashing/#history","text":"Rendezvous hashing was invented by David Thaler and Chinya Ravishankar at the University of Michigan in 1996[ 1] . Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE ) to identify multicast rendezvous points in a distributed fashion.[ 3] [ 4] It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing.[ 5] [ 6] Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.[ 1] Rendezvous\u54c8\u5e0c\u662f1996\u5e74\u7531\u5bc6\u6b47\u6839\u5927\u5b66\u7684David Thaler\u548cChinya Ravishankar\u53d1\u660e\u7684\u3002 \u4e00\u5e74\u540e\uff0c\u6587\u732e\u4e2d\u51fa\u73b0\u4e86\u4e00\u81f4\u7684\u6563\u5217\u3002 \u96c6\u5408\u6563\u5217\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u4e4b\u4e00\u662f\u4f7fInternet\u4e0a\u7684\u591a\u64ad\u5ba2\u6237\u7aef\u80fd\u591f\u4ee5\u5206\u5e03\u5f0f\u65b9\u5f0f\u8bc6\u522b\u591a\u64ad\u96c6\u5408\u70b9\u3002 \u5b83\u4e8e1998\u5e74\u88ab\u5fae\u8f6f\u7684\u7f13\u5b58\u9635\u5217\u8def\u7531\u534f\u8bae\u7528\u4e8e\u5206\u5e03\u5f0f\u7f13\u5b58\u534f\u8c03\u548c\u8def\u7531\u3002 \u67d0\u4e9b\u534f\u8bae\u72ec\u7acb\u591a\u64ad\u8def\u7531\u534f\u8bae\u4f7f\u7528\u96c6\u5408\u6563\u5217\u6765\u9009\u62e9\u96c6\u5408\u70b9\u3002 Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching,[ 7] router design,[ 8] secure key establishment ,[ 9] and sharding and distributed databases.[ 10] SUMMARY : \u4e0a\u8ff0\u8fd9\u4e9b\u9886\u57df\u662f\u975e\u5e38\u91cd\u8981\u7684","title":"History"},{"location":"Hash/Hash-based-data-structures/Distributed-hash-table/Rendezvous-hashing/wikipedia-Rendezvous-hashing/#the_hrw_algorithm_for_rendezvous_hashing","text":"Rendezvous hashing solves the distributed hash table problem: How can a set of clients, given an object O , agree on where in a set of n sites (servers, say) to place O ? Each client is to select a site independently, but all clients must end up picking the same site. This is non-trivial if we add a minimal disruption constraint, and require that only objects mapping to a removed site may be reassigned to other sites. The basic idea is to give each site Sj a score (a weight ) for each object Oi , and assign the object to the highest scoring site. All clients first agree on a hash function h() . For object Oi , the site Sj is defined to have weight wi,j = h(Oi, Sj) . HRW assigns Oi to the site Sm whose weight wi,m is the largest. Since h() is agreed upon, each client can independently compute the weights wi,1, wi,2, ..., wi,n and pick the largest. If the goal is distributed k -agreement, the clients can independently pick the sites with the k largest hash values. If a site S is added or removed, only the objects mapping to S are remapped to different sites, satisfying the minimal disruption constraint above. The HRW assignment can be computed independently by any client, since it depends only on the identifiers for the set of sites S1, S2, ..., Sn and the object being assigned. HRW easily accommodates different capacities among sites. If site Sk has twice the capacity of the other sites, we simply represent Sk twice in the list, say, as Sk,1 and Sk,2 . Clearly, twice as many objects will now map to Sk as to the other sites.","title":"The HRW algorithm for rendezvous hashing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/","text":"Hash table Hashing Choosing a hash function Key statistics Collision resolution Separate chaining Separate chaining with linked lists Separate chaining with list head cells Separate chaining with other structures Open addressing Dynamic resizing Resizing by copying all entries Alternatives to all-at-once rehashing Incremental resizing Monotonic keys Linear hashing Hashing for distributed hash tables Performance analysis Features Advantages Drawbacks Uses Associative arrays Database indexing Caches Sets Object representation Unique data representation Transposition table Implemtentation An Analysis of Hash Map Implementations in Popular Languages CPython dict Hash table # In computing , a hash table ( hash map ) is a data structure that implements an associative array abstract data type , a structure that can map keys to values . A hash table uses a hash function to compute an index into an array of buckets or slots , from which the desired value can be found. SUMMARY : Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key. Such collisions must be accommodated in some way. In a well-dimensioned hash table, the average cost (number of instructions ) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at ( amortized [ 2] ) constant average cost per operation.[ 3] [ 4] In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software , particularly for associative arrays , database indexing , caches , and sets . Hashing # Main article: Hash function The idea of hashing is to distribute the entries (key/value pairs) across an array of buckets . Given a key, the algorithm computes an index that suggests where the entry can be found: index = f(key, array_size) Often this is done in two steps: hash = hashfunc(key) index = hash % array_size In this method, the hash is independent of the array size, and it is then reduced to an index (a number between 0 and array_size \u2212 1 ) using the modulo operator ( % ). In the case that the array size is a power of two , the remainder operation is reduced to masking , which improves speed, but can increase problems with a poor hash function.[ 5] Choosing a hash function # A basic requirement is that the function should provide a uniform distribution \uff08\u79bb\u6563\u5747\u5300\u5206\u5e03\uff09of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity\uff08\u5747\u5300\u6027\uff09 is sometimes difficult to ensure by design, but may be evaluated empirically\uff08\u5148\u9a8c\u5730\uff09 using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[ 6] [ 7] The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two . Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number .[ 8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function. SUMMARY : \u5728\u8bbe\u8ba1hash function\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u8fd8\u9700\u8981\u8003\u8651\u7684\u662fhash\u503c\u662f\u5426\u9700\u8981\u5728table size\u8303\u56f4\u5185\u5747\u5300\u5206\u5e03\uff1b\u4ee5\u53ca\u5f53table size\u53d8\u66f4\u7684\u65f6\u5019\u6240\u9700\u8981\u8003\u8651\u7684\u4e00\u7cfb\u5217\u95ee\u9898\uff1b For open addressing schemes, the hash function should also avoid clustering , the mapping of two or more keys to consecutive\uff08\u8fde\u7eed\u7684\uff09 slots. Such clustering may cause the lookup cost to skyrocket\uff08\u98de\u6da8\uff09, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[ 3] is claimed to have particularly poor clustering behavior.[ 8] Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]. They may also be appropriate if there is a risk of malicious\uff08\u6076\u6bd2\u7684\uff09 users trying to sabotage \uff08\u84c4\u610f\u7834\u574f\uff09a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function ). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[ citation needed ] SUMMARY : \u663e\u7136\uff0c Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]\u7684\u8fd9\u4e2a\u7279\u6027\u662f\u975e\u5e38\u597d\u7684\uff0c\u5b83\u5141\u8bb8\u4f7f\u7528\u6237\u65e0\u9700\u8003\u8651\u6539\u53d8table size\u6240\u5e26\u6765\u7684\u5404\u79cd\u95ee\u9898\uff1b Key statistics # A critical statistic for a hash table is the load factor , defined as $ {\\text{load factor}}={\\frac {n}{k}}, $ where n is the number of entries occupied in the hash table. k is the number of buckets\uff08\u5176\u5b9e\u5c31\u662ftable size\uff09. As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which \"offers a good trade-off between time and space costs.\"[ 9] Second to the load factor, one can examine the variance of number of entries per bucket . For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one. A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory. Collision resolution # Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot. Therefore, almost all hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values. Separate chaining # In the method known as separate chaining , each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing function needs to be fixed.[ citation needed ] Separate chaining with linked lists # Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.[ citation needed ] The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform , the average cost of a lookup depends only on the average number of keys per bucket\u2014that is, it is roughly proportional to the load factor. For this reason, chained hash tables remain effective even when the number of table entries n is much higher than the number of slots. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list. For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number n of entries in the table. The bucket chains are often searched sequentially using the order the entries were added to the bucket. If the load factor is large and some keys are more likely to come up than others, then rearranging the chain with a move-to-front heuristic may be effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.[ citation needed ] Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the next pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance , making the processor cache ineffective. Separate chaining with list head cells # Separate chaining with other structures # Open addressing # Main article: Open addressing Dynamic resizing # When an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be rehashed .[ 9] Rehashing includes increasing the size of the underlying data structure[ 9] and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.[ 9] To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table\u2014followed by a rehash\u2014when items are deleted. From the point of space\u2013time tradeoffs, this operation is similar to the deallocation in dynamic arrays. Resizing by copying all entries # A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold r max. Then a new larger table is allocated , each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Likewise, when the load factor falls below a second threshold r min, all entries are moved to a new smaller table. For hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a \"shrink-to-fit\" operation can be provided that does this only on request. If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries n and of the number m of operations performed. For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If m elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most m \u2212 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation. Alternatives to all-at-once rehashing # Some hash table implementations, notably in real-time systems , cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually \uff08\u6e10\u8fdb\u5f0f\uff09. Disk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high. Incremental resizing # One alternative to enlarging the table all at once is to perform the rehashing gradually: During the resize, allocate the new hash table, but keep the old table unchanged. In each lookup or delete operation, check both tables. Perform insertion operations only in the new table. At each insertion also move r elements from the old table to the new table. When all elements are removed from the old table, deallocate it. To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it is necessary to increase the size of the table by a factor of at least ( r + 1)/ r during resizing. Monotonic keys # If it is known that key values will always increase (or decrease) monotonically , then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function\u2014and indeed hash table\u2014both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(log( N )) ranges to check, and binary search time for the redirection would be O(log(log( N ))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown. Linear hashing # Linear hashing [ 25] is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions. Hashing for distributed hash tables # Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent\uff08\u6d41\u884c\u7684\uff09 in disk-based and distributed hash tables , where rehashing is prohibitively costly. The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem. The four most popular approaches are rendezvous hashing , consistent hashing , the content addressable network algorithm, and Kademlia distance. Performance analysis # In the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size $ k $ with open addressing has no collisions and holds up to $ k $ elements with a single comparison for successful lookup, while a table of size $ k $ with chaining and $ n $ keys has the minimum $ max(0,n-k) $ collisions and $ \\Theta (1+{\\frac {n}{k}}) $ comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with $ \\Theta (n) $ amortized comparisons per insertion and up to $ n $ comparisons for a successful lookup. Adding rehashing to this model is straightforward. As in a dynamic array , geometric resizing by a factor of $ b $ implies that only $ {\\frac {n}{b^{i}}} $keys are inserted $ i $ or more times, so that the total number of insertions is bounded above by $ {\\frac {bn}{b-1}} $, which is $ \\Theta (n) $. By using rehashing to maintain $ n<k $, tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function. In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform , the assumption is called \"simple uniform hashing\" and it can be shown that hashing with chaining requires $ \\Theta (1+{\\frac {n}{k}}) $ comparisons on average for an unsuccessful lookup, and hashing with open addressing requires $ \\Theta \\left({\\frac {1}{1-n/k}}\\right) $.[ 26] Both these bounds are constant, if we maintain '$ {\\frac {n}{k}}<c $ using table resizing, where $ c $ is a fixed constant less than 1. Features # Advantages # The main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance, so that the bucket array can be allocated once with the optimum size and never resized. If the set of key-value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect. In this case the keys need not be stored in the table. Drawbacks # Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.) For certain string processing applications, such as spell-checking , hash tables may be less efficient than tries , finite automata , or Judy arrays . Also, if there are not too many possible keys to store\u2014that is, if each key can be represented by a small enough number of bits\u2014then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case. The entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is nearest to a given key. Listing all n entries in some specific order generally requires a separate sorting step, whose cost is proportional to log( n ) per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log( n ), but allow finding the nearest key at about the same cost, and ordered enumeration of all entries at constant cost per entry. If the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment. Although the average cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses dynamic resizing , an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications. Hash tables in general exhibit poor locality of reference \u2014that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger microprocessor cache misses that cause long delays. Compact data structures such as arrays searched with linear search may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system. Hash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a malicious adversary with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g., a denial of service attack .[ 27] [ 28] [ 29] In critical applications, a data structure with better worst-case guarantees can be used; however, universal hashing \u2014a randomized algorithm that prevents the attacker from predicting which inputs cause worst-case behavior\u2014may be preferable.[ 30] The hash function used by the hash table in the Linux routing table cache was changed with Linux version 2.4.2 as a countermeasure against such attacks.[ 31] Uses # Associative arrays # Main article: Associative array Database indexing # Hash tables may also be used as disk -based data structures and database indices (such as in dbm ) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins. Caches # Main article: Cache (computing) Hash tables can be used to implement caches , auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries\u2014usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value. Sets # Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not. Those structures can therefore be used to implement a set data structure , which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets. Object representation # Several dynamic languages, such as Perl , Python , JavaScript , Lua , and Ruby , use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method. Unique data representation # Main article: String interning Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single string pool implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing , and can be used with many other kinds of data ( expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.). Transposition table # Main article: Transposition table Implemtentation # An Analysis of Hash Map Implementations in Popular Languages # CPython dict # https://github.com/zpoint/CPython-Internals/blob/master/BasicObject/dict/dict.md https://github.com/python/cpython/blob/master/Objects/dictnotes.txt","title":"wikipedia Hash table"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#hash_table","text":"In computing , a hash table ( hash map ) is a data structure that implements an associative array abstract data type , a structure that can map keys to values . A hash table uses a hash function to compute an index into an array of buckets or slots , from which the desired value can be found. SUMMARY : Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key. Such collisions must be accommodated in some way. In a well-dimensioned hash table, the average cost (number of instructions ) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at ( amortized [ 2] ) constant average cost per operation.[ 3] [ 4] In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software , particularly for associative arrays , database indexing , caches , and sets .","title":"Hash table"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#hashing","text":"Main article: Hash function The idea of hashing is to distribute the entries (key/value pairs) across an array of buckets . Given a key, the algorithm computes an index that suggests where the entry can be found: index = f(key, array_size) Often this is done in two steps: hash = hashfunc(key) index = hash % array_size In this method, the hash is independent of the array size, and it is then reduced to an index (a number between 0 and array_size \u2212 1 ) using the modulo operator ( % ). In the case that the array size is a power of two , the remainder operation is reduced to masking , which improves speed, but can increase problems with a poor hash function.[ 5]","title":"Hashing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#choosing_a_hash_function","text":"A basic requirement is that the function should provide a uniform distribution \uff08\u79bb\u6563\u5747\u5300\u5206\u5e03\uff09of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity\uff08\u5747\u5300\u6027\uff09 is sometimes difficult to ensure by design, but may be evaluated empirically\uff08\u5148\u9a8c\u5730\uff09 using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[ 6] [ 7] The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two . Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number .[ 8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function. SUMMARY : \u5728\u8bbe\u8ba1hash function\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u8fd8\u9700\u8981\u8003\u8651\u7684\u662fhash\u503c\u662f\u5426\u9700\u8981\u5728table size\u8303\u56f4\u5185\u5747\u5300\u5206\u5e03\uff1b\u4ee5\u53ca\u5f53table size\u53d8\u66f4\u7684\u65f6\u5019\u6240\u9700\u8981\u8003\u8651\u7684\u4e00\u7cfb\u5217\u95ee\u9898\uff1b For open addressing schemes, the hash function should also avoid clustering , the mapping of two or more keys to consecutive\uff08\u8fde\u7eed\u7684\uff09 slots. Such clustering may cause the lookup cost to skyrocket\uff08\u98de\u6da8\uff09, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[ 3] is claimed to have particularly poor clustering behavior.[ 8] Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]. They may also be appropriate if there is a risk of malicious\uff08\u6076\u6bd2\u7684\uff09 users trying to sabotage \uff08\u84c4\u610f\u7834\u574f\uff09a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function ). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[ citation needed ] SUMMARY : \u663e\u7136\uff0c Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]\u7684\u8fd9\u4e2a\u7279\u6027\u662f\u975e\u5e38\u597d\u7684\uff0c\u5b83\u5141\u8bb8\u4f7f\u7528\u6237\u65e0\u9700\u8003\u8651\u6539\u53d8table size\u6240\u5e26\u6765\u7684\u5404\u79cd\u95ee\u9898\uff1b","title":"Choosing a hash function"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#key_statistics","text":"A critical statistic for a hash table is the load factor , defined as $ {\\text{load factor}}={\\frac {n}{k}}, $ where n is the number of entries occupied in the hash table. k is the number of buckets\uff08\u5176\u5b9e\u5c31\u662ftable size\uff09. As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which \"offers a good trade-off between time and space costs.\"[ 9] Second to the load factor, one can examine the variance of number of entries per bucket . For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one. A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.","title":"Key statistics"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#collision_resolution","text":"Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot. Therefore, almost all hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values.","title":"Collision resolution"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#separate_chaining","text":"In the method known as separate chaining , each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing function needs to be fixed.[ citation needed ]","title":"Separate chaining"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#separate_chaining_with_linked_lists","text":"Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.[ citation needed ] The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform , the average cost of a lookup depends only on the average number of keys per bucket\u2014that is, it is roughly proportional to the load factor. For this reason, chained hash tables remain effective even when the number of table entries n is much higher than the number of slots. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list. For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number n of entries in the table. The bucket chains are often searched sequentially using the order the entries were added to the bucket. If the load factor is large and some keys are more likely to come up than others, then rearranging the chain with a move-to-front heuristic may be effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.[ citation needed ] Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the next pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance , making the processor cache ineffective.","title":"Separate chaining with linked lists"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#separate_chaining_with_list_head_cells","text":"","title":"Separate chaining with list head cells"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#separate_chaining_with_other_structures","text":"","title":"Separate chaining with other structures"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#open_addressing","text":"Main article: Open addressing","title":"Open addressing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#dynamic_resizing","text":"When an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be rehashed .[ 9] Rehashing includes increasing the size of the underlying data structure[ 9] and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.[ 9] To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table\u2014followed by a rehash\u2014when items are deleted. From the point of space\u2013time tradeoffs, this operation is similar to the deallocation in dynamic arrays.","title":"Dynamic resizing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#resizing_by_copying_all_entries","text":"A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold r max. Then a new larger table is allocated , each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Likewise, when the load factor falls below a second threshold r min, all entries are moved to a new smaller table. For hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a \"shrink-to-fit\" operation can be provided that does this only on request. If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries n and of the number m of operations performed. For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If m elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most m \u2212 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.","title":"Resizing by copying all entries"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#alternatives_to_all-at-once_rehashing","text":"Some hash table implementations, notably in real-time systems , cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually \uff08\u6e10\u8fdb\u5f0f\uff09. Disk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high.","title":"Alternatives to all-at-once rehashing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#incremental_resizing","text":"One alternative to enlarging the table all at once is to perform the rehashing gradually: During the resize, allocate the new hash table, but keep the old table unchanged. In each lookup or delete operation, check both tables. Perform insertion operations only in the new table. At each insertion also move r elements from the old table to the new table. When all elements are removed from the old table, deallocate it. To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it is necessary to increase the size of the table by a factor of at least ( r + 1)/ r during resizing.","title":"Incremental resizing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#monotonic_keys","text":"If it is known that key values will always increase (or decrease) monotonically , then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function\u2014and indeed hash table\u2014both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(log( N )) ranges to check, and binary search time for the redirection would be O(log(log( N ))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown.","title":"Monotonic keys"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#linear_hashing","text":"Linear hashing [ 25] is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions.","title":"Linear hashing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#hashing_for_distributed_hash_tables","text":"Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent\uff08\u6d41\u884c\u7684\uff09 in disk-based and distributed hash tables , where rehashing is prohibitively costly. The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem. The four most popular approaches are rendezvous hashing , consistent hashing , the content addressable network algorithm, and Kademlia distance.","title":"Hashing for distributed hash tables"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#performance_analysis","text":"In the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size $ k $ with open addressing has no collisions and holds up to $ k $ elements with a single comparison for successful lookup, while a table of size $ k $ with chaining and $ n $ keys has the minimum $ max(0,n-k) $ collisions and $ \\Theta (1+{\\frac {n}{k}}) $ comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with $ \\Theta (n) $ amortized comparisons per insertion and up to $ n $ comparisons for a successful lookup. Adding rehashing to this model is straightforward. As in a dynamic array , geometric resizing by a factor of $ b $ implies that only $ {\\frac {n}{b^{i}}} $keys are inserted $ i $ or more times, so that the total number of insertions is bounded above by $ {\\frac {bn}{b-1}} $, which is $ \\Theta (n) $. By using rehashing to maintain $ n<k $, tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function. In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform , the assumption is called \"simple uniform hashing\" and it can be shown that hashing with chaining requires $ \\Theta (1+{\\frac {n}{k}}) $ comparisons on average for an unsuccessful lookup, and hashing with open addressing requires $ \\Theta \\left({\\frac {1}{1-n/k}}\\right) $.[ 26] Both these bounds are constant, if we maintain '$ {\\frac {n}{k}}<c $ using table resizing, where $ c $ is a fixed constant less than 1.","title":"Performance analysis"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#features","text":"","title":"Features"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#advantages","text":"The main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance, so that the bucket array can be allocated once with the optimum size and never resized. If the set of key-value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect. In this case the keys need not be stored in the table.","title":"Advantages"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#drawbacks","text":"Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.) For certain string processing applications, such as spell-checking , hash tables may be less efficient than tries , finite automata , or Judy arrays . Also, if there are not too many possible keys to store\u2014that is, if each key can be represented by a small enough number of bits\u2014then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case. The entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is nearest to a given key. Listing all n entries in some specific order generally requires a separate sorting step, whose cost is proportional to log( n ) per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log( n ), but allow finding the nearest key at about the same cost, and ordered enumeration of all entries at constant cost per entry. If the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment. Although the average cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses dynamic resizing , an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications. Hash tables in general exhibit poor locality of reference \u2014that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger microprocessor cache misses that cause long delays. Compact data structures such as arrays searched with linear search may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system. Hash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a malicious adversary with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g., a denial of service attack .[ 27] [ 28] [ 29] In critical applications, a data structure with better worst-case guarantees can be used; however, universal hashing \u2014a randomized algorithm that prevents the attacker from predicting which inputs cause worst-case behavior\u2014may be preferable.[ 30] The hash function used by the hash table in the Linux routing table cache was changed with Linux version 2.4.2 as a countermeasure against such attacks.[ 31]","title":"Drawbacks"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#uses","text":"","title":"Uses"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#associative_arrays","text":"Main article: Associative array","title":"Associative arrays"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#database_indexing","text":"Hash tables may also be used as disk -based data structures and database indices (such as in dbm ) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins.","title":"Database indexing"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#caches","text":"Main article: Cache (computing) Hash tables can be used to implement caches , auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries\u2014usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.","title":"Caches"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#sets","text":"Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not. Those structures can therefore be used to implement a set data structure , which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.","title":"Sets"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#object_representation","text":"Several dynamic languages, such as Perl , Python , JavaScript , Lua , and Ruby , use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.","title":"Object representation"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#unique_data_representation","text":"Main article: String interning Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single string pool implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing , and can be used with many other kinds of data ( expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.).","title":"Unique data representation"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#transposition_table","text":"Main article: Transposition table","title":"Transposition table"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#implemtentation","text":"","title":"Implemtentation"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#an_analysis_of_hash_map_implementations_in_popular_languages","text":"","title":"An Analysis of Hash Map Implementations in Popular Languages"},{"location":"Hash/Hash-based-data-structures/Hash-table/wikipedia-Hash-table/#cpython_dict","text":"https://github.com/zpoint/CPython-Internals/blob/master/BasicObject/dict/dict.md https://github.com/python/cpython/blob/master/Objects/dictnotes.txt","title":"CPython dict"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/","text":"Authenticated Data Structures, Generically # \u8bba\u6587\u5176\u4ed6\u5730\u5740\uff1a Authenticated data structures, generically authenticated-data-structures # An implementation of generic authenticated data structures in OCaml Authenticated Data Structures, as a Library, for Free! #","title":"[Authenticated Data Structures, Generically](http://soc1024.ece.illinois.edu/gpads/)"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/#authenticated_data_structures_generically","text":"\u8bba\u6587\u5176\u4ed6\u5730\u5740\uff1a Authenticated data structures, generically","title":"Authenticated Data Structures, Generically"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/#authenticated-data-structures","text":"An implementation of generic authenticated data structures in OCaml","title":"authenticated-data-structures"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/#authenticated_data_structures_as_a_library_for_free","text":"","title":"Authenticated Data Structures, as a Library, for Free!"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/","text":"\u524d\u8a00 # \u662f\u5728\u5b66\u4e60 git \u7684\u65f6\u5019\uff0c\u53d1\u73b0 Merkle tree \u7684\u3002\u5728\u5b66\u4e60 Merkle tree \u4e4b\u524d\uff0c\u5148\u5b66\u4e60 Hash list \u548c Hash chain \uff0c\u4e24\u8005\u662f\u6df1\u523b\u638c\u63e1 Merkle tree \u7684\u57fa\u7840\u3002 Hash list # In computer science , a hash list is typically a list of hashes of the data blocks in a file or set of files. NOTE: hash list\u7684\u542b\u4e49\u5176\u5b9e\u975e\u5e38\u7b80\u5355\uff0c\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bf4\u7684list of hash\uff0c\u4ec5\u6b64\u800c\u5df2\u3002\u5f53\u7136\u53ef\u4ee5\u5728\u5b83\u4e0a\u9762\u518d\u8fdb\u884c\u6269\u5c55\uff0c\u6bd4\u5982\u4e0b\u9762\u5c06\u8981\u4ecb\u7ecd\u7684Root hash\u3002 Root hash # Often, an additional hash of the hash list itself (a top hash , also called root hash or master hash ) is used. A hash list with a top hash \u4e0a\u8ff0\u5c31\u662f\u4e00\u4e2a\u5e26\u6709root hash\u7684hash list\u3002 NOTE: \u601d\u8003\uff1aroot hash\u7684\u4ef7\u503c\u4f55\u5728\u5462\uff1f\u5982\u679c\u5c06hash list\u4e2d\u7684\u6bcf\u4e2ahash\u90fd\u5bf9\u5e94\u7684\u662f\u4e00\u4e2adata block\uff0c\u800croot hash\u5219\u5bf9\u5e94\u7684\u662f\u6240\u6709\u7684data block\uff0c\u4e5f\u5c31\u662f\u8bf4\u5b83\u5176\u5b9e\u6240\u5bf9\u5e94\u7684\u662f\u6574\u4f53data\uff0c\u800c\u975e\u5c40\u90e8\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u5c31\u63cf\u8ff0\u7684\u662f\u8fd9\u79cd\u542b\u4e49\u3002 Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash list can be received from any non-trusted source, like any peer in the p2p network. Then the received hash list is checked against the trusted top hash, and if the hash list is damaged or fake, another hash list from another source will be tried until the program finds one that matches the top hash. Applications # Hash lists can be used to protect any kind of data stored, handled and transferred in and between computers. An important use of hash lists is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and to check that the other peers do not \"lie\" and send fake blocks. Usually a cryptographic hash function such as SHA-256 is used for the hashing. If the hash list only needs to protect against unintentional damage unsecured checksums such as CRCs can be used. NOTE: \u4e0a\u8ff0\u5176\u5b9e\u662fhash\u7684\u5e38\u89c1\u7528\u6cd5\uff0c\u5e76\u65e0\u4ec0\u4e48\u7279\u6b8a\u4e4b\u5904\u3002\u4e0b\u9762\u5219\u5f3a\u8c03hash list\u7684\u7279\u6b8a\u4e4b\u5904\u3002 Hash lists are better than a simple hash of the entire file since, in the case of a data block being damaged, this is noticed, and only the damaged block needs to be redownloaded. With only a hash of the file, many undamaged blocks would have to be redownloaded, and the file reconstructed and tested until the correct hash of the entire file is obtained. Hash lists also protect against nodes that try to sabotage by sending fake blocks, since in such a case the damaged block can be acquired from some other source. Hash chain # A hash chain is the successive application of a cryptographic hash function to a piece of data. Definition # A hash chain is a successive application of a cryptographic hash function $ h $ to a string $ x $. For example, $ h(h(h(h(x)))) $ gives a hash chain of length 4, often denoted $ h^{4}(x) $ NOTE: \u663e\u7136\uff0chash chain\u7684\u5b9a\u4e49\u6240\u5f3a\u8c03\u7684\u662f\u91cd\u590d\u8fed\u4ee3\u5730\u4f7f\u7528hash function\u3002\u663e\u7136\u5b83\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u4e00\u4e2a\u6570\u5b66\u8fc7\u7a0b\u3002 Features # \u672c\u6587\u7ed9\u51fa\u7684hash chain\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c \u8fd0\u7528hash chain\u8fd8\u80fd\u591f\u5e26\u6765\u5176\u4ed6\u7684\u975e\u5e38\u4f18\u826f\u7684\u7279\u6027\uff0c\u8fd9\u5c31\u4f7f\u5f97hash chain\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4e0b\u9762\u5c31\u6765\u63a2\u8ba8\u8fd0\u7528hash chain\u53ef\u4ee5\u5e26\u6765\u7684\u4f18\u826f\u7279\u6027\uff1a Pre-image resistance / one-way # \u7531\u4e8ehash chain\u6240\u4f7f\u7528\u7684\u662f cryptographic hash function \uff0c cryptographic hash function \u662f one-way function ,\u6240\u6709hash chain\u5c31\u5177\u5907\u4e86 cryptographic hash function \u7684 Pre-image resistance \u7279\u6027\uff0c\u5373\u6839\u636e\u51fd\u6570\u503c\u65e0\u6cd5\u9006\u5411\u8ba1\u7b97\u51fa\u5b83\u7684\u81ea\u53d8\u91cf\u7684\u503c\uff0c\u8bf4\u7684\u66f4\u52a0\u5f62\u8c61\u4e00\u4e9b\u5c31\u662fhash chain\u53ea\u80fd\u591f\u987a\u63a8\u800c\u4e0d\u80fd\u591f\u9006\u63a8\u3002 \u4e0b\u9762\u8fd9\u6bb5\u8bdd\u662f\u6458\u81ea Hash Chain \u9879\u76ee\u7684\u6587\u6863\uff0c\u76ee\u7684\u662f\u5e2e\u52a9\u7406\u89e3\uff1a The idea of a hash chain is simple: you start with a base (could be a password, or just a number, or some other data) and hash ( cryptographic hash function ) it. You then take the result and hash that too. You continue hashing the results repeatedly, till you have a series of hashes like this: Base -> Hash0 = H(Base) -> Hash1 = H(Hash0) -> ... -> HashN = H(HashN-1) The exciting property of these hash chains is that given the last hash in the chain, HashN, it is very difficult to determine any of the previous hashes, or the base. However, given the last hash, it is trivial to verify whether another hash is part of the chain. This means that a hash chain has the potential to be a limited source of authentication. You can deploy a resource in public along with the last hash of the chain. Then you can give commands to this resource, passing along each previous hash as authentication of your identity. Immutable # \u7be1\u6539 Hash chain \u4e2d\u7684\u4efb\u4f55\u4e00\u4e2anode\uff08\u8282\u70b9\uff09\u7684hash value\u90fd\u4f1a\u5bfc\u81f4\u8fd9\u4e2a\u8282\u70b9\u540e\u7684\u6240\u6709\u7684\u8282\u70b9\u7684hash value\u90fd\u5fc5\u987b\u91cd\u65b0\u8ba1\u7b97\u4e00\u904d\u624d\u80fd\u591f\u91cd\u65b0\u7ec4\u6210\u94fe\uff0c\u5426\u5219\u5c31\u65e0\u6cd5\u5f62\u6210hash chain\u4e86\u3002\u8fd9\u610f\u5473\u4e2d\u6574\u4e2a Hash chain \u662fimmutable\u7684\u3002 \u4e0b\u9762\u662f\u5bfb\u627e\u5230\u7684\u4e00\u4e9b\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\uff1a Where did the idea of blockchain come from? Git was already using it since 2005 With bitcoin's blockchain, each block's hash is computed with the previous block's hash. That makes all blocks an immutable chain . This is not the first time people used a hash-chain. As far as I know, Git was already used it since 2005 for the commit-hash computing. So, what is the earliest use of hash chains? Where did the idea of blockchain come from? \u8fd9\u79cdimmutable\u7684\u7279\u6027\u6709\u7740\u975e\u5e38\u91cd\u8981\u7684\u4ef7\u503c\uff0c\u6bd4\u5982\uff1a \u539f\u6587\u94fe\u63a5 According to Bitcoin and Cryptocurrency Technologies (BaCT) , the Princeton Bitcoin textbook, the block chain dates back to a \"paper by Haber and Stornetta in 1991. Their proposal was a method for secure timestamping of digitaldocuments, rather than a digital money scheme.\" (BaCT p.15) The chaining of Merkle trees instead of single documents was proposed in a later paper. (BaCT p.16) Applications # one-time keys # In computer security , a hash chain is a method to produce many one-time keys from a single key or password . For non-repudiation \uff08\u4e0d\u53ef\u62b5\u8d56\uff09 a hash function can be applied successively to additional pieces of data in order to record the chronology of data's existence. see also: S/KEY Binary hash chains # Main article: Merkle tree Hash chain vs. blockchain # A hash chain is similar to a blockchain , as they both utilize a cryptographic hash function for creating a link between two nodes. However, a blockchain (as used by Bitcoin and related systems) is generally intended to support distributed consensus around a public ledger (data), and incorporates a set of rules for encapsulation of data and associated data permissions. NOTE: Blockchain \u80af\u5b9a\u8fdc\u6bd4Hash chain\u590d\u6742\uff0c\u5173\u4e8e\u4e24\u8005\u7684\u5173\u8054\uff0c\u6709\u592a\u591a\u592a\u591a\u7684\u6587\u7ae0\u8fdb\u884c\u5256\u6790\u4e86\u3002 Blockchain \u8fd0\u7528\u4e86\u591a\u79cd\u6280\u672f\uff0c\u5176\u4e2d\u4e4b\u4e00\u5c31\u662fHash chain\uff0c Blockchain \u4f7f\u7528Hash chain\u6765\u5c06\u5b83\u7684block\u94fe\u63a5\u8d77\u6765\u3002\u5173\u4e8e Blockchain \u4e2dHash chain\u7684\u5e94\u7528\uff0c\u8fd8\u9700\u8981\u53c2\u89c1 Blockchain \u6587\u7ae0\u3002 \u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u5173\u4e8e\u4e24\u8005\u7684\u6587\u7ae0\uff1a https://www.researchgate.net/figure/The-Bitcoin-blockchain-is-a-hash-chain-of-blocks-Each-block-has-a-Merkle-tree-of_fig1_316789505 The Bitcoin blockchain is a hash chain of blocks. Each block has a Merkle tree of transactions. Efficient membership proofs of transactions can be constructed with respect to the Merkle root. \u8fd9\u7bc7\u6587\u7ae0\u5c06block chain\u7684\u7b80\u5355\u6a21\u578b\uff0c\u901a\u8fc7\u8fd9\u4e2a\u7b80\u5355\u6a21\u578b\u53ef\u4ee5\u770b\u51fahash chain\u5728block chain\u4e2d\u7684\u4f7f\u7528\u3002 https://crypto.stackexchange.com/questions/68290/when-was-hash-chain-first-used Hash linking is used to prove the integrity of a blockchain, or similar systems. When was that technique first used? I would guess it was early, maybe 1950s/1960s? Hash list VS Hash chain # \u5728 Hash chain \u4e2d\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a In contrast to the recursive structure of hash chains, the elements of a hash list are independent of each other. TO READ When was hash chain first used? https://www.techopedia.com/definition/32920/hash-chain https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-5906-5_780 Merkle tree # In cryptography and computer science , a hash tree or Merkle tree is a tree in which every leaf node is labelled with the cryptographic hash of a data block, and every non-leaf node is labelled with the hash of the labels of its child nodes. Hash trees are a generalization of hash lists and hash chains . NOTE: \u6709\u4e86\u524d\u9762 Hash list \u548c Hash chain \u7684\u57fa\u7840\u73b0\u5728\u6765\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u5c31\u76f8\u5bf9\u6bd4\u8f83\u5bb9\u6613\u4e86\u3002hash chain\u662f\u7ebf\u6027\u7ed3\u6784\uff0chash tree\u5219\u662fhierarchy\u7ed3\u6784\u3002 An example of a binary hash tree. Hashes 0-0 and 0-1 are the hash values of data blocks L1 and L2, respectively, and hash 0 is the hash of the concatenation of hashes 0-0 and 0-1. Uses # Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. They can help ensure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Hash trees are used in hash-based cryptography . Hash trees are also used in file systems: IPFS , Btrfs and ZFS distributed revision control systems: Git and Mercurial peer-to-peer network: Bitcoin NoSQL systems such as Apache Cassandra NOTE: \u901a\u8fc7\u4e0a\u8ff0\u4f8b\u5b50\u53ef\u4ee5\u770b\u51fa Merkle tree \u5728distributed application\u4e2d\u6709\u7740\u5e7f\u6cdb\u8fd0\u7528\u3002 Overview # A hash tree is a tree of hashes in which the leaves are hashes of data blocks in, for instance, a file or set of files. Nodes further up in the tree are the hashes of their respective children. For example, in the picture hash 0 is the result of hashing the concatenation of hash 0-0 and hash 0-1 . That is, hash 0 = hash( hash(0-0) + hash(0-1) ) where + denotes concatenation . Usually, a cryptographic hash function such as SHA-2 is used for the hashing. If the hash tree only needs to protect against unintentional damage, unsecured checksums such as CRCs can be used. In the top of a hash tree there is a top hash (or root hash or master hash ). Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash tree can be received from any non-trusted source, like any peer in the p2p network. Then, the received hash tree is checked against the trusted top hash , and if the hash tree is damaged or fake, another hash tree from another source will be tried until the program finds one that matches the top hash. Merkel DAG # DAG\u7684\u542b\u4e49\u662fdirected acyclic graph\u3002Merkel DAG\u662f\u6211\u5728\u9605\u8bfb Is Git a Block Chain? \u53d1\u73b0\u5b83\u4eec\u63d0\u51fa\u7684\u4e00\u4e2a\u6982\u5ff5\u3002 TO READ # https://blockchainlabs.ai/the-merkle-tree/ Linked timestamping # https://en.wikipedia.org/wiki/Linked_timestamping","title":"\u524d\u8a00"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#_1","text":"\u662f\u5728\u5b66\u4e60 git \u7684\u65f6\u5019\uff0c\u53d1\u73b0 Merkle tree \u7684\u3002\u5728\u5b66\u4e60 Merkle tree \u4e4b\u524d\uff0c\u5148\u5b66\u4e60 Hash list \u548c Hash chain \uff0c\u4e24\u8005\u662f\u6df1\u523b\u638c\u63e1 Merkle tree \u7684\u57fa\u7840\u3002","title":"\u524d\u8a00"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash_list","text":"In computer science , a hash list is typically a list of hashes of the data blocks in a file or set of files. NOTE: hash list\u7684\u542b\u4e49\u5176\u5b9e\u975e\u5e38\u7b80\u5355\uff0c\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bf4\u7684list of hash\uff0c\u4ec5\u6b64\u800c\u5df2\u3002\u5f53\u7136\u53ef\u4ee5\u5728\u5b83\u4e0a\u9762\u518d\u8fdb\u884c\u6269\u5c55\uff0c\u6bd4\u5982\u4e0b\u9762\u5c06\u8981\u4ecb\u7ecd\u7684Root hash\u3002","title":"Hash list"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#root_hash","text":"Often, an additional hash of the hash list itself (a top hash , also called root hash or master hash ) is used. A hash list with a top hash \u4e0a\u8ff0\u5c31\u662f\u4e00\u4e2a\u5e26\u6709root hash\u7684hash list\u3002 NOTE: \u601d\u8003\uff1aroot hash\u7684\u4ef7\u503c\u4f55\u5728\u5462\uff1f\u5982\u679c\u5c06hash list\u4e2d\u7684\u6bcf\u4e2ahash\u90fd\u5bf9\u5e94\u7684\u662f\u4e00\u4e2adata block\uff0c\u800croot hash\u5219\u5bf9\u5e94\u7684\u662f\u6240\u6709\u7684data block\uff0c\u4e5f\u5c31\u662f\u8bf4\u5b83\u5176\u5b9e\u6240\u5bf9\u5e94\u7684\u662f\u6574\u4f53data\uff0c\u800c\u975e\u5c40\u90e8\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u5c31\u63cf\u8ff0\u7684\u662f\u8fd9\u79cd\u542b\u4e49\u3002 Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash list can be received from any non-trusted source, like any peer in the p2p network. Then the received hash list is checked against the trusted top hash, and if the hash list is damaged or fake, another hash list from another source will be tried until the program finds one that matches the top hash.","title":"Root hash"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#applications","text":"Hash lists can be used to protect any kind of data stored, handled and transferred in and between computers. An important use of hash lists is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and to check that the other peers do not \"lie\" and send fake blocks. Usually a cryptographic hash function such as SHA-256 is used for the hashing. If the hash list only needs to protect against unintentional damage unsecured checksums such as CRCs can be used. NOTE: \u4e0a\u8ff0\u5176\u5b9e\u662fhash\u7684\u5e38\u89c1\u7528\u6cd5\uff0c\u5e76\u65e0\u4ec0\u4e48\u7279\u6b8a\u4e4b\u5904\u3002\u4e0b\u9762\u5219\u5f3a\u8c03hash list\u7684\u7279\u6b8a\u4e4b\u5904\u3002 Hash lists are better than a simple hash of the entire file since, in the case of a data block being damaged, this is noticed, and only the damaged block needs to be redownloaded. With only a hash of the file, many undamaged blocks would have to be redownloaded, and the file reconstructed and tested until the correct hash of the entire file is obtained. Hash lists also protect against nodes that try to sabotage by sending fake blocks, since in such a case the damaged block can be acquired from some other source.","title":"Applications"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash_chain","text":"A hash chain is the successive application of a cryptographic hash function to a piece of data.","title":"Hash chain"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#definition","text":"A hash chain is a successive application of a cryptographic hash function $ h $ to a string $ x $. For example, $ h(h(h(h(x)))) $ gives a hash chain of length 4, often denoted $ h^{4}(x) $ NOTE: \u663e\u7136\uff0chash chain\u7684\u5b9a\u4e49\u6240\u5f3a\u8c03\u7684\u662f\u91cd\u590d\u8fed\u4ee3\u5730\u4f7f\u7528hash function\u3002\u663e\u7136\u5b83\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u4e00\u4e2a\u6570\u5b66\u8fc7\u7a0b\u3002","title":"Definition"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#features","text":"\u672c\u6587\u7ed9\u51fa\u7684hash chain\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c \u8fd0\u7528hash chain\u8fd8\u80fd\u591f\u5e26\u6765\u5176\u4ed6\u7684\u975e\u5e38\u4f18\u826f\u7684\u7279\u6027\uff0c\u8fd9\u5c31\u4f7f\u5f97hash chain\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4e0b\u9762\u5c31\u6765\u63a2\u8ba8\u8fd0\u7528hash chain\u53ef\u4ee5\u5e26\u6765\u7684\u4f18\u826f\u7279\u6027\uff1a","title":"Features"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#pre-image_resistance_one-way","text":"\u7531\u4e8ehash chain\u6240\u4f7f\u7528\u7684\u662f cryptographic hash function \uff0c cryptographic hash function \u662f one-way function ,\u6240\u6709hash chain\u5c31\u5177\u5907\u4e86 cryptographic hash function \u7684 Pre-image resistance \u7279\u6027\uff0c\u5373\u6839\u636e\u51fd\u6570\u503c\u65e0\u6cd5\u9006\u5411\u8ba1\u7b97\u51fa\u5b83\u7684\u81ea\u53d8\u91cf\u7684\u503c\uff0c\u8bf4\u7684\u66f4\u52a0\u5f62\u8c61\u4e00\u4e9b\u5c31\u662fhash chain\u53ea\u80fd\u591f\u987a\u63a8\u800c\u4e0d\u80fd\u591f\u9006\u63a8\u3002 \u4e0b\u9762\u8fd9\u6bb5\u8bdd\u662f\u6458\u81ea Hash Chain \u9879\u76ee\u7684\u6587\u6863\uff0c\u76ee\u7684\u662f\u5e2e\u52a9\u7406\u89e3\uff1a The idea of a hash chain is simple: you start with a base (could be a password, or just a number, or some other data) and hash ( cryptographic hash function ) it. You then take the result and hash that too. You continue hashing the results repeatedly, till you have a series of hashes like this: Base -> Hash0 = H(Base) -> Hash1 = H(Hash0) -> ... -> HashN = H(HashN-1) The exciting property of these hash chains is that given the last hash in the chain, HashN, it is very difficult to determine any of the previous hashes, or the base. However, given the last hash, it is trivial to verify whether another hash is part of the chain. This means that a hash chain has the potential to be a limited source of authentication. You can deploy a resource in public along with the last hash of the chain. Then you can give commands to this resource, passing along each previous hash as authentication of your identity.","title":"Pre-image resistance / one-way"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#immutable","text":"\u7be1\u6539 Hash chain \u4e2d\u7684\u4efb\u4f55\u4e00\u4e2anode\uff08\u8282\u70b9\uff09\u7684hash value\u90fd\u4f1a\u5bfc\u81f4\u8fd9\u4e2a\u8282\u70b9\u540e\u7684\u6240\u6709\u7684\u8282\u70b9\u7684hash value\u90fd\u5fc5\u987b\u91cd\u65b0\u8ba1\u7b97\u4e00\u904d\u624d\u80fd\u591f\u91cd\u65b0\u7ec4\u6210\u94fe\uff0c\u5426\u5219\u5c31\u65e0\u6cd5\u5f62\u6210hash chain\u4e86\u3002\u8fd9\u610f\u5473\u4e2d\u6574\u4e2a Hash chain \u662fimmutable\u7684\u3002 \u4e0b\u9762\u662f\u5bfb\u627e\u5230\u7684\u4e00\u4e9b\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\uff1a Where did the idea of blockchain come from? Git was already using it since 2005 With bitcoin's blockchain, each block's hash is computed with the previous block's hash. That makes all blocks an immutable chain . This is not the first time people used a hash-chain. As far as I know, Git was already used it since 2005 for the commit-hash computing. So, what is the earliest use of hash chains? Where did the idea of blockchain come from? \u8fd9\u79cdimmutable\u7684\u7279\u6027\u6709\u7740\u975e\u5e38\u91cd\u8981\u7684\u4ef7\u503c\uff0c\u6bd4\u5982\uff1a \u539f\u6587\u94fe\u63a5 According to Bitcoin and Cryptocurrency Technologies (BaCT) , the Princeton Bitcoin textbook, the block chain dates back to a \"paper by Haber and Stornetta in 1991. Their proposal was a method for secure timestamping of digitaldocuments, rather than a digital money scheme.\" (BaCT p.15) The chaining of Merkle trees instead of single documents was proposed in a later paper. (BaCT p.16)","title":"Immutable"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#applications_1","text":"","title":"Applications"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#one-time_keys","text":"In computer security , a hash chain is a method to produce many one-time keys from a single key or password . For non-repudiation \uff08\u4e0d\u53ef\u62b5\u8d56\uff09 a hash function can be applied successively to additional pieces of data in order to record the chronology of data's existence. see also: S/KEY","title":"one-time keys"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#binary_hash_chains","text":"Main article: Merkle tree","title":"Binary hash chains"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash_chain_vs_blockchain","text":"A hash chain is similar to a blockchain , as they both utilize a cryptographic hash function for creating a link between two nodes. However, a blockchain (as used by Bitcoin and related systems) is generally intended to support distributed consensus around a public ledger (data), and incorporates a set of rules for encapsulation of data and associated data permissions. NOTE: Blockchain \u80af\u5b9a\u8fdc\u6bd4Hash chain\u590d\u6742\uff0c\u5173\u4e8e\u4e24\u8005\u7684\u5173\u8054\uff0c\u6709\u592a\u591a\u592a\u591a\u7684\u6587\u7ae0\u8fdb\u884c\u5256\u6790\u4e86\u3002 Blockchain \u8fd0\u7528\u4e86\u591a\u79cd\u6280\u672f\uff0c\u5176\u4e2d\u4e4b\u4e00\u5c31\u662fHash chain\uff0c Blockchain \u4f7f\u7528Hash chain\u6765\u5c06\u5b83\u7684block\u94fe\u63a5\u8d77\u6765\u3002\u5173\u4e8e Blockchain \u4e2dHash chain\u7684\u5e94\u7528\uff0c\u8fd8\u9700\u8981\u53c2\u89c1 Blockchain \u6587\u7ae0\u3002 \u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u5173\u4e8e\u4e24\u8005\u7684\u6587\u7ae0\uff1a https://www.researchgate.net/figure/The-Bitcoin-blockchain-is-a-hash-chain-of-blocks-Each-block-has-a-Merkle-tree-of_fig1_316789505 The Bitcoin blockchain is a hash chain of blocks. Each block has a Merkle tree of transactions. Efficient membership proofs of transactions can be constructed with respect to the Merkle root. \u8fd9\u7bc7\u6587\u7ae0\u5c06block chain\u7684\u7b80\u5355\u6a21\u578b\uff0c\u901a\u8fc7\u8fd9\u4e2a\u7b80\u5355\u6a21\u578b\u53ef\u4ee5\u770b\u51fahash chain\u5728block chain\u4e2d\u7684\u4f7f\u7528\u3002 https://crypto.stackexchange.com/questions/68290/when-was-hash-chain-first-used Hash linking is used to prove the integrity of a blockchain, or similar systems. When was that technique first used? I would guess it was early, maybe 1950s/1960s?","title":"Hash chain vs. blockchain"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash_list_vs_hash_chain","text":"\u5728 Hash chain \u4e2d\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a In contrast to the recursive structure of hash chains, the elements of a hash list are independent of each other. TO READ When was hash chain first used? https://www.techopedia.com/definition/32920/hash-chain https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-5906-5_780","title":"Hash list VS  Hash chain"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#merkle_tree","text":"In cryptography and computer science , a hash tree or Merkle tree is a tree in which every leaf node is labelled with the cryptographic hash of a data block, and every non-leaf node is labelled with the hash of the labels of its child nodes. Hash trees are a generalization of hash lists and hash chains . NOTE: \u6709\u4e86\u524d\u9762 Hash list \u548c Hash chain \u7684\u57fa\u7840\u73b0\u5728\u6765\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u5c31\u76f8\u5bf9\u6bd4\u8f83\u5bb9\u6613\u4e86\u3002hash chain\u662f\u7ebf\u6027\u7ed3\u6784\uff0chash tree\u5219\u662fhierarchy\u7ed3\u6784\u3002 An example of a binary hash tree. Hashes 0-0 and 0-1 are the hash values of data blocks L1 and L2, respectively, and hash 0 is the hash of the concatenation of hashes 0-0 and 0-1.","title":"Merkle tree"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#uses","text":"Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. They can help ensure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Hash trees are used in hash-based cryptography . Hash trees are also used in file systems: IPFS , Btrfs and ZFS distributed revision control systems: Git and Mercurial peer-to-peer network: Bitcoin NoSQL systems such as Apache Cassandra NOTE: \u901a\u8fc7\u4e0a\u8ff0\u4f8b\u5b50\u53ef\u4ee5\u770b\u51fa Merkle tree \u5728distributed application\u4e2d\u6709\u7740\u5e7f\u6cdb\u8fd0\u7528\u3002","title":"Uses"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#overview","text":"A hash tree is a tree of hashes in which the leaves are hashes of data blocks in, for instance, a file or set of files. Nodes further up in the tree are the hashes of their respective children. For example, in the picture hash 0 is the result of hashing the concatenation of hash 0-0 and hash 0-1 . That is, hash 0 = hash( hash(0-0) + hash(0-1) ) where + denotes concatenation . Usually, a cryptographic hash function such as SHA-2 is used for the hashing. If the hash tree only needs to protect against unintentional damage, unsecured checksums such as CRCs can be used. In the top of a hash tree there is a top hash (or root hash or master hash ). Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash tree can be received from any non-trusted source, like any peer in the p2p network. Then, the received hash tree is checked against the trusted top hash , and if the hash tree is damaged or fake, another hash tree from another source will be tried until the program finds one that matches the top hash.","title":"Overview"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#merkel_dag","text":"DAG\u7684\u542b\u4e49\u662fdirected acyclic graph\u3002Merkel DAG\u662f\u6211\u5728\u9605\u8bfb Is Git a Block Chain? \u53d1\u73b0\u5b83\u4eec\u63d0\u51fa\u7684\u4e00\u4e2a\u6982\u5ff5\u3002","title":"Merkel DAG"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#to_read","text":"https://blockchainlabs.ai/the-merkle-tree/","title":"TO READ"},{"location":"Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#linked_timestamping","text":"https://en.wikipedia.org/wiki/Linked_timestamping","title":"Linked timestamping"},{"location":"Hash/Hash-function/Category-Hash-functions/","text":"Category:Hash functions # List of hash functions # Application Property of hash function length Hash table Cryptographic hash function \u56fa\u5b9a Distributed hash tables VS-check-sum-VS-cryptographic-hash # https://security.stackexchange.com/a/194602 A checksum (such as CRC32) is to prevent accidental changes. If one byte changes, the checksum changes. The checksum is not safe to protect against malicious changes: it is pretty easy to create a file with a particular checksum. A hash function maps some data to other data. It is often used to speed up comparisons or create a hash table. Not all hash functions are secure and the hash does not necessarily changes when the data changes. A cryptographic hash function (such as SHA1) is a checksum that is secure against malicious changes. It is pretty hard to create a file with a specific cryptographic hash. To make things more complicated, cryptographic hash functions are sometimes simply referred to as hash functions. VS-cryptographic-hash-function-VS-perfect hash function? # Are cryptographic hash functions perfect hash functions? TODO # https://security.stackexchange.com/questions/11839/what-is-the-difference-between-a-hash-function-and-a-cryptographic-hash-function https://crypto.stackexchange.com/questions/879/what-is-the-random-o racle-model-and-why-is-it-controversial/880#880 https://computer.howstuffworks.com/encryption7.htm https://en.wikipedia.org/wiki/Hash","title":"[Category:Hash functions](https://en.wikipedia.org/wiki/Category:Hash_functions)"},{"location":"Hash/Hash-function/Category-Hash-functions/#categoryhash_functions","text":"","title":"Category:Hash functions"},{"location":"Hash/Hash-function/Category-Hash-functions/#list_of_hash_functions","text":"Application Property of hash function length Hash table Cryptographic hash function \u56fa\u5b9a Distributed hash tables","title":"List of hash functions"},{"location":"Hash/Hash-function/Category-Hash-functions/#vs-check-sum-vs-cryptographic-hash","text":"https://security.stackexchange.com/a/194602 A checksum (such as CRC32) is to prevent accidental changes. If one byte changes, the checksum changes. The checksum is not safe to protect against malicious changes: it is pretty easy to create a file with a particular checksum. A hash function maps some data to other data. It is often used to speed up comparisons or create a hash table. Not all hash functions are secure and the hash does not necessarily changes when the data changes. A cryptographic hash function (such as SHA1) is a checksum that is secure against malicious changes. It is pretty hard to create a file with a specific cryptographic hash. To make things more complicated, cryptographic hash functions are sometimes simply referred to as hash functions.","title":"VS-check-sum-VS-cryptographic-hash"},{"location":"Hash/Hash-function/Category-Hash-functions/#vs-cryptographic-hash-function-vs-perfect_hash_function","text":"Are cryptographic hash functions perfect hash functions?","title":"VS-cryptographic-hash-function-VS-perfect hash function?"},{"location":"Hash/Hash-function/Category-Hash-functions/#todo","text":"https://security.stackexchange.com/questions/11839/what-is-the-difference-between-a-hash-function-and-a-cryptographic-hash-function https://crypto.stackexchange.com/questions/879/what-is-the-random-o racle-model-and-why-is-it-controversial/880#880 https://computer.howstuffworks.com/encryption7.htm https://en.wikipedia.org/wiki/Hash","title":"TODO"},{"location":"Hash/Hash-function/Hash-function/","text":"\u7efc\u8ff0 # \u6b63\u5982\u5728 List of hash functions \u4e2d\u6240\u7f57\u5217\u7684\uff0c\u6709\u7740\u5f62\u5f62\u8272\u8272\u7684hash function\u3002\u90a3\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u6765\u628a\u6211hash function\u5462\uff1f\u6211\u89c9\u5f97\u4ece\u6570\u5b66\u4e0a\u5bf9 function \u7684\u5b9a\u4e49\u5165\u624b\u8f83\u597d\u3002 hash function\u548c\u666e\u901a\u7684function\u4e00\u6837\uff0c\u6709 domain \u548c codomain \u3002\u5728hash function\u4e2d\uff0cdomain\u7684\u540c\u4e49\u8bcd\u6709\uff1akey space \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u54ea\u4e9b\u6570\u636e\u3002\u6bd4\u5982\uff1a \u5bf9\u4e8e\u4f7f\u7528 cryptographic hash function \u6765\u52a0\u5bc6\u5bc6\u7801\u7684function\uff0c\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u5c31\u662f\u6240\u6709\u53ef\u4ee5\u7528\u4e8e\u5bc6\u7801\u7684\u5b57\u7b26\u7684\u7ec4\u5408\u800c\u6210\u7684\u5b57\u7b26\u4e32 \u53ef\u80fdhash\u6570\u5b57 \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81\uff0c\u6bd4\u5982\uff1a identity hash function \u5c31\u662f\u4f7f\u7528\u7684 domain \u4e2d\u6570\u636e\u662fidentity\u7684\u7279\u5f81 trivial hash function \u5c31\u662f\u4f7f\u7528domain\u4e2d\u7684\u6570\u636e\u662funiformly or sufficiently uniformly distributed \u7684\u7279\u5f81 modulo division hash function \u4e00\u822c\u9009\u62e9\u4e00\u4e2abig prime\u6765\u4f5c\u4e3a modulo \uff0c\u5177\u4f53\u539f\u56e0\uff0c\u53c2\u89c1 Why is it best to use a prime number as a mod in a hashing function? \u3002 domain \u548c codomain \u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff1a Injective function \uff0c\u663e\u7136 perfect hash function \u5177\u5907\u8fd9\u79cd\u6027\u8d28\u3002 Hash function # A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values , hash codes , digests , or simply hashes . NOTE: \"digests\" means \u6458\u8981 in Chinese. Use of hash functions relies on statistical properties of key and function interaction: worst case behavior is intolerably bad with a vanishingly small probability, and average case behavior can be nearly optimal (minimal collisions). Hash functions are related to (and often confused with) checksums , check digits , fingerprints , lossy compression , randomization functions , error-correcting codes , and ciphers . Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. Overview # NOTE: Although this article is named Hash function , what this chapter talks about is hash table. A hash function takes as input a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length , like an integer, or variable length , like a name. In some cases, the key is the datum itself. The output is a hash code used to index a hash table holding the data or records, or pointers to them. A hash function may be considered to perform three functions: Convert variable length keys into fixed length (usually machine word length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR. NOTE: Folding means bitwise operation. NOTE: What is parity-preserving operator? Parity bit Scramble the bits of the key so that the resulting values are uniformly distributed over the key space . NOTE: What is the key space? ASCII code? Map the key values into ones less than or equal to the size of the table NOTE: This function will be used only when it is used as index of the table in hash table . In other applications, this function may be omitted. A good hash function satisfies two basic properties: 1) it should be very fast to compute; NOTE: This is efficiency 2) it should minimize duplication of output values (collisions). NOTE: This is uniformity Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant. High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists , or systematic probing of the table to find an empty slot. Application # Hash table \uff0c which is also used to implement associative arrays and dynamic sets . Hash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. NOTE: B-tree Hash functions are an essential ingredient of the Bloom filter , a space-efficient probabilistic data structure that is used to test whether an element is a member of a set . geometric hashing . In these applications, the set of all inputs is some sort of metric space , and the hashing function can be interpreted as a partition of that space into a grid of cells . The table is often an array with two or more indices (called a grid file , grid index , bucket grid , and similar names), and the hash function returns an index tuple . This principle is widely used in computer graphics , computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space , such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database , and so on. Properties # Note: Properties determine their usage, it is necessary to know the properties a hash function possess. Uniformity # Note: \u5747\u5300\u6027 A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability . The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of collisions \u2014pairs of inputs that are mapped to the same hash value \u2014increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries. Note that this criterion only requires the value to be uniformly distributed , not random in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function , but the converse need not be true. NOTE: hash function VS randomizing function Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries. NOTE: Typical subset VS global set In special cases when the keys are known in advance and the key set is static, a hash function can be found that achieves absolute (or collisionless) uniformity. Such a hash function is said to be perfect . There is no algorithmic way of constructing such a function - searching for one is a factorial function of the number of keys to be mapped versus the number of table slots they're mapped into. Finding a perfect hash function over more than a very small set of keys is usually computationally infeasible; the resulting function is likely to be more computationally complex than a standard hash function, and provides only a marginal advantage over a function with good statistical properties that yields a minimum number of collisions. See universal hash function . Efficiency # In most applications, it is highly desirable that the hash function be computable with minimum latency and secondarily in a minimum number of instructions. Computational complexity varies with the number of instructions required and latency of individual instructions, with the simplest being the bitwise methods (folding), followed by the multiplicative methods, and the most complex (slowest) are the division-based methods. Because collisions should be infrequent, and cause a marginal delay but are otherwise harmless, it's usually preferable to choose a faster hash function over one that needs more computation but saves a few collisions. Division-based implementations can be of particular concern, because division is microprogrammed on nearly all chip architectures. Divide (modulo) by a constant can be inverted to become a multiply by the word-size multiplicative-inverse of the constant. This can be done by the programmer, or by the compiler. Divide can also be reduced directly into a series of shift-subtracts and shift-adds, though minimizing the number of such operations required is a daunting problem; the number of assembly instructions resulting may be more than a dozen, and swamp the pipeline. If the architecture has a hardware multiply functional unit, the multiply-by-inverse is likely a better approach. Note: There is a special introduction of computer-arithmetic in repository computer-arithmetic. We can allow the table size n to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let n be significantly less than $2^b$. Consider a pseudorandom number generator function P (key) that is uniform on the interval $[0, 2^b \u2212 1]$. A hash function uniform on the interval [0, n-1] is $n * P(key)/2^b$. We can replace the division by a (possibly faster) right bit shift : nP (key) >> b . Note: I do not understand the last paragraph. If keys are being hashed repeatedly, and the hash function is costly, computing time can be saved by precomputing the hash codes and storing them with the keys. Matching hash codes almost certainly mean the keys are identical. This technique is used for the transposition table in game-playing programs, which stores a 64-bit hashed representation of the board position. Universality # Universal hashing Applicability # A hash function should be applicable to all situations in which a hash function might be used. A hash function that allows only certain table sizes, strings only up to a certain length, or can't accept a seed (i.e. allow double hashing) isn't as useful as one that does. NOTE: The seed in the paragraph is salt (cryptography) . Reference the Deterministic to see the value of the seed. Deterministic # A hash procedure must be deterministic \u2014meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection ), although sometimes rehashing of the item is possible. The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ. Note: Reference python doc: object.__hash__ ( self ) \u00b6 Defined range # Fixed-length hash # It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value. Variable range # In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters\u2014the input data z , and the number n of allowed hash values. NOTE: n \u662f codomain \u7684 cardinality A common solution is to compute a fixed hash function with a very large range (say, 0 to $2^{32} \u2212 1$), divide the result by n , and use the division's remainder . If n is itself a power of 2, this can be done by bit masking and bit shifting . When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and n \u2212 1, for any value of n that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of n , e.g. odd or prime numbers . Variable range with minimal movement (dynamic hash function) # When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table . A hash function that will relocate the minimum number of records when the table is resized is desirable. What is needed is a hash function H ( z , n ) \u2013 where z is the key being hashed and n is the number of allowed hash values \u2013 such that H ( z , n + 1) = H ( z , n ) with probability close to n /( n + 1). Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property. Extendible hashing uses a dynamic hash function that requires space proportional to n to compute the hash function, and it becomes a function of the previous keys that have been inserted. Several algorithms that preserve the uniformity property but require time proportional to n to compute the value of H ( z , n ) have been invented. A hash function with minimal movement is especially useful in distributed hash tables . Hashing integer data types # Identity hash function # identity function Trivial hash function # Folding # NOTE: \u5230\u5e95\u5728\u54ea\u4e2a\u7ea7\u522b\u8fdb\u884cfold\uff1f\u662f\u5728bit\u7ea7\u522b\u8fd8\u662fdigit\u7ea7\u522b\uff1f\u5728 What is Folding technique in hashing and how to implement it? \u4e2d\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728digit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002\u4f46\u662fWikipedia\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002 \u8fd9\u5176\u5b9e\u662f\u7531hash function\u7684domain\u51b3\u5b9a\u7684\uff0c\u5982\u679c\u8981hash\u7684\u662f\u6570\u5b57\uff0c\u5219\u5c31\u5728digit\u7ea7\u522b\u8fdb\u884cfold\uff1b\u5982\u679c\u8981hash\u7684\u662f\u5b57\u7b26\u4e32\uff0c\u5219\u5c31\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u3002 Hashing variable-length dat # Perfect hash function # Implementation of hash function # \u975e\u5e38\u591a\u7684programming language\u4e2d\u90fd\u652f\u6301Customizing hash\uff0c\u6bd4\u5982 python object.__hash__ ( self ) \u00b6 java hashCode c# Object.GetHashCode Method \u6240\u4ee5\u4e86\u89e3\u4e00\u4e0b\u5b9e\u73b0hash function\u7684\u4e00\u4e9b\u7ec6\u8282\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 Why is it best to use a prime number as a mod in a hashing function? # A Consider the set of keys K={0,1,...,100} and a hash table where the number of buckets is m=12 . Since 3 is a factor of 12, the keys that are multiples of 3 will be hashed to buckets that are multiples of 3: Keys {0,12,24,36,...} will be hashed to bucket 0. Keys {3,15,27,39,...} will be hashed to bucket 3. Keys {6,18,30,42,...} will be hashed to bucket 6. Keys {9,21,33,45,...} will be hashed to bucket 9. If K is uniformly distributed (i.e., every key in K is equally likely to occur), then the choice of m is not so critical. But, what happens if K is not uniformly distributed? Imagine that the keys that are most likely to occur are the multiples of 3. In this case, all of the buckets that are not multiples of 3 will be empty with high probability (which is really bad in terms of hash table performance). This situation is more common that it may seem. Imagine, for instance, that you are keeping track of objects based on where they are stored in memory. If your computer's word size is four bytes, then you will be hashing keys that are multiples of 4. Needless to say that choosing m to be a multiple of 4 would be a terrible choice: you would have 3m/4 buckets completely empty, and all of your keys colliding in the remaining m/4 buckets. NOTE: key\uff1a 0=4*0 \u3001 4=4*1 \u3001 8=4*2 \u3001 12=4*3 \u3001 16=4*4 \u3001 20=4*5 \u3001 24=4*6 \u3001 28=4*7 bracket\uff1a 12=4*3 key bracket 0 0 4 1 8 2 12 0 16 1 20 2 24 3 \u4e5f\u5c31\u662f\u8bf4\u63d0\u4f9b\u4e8612\u4e2abracket\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u53ea\u4f1a\u6709 12/4=3 \u4e2abracket\u4f1a\u88ab\u4f7f\u7528\uff0c\u5269\u4f59\u7684 12-3=(12*4)/4 - 12/4= 12 * 3 /4 \u4e2abracket\u4e0d\u4f1a\u88ab\u4f7f\u7528\uff0c\u8fd9\u5c31\u662f\u4e0a\u8ff0\u7684 3m/4 buckets completely empty\u3002 In general: Every key in K that shares a common factor with the number of buckets m will be hashed to a bucket that is a multiple of this factor. Therefore, to minimize collisions, it is important to reduce the number of common factors between m and the elements of K. How can this be achieved? By choosing m to be a number that has very few factors: a prime number . SUMMARY : \u4e0a\u8ff0\u9a8c\u8bc1\u4e86\u5728 \u7efc\u8ff0 \u4e2d\u63d0\u51fa\u7684\uff1a \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81 Why is XOR the default way to combine hashes? # A Assuming uniformly random (1-bit) inputs, the AND function output probability distribution is 75% 0 and 25% 1 . Conversely, OR is 25% 0 and 75% 1 . The XOR function is 50% 0 and 50% 1 , therefore it is good for combining uniform probability distributions. This can be seen by writing out truth tables: a | b | a AND b ---+---+-------- 0 | 0 | 0 0 | 1 | 0 1 | 0 | 0 1 | 1 | 1 a | b | a OR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 1 a | b | a XOR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 0 Exercise: How many logical functions of two 1-bit inputs a and b have this uniform output distribution? Why is XOR the most suitable for the purpose stated in your question? What is the best algorithm for overriding GetHashCode? # Writing a hash function in Java: a practical guide to implementing hashCode() # CPython hashlib # redis hash # https://github.com/antirez/redis/blob/unstable/src/sha1.h https://github.com/antirez/redis/blob/unstable/src/sha256.h https://github.com/antirez/redis/blob/unstable/src/siphash.c","title":"Hash-function"},{"location":"Hash/Hash-function/Hash-function/#_1","text":"\u6b63\u5982\u5728 List of hash functions \u4e2d\u6240\u7f57\u5217\u7684\uff0c\u6709\u7740\u5f62\u5f62\u8272\u8272\u7684hash function\u3002\u90a3\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u6765\u628a\u6211hash function\u5462\uff1f\u6211\u89c9\u5f97\u4ece\u6570\u5b66\u4e0a\u5bf9 function \u7684\u5b9a\u4e49\u5165\u624b\u8f83\u597d\u3002 hash function\u548c\u666e\u901a\u7684function\u4e00\u6837\uff0c\u6709 domain \u548c codomain \u3002\u5728hash function\u4e2d\uff0cdomain\u7684\u540c\u4e49\u8bcd\u6709\uff1akey space \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u54ea\u4e9b\u6570\u636e\u3002\u6bd4\u5982\uff1a \u5bf9\u4e8e\u4f7f\u7528 cryptographic hash function \u6765\u52a0\u5bc6\u5bc6\u7801\u7684function\uff0c\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u5c31\u662f\u6240\u6709\u53ef\u4ee5\u7528\u4e8e\u5bc6\u7801\u7684\u5b57\u7b26\u7684\u7ec4\u5408\u800c\u6210\u7684\u5b57\u7b26\u4e32 \u53ef\u80fdhash\u6570\u5b57 \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81\uff0c\u6bd4\u5982\uff1a identity hash function \u5c31\u662f\u4f7f\u7528\u7684 domain \u4e2d\u6570\u636e\u662fidentity\u7684\u7279\u5f81 trivial hash function \u5c31\u662f\u4f7f\u7528domain\u4e2d\u7684\u6570\u636e\u662funiformly or sufficiently uniformly distributed \u7684\u7279\u5f81 modulo division hash function \u4e00\u822c\u9009\u62e9\u4e00\u4e2abig prime\u6765\u4f5c\u4e3a modulo \uff0c\u5177\u4f53\u539f\u56e0\uff0c\u53c2\u89c1 Why is it best to use a prime number as a mod in a hashing function? \u3002 domain \u548c codomain \u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff1a Injective function \uff0c\u663e\u7136 perfect hash function \u5177\u5907\u8fd9\u79cd\u6027\u8d28\u3002","title":"\u7efc\u8ff0"},{"location":"Hash/Hash-function/Hash-function/#hash_function","text":"A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values , hash codes , digests , or simply hashes . NOTE: \"digests\" means \u6458\u8981 in Chinese. Use of hash functions relies on statistical properties of key and function interaction: worst case behavior is intolerably bad with a vanishingly small probability, and average case behavior can be nearly optimal (minimal collisions). Hash functions are related to (and often confused with) checksums , check digits , fingerprints , lossy compression , randomization functions , error-correcting codes , and ciphers . Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently.","title":"Hash function"},{"location":"Hash/Hash-function/Hash-function/#overview","text":"NOTE: Although this article is named Hash function , what this chapter talks about is hash table. A hash function takes as input a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length , like an integer, or variable length , like a name. In some cases, the key is the datum itself. The output is a hash code used to index a hash table holding the data or records, or pointers to them. A hash function may be considered to perform three functions: Convert variable length keys into fixed length (usually machine word length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR. NOTE: Folding means bitwise operation. NOTE: What is parity-preserving operator? Parity bit Scramble the bits of the key so that the resulting values are uniformly distributed over the key space . NOTE: What is the key space? ASCII code? Map the key values into ones less than or equal to the size of the table NOTE: This function will be used only when it is used as index of the table in hash table . In other applications, this function may be omitted. A good hash function satisfies two basic properties: 1) it should be very fast to compute; NOTE: This is efficiency 2) it should minimize duplication of output values (collisions). NOTE: This is uniformity Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant. High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists , or systematic probing of the table to find an empty slot.","title":"Overview"},{"location":"Hash/Hash-function/Hash-function/#application","text":"Hash table \uff0c which is also used to implement associative arrays and dynamic sets . Hash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. NOTE: B-tree Hash functions are an essential ingredient of the Bloom filter , a space-efficient probabilistic data structure that is used to test whether an element is a member of a set . geometric hashing . In these applications, the set of all inputs is some sort of metric space , and the hashing function can be interpreted as a partition of that space into a grid of cells . The table is often an array with two or more indices (called a grid file , grid index , bucket grid , and similar names), and the hash function returns an index tuple . This principle is widely used in computer graphics , computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space , such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database , and so on.","title":"Application"},{"location":"Hash/Hash-function/Hash-function/#properties","text":"Note: Properties determine their usage, it is necessary to know the properties a hash function possess.","title":"Properties"},{"location":"Hash/Hash-function/Hash-function/#uniformity","text":"Note: \u5747\u5300\u6027 A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability . The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of collisions \u2014pairs of inputs that are mapped to the same hash value \u2014increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries. Note that this criterion only requires the value to be uniformly distributed , not random in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function , but the converse need not be true. NOTE: hash function VS randomizing function Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries. NOTE: Typical subset VS global set In special cases when the keys are known in advance and the key set is static, a hash function can be found that achieves absolute (or collisionless) uniformity. Such a hash function is said to be perfect . There is no algorithmic way of constructing such a function - searching for one is a factorial function of the number of keys to be mapped versus the number of table slots they're mapped into. Finding a perfect hash function over more than a very small set of keys is usually computationally infeasible; the resulting function is likely to be more computationally complex than a standard hash function, and provides only a marginal advantage over a function with good statistical properties that yields a minimum number of collisions. See universal hash function .","title":"Uniformity"},{"location":"Hash/Hash-function/Hash-function/#efficiency","text":"In most applications, it is highly desirable that the hash function be computable with minimum latency and secondarily in a minimum number of instructions. Computational complexity varies with the number of instructions required and latency of individual instructions, with the simplest being the bitwise methods (folding), followed by the multiplicative methods, and the most complex (slowest) are the division-based methods. Because collisions should be infrequent, and cause a marginal delay but are otherwise harmless, it's usually preferable to choose a faster hash function over one that needs more computation but saves a few collisions. Division-based implementations can be of particular concern, because division is microprogrammed on nearly all chip architectures. Divide (modulo) by a constant can be inverted to become a multiply by the word-size multiplicative-inverse of the constant. This can be done by the programmer, or by the compiler. Divide can also be reduced directly into a series of shift-subtracts and shift-adds, though minimizing the number of such operations required is a daunting problem; the number of assembly instructions resulting may be more than a dozen, and swamp the pipeline. If the architecture has a hardware multiply functional unit, the multiply-by-inverse is likely a better approach. Note: There is a special introduction of computer-arithmetic in repository computer-arithmetic. We can allow the table size n to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let n be significantly less than $2^b$. Consider a pseudorandom number generator function P (key) that is uniform on the interval $[0, 2^b \u2212 1]$. A hash function uniform on the interval [0, n-1] is $n * P(key)/2^b$. We can replace the division by a (possibly faster) right bit shift : nP (key) >> b . Note: I do not understand the last paragraph. If keys are being hashed repeatedly, and the hash function is costly, computing time can be saved by precomputing the hash codes and storing them with the keys. Matching hash codes almost certainly mean the keys are identical. This technique is used for the transposition table in game-playing programs, which stores a 64-bit hashed representation of the board position.","title":"Efficiency"},{"location":"Hash/Hash-function/Hash-function/#universality","text":"Universal hashing","title":"Universality"},{"location":"Hash/Hash-function/Hash-function/#applicability","text":"A hash function should be applicable to all situations in which a hash function might be used. A hash function that allows only certain table sizes, strings only up to a certain length, or can't accept a seed (i.e. allow double hashing) isn't as useful as one that does. NOTE: The seed in the paragraph is salt (cryptography) . Reference the Deterministic to see the value of the seed.","title":"Applicability"},{"location":"Hash/Hash-function/Hash-function/#deterministic","text":"A hash procedure must be deterministic \u2014meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection ), although sometimes rehashing of the item is possible. The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ. Note: Reference python doc: object.__hash__ ( self ) \u00b6","title":"Deterministic"},{"location":"Hash/Hash-function/Hash-function/#defined_range","text":"","title":"Defined range"},{"location":"Hash/Hash-function/Hash-function/#fixed-length_hash","text":"It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value.","title":"Fixed-length hash"},{"location":"Hash/Hash-function/Hash-function/#variable_range","text":"In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters\u2014the input data z , and the number n of allowed hash values. NOTE: n \u662f codomain \u7684 cardinality A common solution is to compute a fixed hash function with a very large range (say, 0 to $2^{32} \u2212 1$), divide the result by n , and use the division's remainder . If n is itself a power of 2, this can be done by bit masking and bit shifting . When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and n \u2212 1, for any value of n that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of n , e.g. odd or prime numbers .","title":"Variable range"},{"location":"Hash/Hash-function/Hash-function/#variable_range_with_minimal_movement_dynamic_hash_function","text":"When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table . A hash function that will relocate the minimum number of records when the table is resized is desirable. What is needed is a hash function H ( z , n ) \u2013 where z is the key being hashed and n is the number of allowed hash values \u2013 such that H ( z , n + 1) = H ( z , n ) with probability close to n /( n + 1). Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property. Extendible hashing uses a dynamic hash function that requires space proportional to n to compute the hash function, and it becomes a function of the previous keys that have been inserted. Several algorithms that preserve the uniformity property but require time proportional to n to compute the value of H ( z , n ) have been invented. A hash function with minimal movement is especially useful in distributed hash tables .","title":"Variable range with minimal movement (dynamic hash function)"},{"location":"Hash/Hash-function/Hash-function/#hashing_integer_data_types","text":"","title":"Hashing integer data types"},{"location":"Hash/Hash-function/Hash-function/#identity_hash_function","text":"identity function","title":"Identity hash function"},{"location":"Hash/Hash-function/Hash-function/#trivial_hash_function","text":"","title":"Trivial hash function"},{"location":"Hash/Hash-function/Hash-function/#folding","text":"NOTE: \u5230\u5e95\u5728\u54ea\u4e2a\u7ea7\u522b\u8fdb\u884cfold\uff1f\u662f\u5728bit\u7ea7\u522b\u8fd8\u662fdigit\u7ea7\u522b\uff1f\u5728 What is Folding technique in hashing and how to implement it? \u4e2d\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728digit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002\u4f46\u662fWikipedia\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002 \u8fd9\u5176\u5b9e\u662f\u7531hash function\u7684domain\u51b3\u5b9a\u7684\uff0c\u5982\u679c\u8981hash\u7684\u662f\u6570\u5b57\uff0c\u5219\u5c31\u5728digit\u7ea7\u522b\u8fdb\u884cfold\uff1b\u5982\u679c\u8981hash\u7684\u662f\u5b57\u7b26\u4e32\uff0c\u5219\u5c31\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u3002","title":"Folding"},{"location":"Hash/Hash-function/Hash-function/#hashing_variable-length_dat","text":"","title":"Hashing variable-length dat"},{"location":"Hash/Hash-function/Hash-function/#perfect_hash_function","text":"","title":"Perfect hash function"},{"location":"Hash/Hash-function/Hash-function/#implementation_of_hash_function","text":"\u975e\u5e38\u591a\u7684programming language\u4e2d\u90fd\u652f\u6301Customizing hash\uff0c\u6bd4\u5982 python object.__hash__ ( self ) \u00b6 java hashCode c# Object.GetHashCode Method \u6240\u4ee5\u4e86\u89e3\u4e00\u4e0b\u5b9e\u73b0hash function\u7684\u4e00\u4e9b\u7ec6\u8282\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002","title":"Implementation of hash function"},{"location":"Hash/Hash-function/Hash-function/#why_is_it_best_to_use_a_prime_number_as_a_mod_in_a_hashing_function","text":"A Consider the set of keys K={0,1,...,100} and a hash table where the number of buckets is m=12 . Since 3 is a factor of 12, the keys that are multiples of 3 will be hashed to buckets that are multiples of 3: Keys {0,12,24,36,...} will be hashed to bucket 0. Keys {3,15,27,39,...} will be hashed to bucket 3. Keys {6,18,30,42,...} will be hashed to bucket 6. Keys {9,21,33,45,...} will be hashed to bucket 9. If K is uniformly distributed (i.e., every key in K is equally likely to occur), then the choice of m is not so critical. But, what happens if K is not uniformly distributed? Imagine that the keys that are most likely to occur are the multiples of 3. In this case, all of the buckets that are not multiples of 3 will be empty with high probability (which is really bad in terms of hash table performance). This situation is more common that it may seem. Imagine, for instance, that you are keeping track of objects based on where they are stored in memory. If your computer's word size is four bytes, then you will be hashing keys that are multiples of 4. Needless to say that choosing m to be a multiple of 4 would be a terrible choice: you would have 3m/4 buckets completely empty, and all of your keys colliding in the remaining m/4 buckets. NOTE: key\uff1a 0=4*0 \u3001 4=4*1 \u3001 8=4*2 \u3001 12=4*3 \u3001 16=4*4 \u3001 20=4*5 \u3001 24=4*6 \u3001 28=4*7 bracket\uff1a 12=4*3 key bracket 0 0 4 1 8 2 12 0 16 1 20 2 24 3 \u4e5f\u5c31\u662f\u8bf4\u63d0\u4f9b\u4e8612\u4e2abracket\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u53ea\u4f1a\u6709 12/4=3 \u4e2abracket\u4f1a\u88ab\u4f7f\u7528\uff0c\u5269\u4f59\u7684 12-3=(12*4)/4 - 12/4= 12 * 3 /4 \u4e2abracket\u4e0d\u4f1a\u88ab\u4f7f\u7528\uff0c\u8fd9\u5c31\u662f\u4e0a\u8ff0\u7684 3m/4 buckets completely empty\u3002 In general: Every key in K that shares a common factor with the number of buckets m will be hashed to a bucket that is a multiple of this factor. Therefore, to minimize collisions, it is important to reduce the number of common factors between m and the elements of K. How can this be achieved? By choosing m to be a number that has very few factors: a prime number . SUMMARY : \u4e0a\u8ff0\u9a8c\u8bc1\u4e86\u5728 \u7efc\u8ff0 \u4e2d\u63d0\u51fa\u7684\uff1a \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81","title":"Why is it best to use a prime number as a mod in a hashing function?"},{"location":"Hash/Hash-function/Hash-function/#why_is_xor_the_default_way_to_combine_hashes","text":"A Assuming uniformly random (1-bit) inputs, the AND function output probability distribution is 75% 0 and 25% 1 . Conversely, OR is 25% 0 and 75% 1 . The XOR function is 50% 0 and 50% 1 , therefore it is good for combining uniform probability distributions. This can be seen by writing out truth tables: a | b | a AND b ---+---+-------- 0 | 0 | 0 0 | 1 | 0 1 | 0 | 0 1 | 1 | 1 a | b | a OR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 1 a | b | a XOR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 0 Exercise: How many logical functions of two 1-bit inputs a and b have this uniform output distribution? Why is XOR the most suitable for the purpose stated in your question?","title":"Why is XOR the default way to combine hashes?"},{"location":"Hash/Hash-function/Hash-function/#what_is_the_best_algorithm_for_overriding_gethashcode","text":"","title":"What is the best algorithm for overriding GetHashCode?"},{"location":"Hash/Hash-function/Hash-function/#writing_a_hash_function_in_java_a_practical_guide_to_implementing_hashcode","text":"","title":"Writing a hash function in Java: a practical guide to implementing hashCode()"},{"location":"Hash/Hash-function/Hash-function/#cpython_hashlib","text":"","title":"CPython hashlib"},{"location":"Hash/Hash-function/Hash-function/#redis_hash","text":"https://github.com/antirez/redis/blob/unstable/src/sha1.h https://github.com/antirez/redis/blob/unstable/src/sha256.h https://github.com/antirez/redis/blob/unstable/src/siphash.c","title":"redis hash"},{"location":"Hash/Hash-function/Rolling-hash/","text":"Rolling hash # A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input. A few hash functions allow a rolling hash to be computed very quickly\u2014the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window\u2014similar to the way a moving average function can be computed much more quickly than other low-pass filters. One of the main applications is the Rabin\u2013Karp string search algorithm , which uses the rolling hash described below. Another popular application is the rsync program, which uses a checksum based on Mark Adler's adler-32 as its rolling hash. Low Bandwidth Network Filesystem (LBFS) uses a Rabin fingerprint as its rolling hash. At best, rolling hash values are pairwise independent [ 1] or strongly universal . They cannot be 3-wise independent , for example.","title":"Rolling-hash"},{"location":"Hash/Hash-function/Rolling-hash/#rolling_hash","text":"A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input. A few hash functions allow a rolling hash to be computed very quickly\u2014the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window\u2014similar to the way a moving average function can be computed much more quickly than other low-pass filters. One of the main applications is the Rabin\u2013Karp string search algorithm , which uses the rolling hash described below. Another popular application is the rsync program, which uses a checksum based on Mark Adler's adler-32 as its rolling hash. Low Bandwidth Network Filesystem (LBFS) uses a Rabin fingerprint as its rolling hash. At best, rolling hash values are pairwise independent [ 1] or strongly universal . They cannot be 3-wise independent , for example.","title":"Rolling hash"},{"location":"Hash/Hash-function/SipHash/","text":"SipHash SipHash #","title":"SipHash"},{"location":"Hash/Hash-function/SipHash/#siphash","text":"","title":"SipHash"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Category-Hashing/","text":"","title":"Category Hashing"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cipher/","text":"Cipher #","title":"Cipher"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cipher/#cipher","text":"","title":"Cipher"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/","text":"Cryptographic hash function Properties Applications Cryptographic hash algorithms Cryptographic hash function # A cryptographic hash function ( CHF ) is a hash function that is suitable for use in cryptography . It is a mathematical algorithm that maps data of arbitrary size (often called the \"message\") to a bit string of a fixed size (the \"hash value\", \"hash\", or \"message digest\") and is a one-way function , that is, a function which is practically infeasible to invert. Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Cryptographic hash functions are a basic tool of modern cryptography. Properties # The ideal cryptographic hash function has the following main properties: it is deterministic , meaning that the same message always results in the same hash it is quick to compute the hash value for any given message Note: The above two properties are the basic properties a good hash function should satisfy, which is introduced in Hash function . The below three properties are necessary for a ideal cryptographic hash function: it is infeasible to generate a message that yields a given hash value it is infeasible to find two different messages with the same hash value a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value ( avalanche effect ) A cryptographic hash function must be able to withstand all known types of cryptanalytic attack . In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance Given a hash value h it should be difficult to find any message m such that h = hash( m ). This concept is related to that of a one-way function . Functions that lack this property are vulnerable to preimage attacks . Second pre-image resistance Given an input m 1, it should be difficult to find a different input m 2 such that hash( m 1) = hash( m 2). This property is sometimes referred to as weak collision resistance . Functions that lack this property are vulnerable to second-preimage attacks . Collision resistance It should be difficult to find two different messages m 1 and m 2 such that hash( m 1) = hash( m 2). Such a pair is called a cryptographic hash collision . This property is sometimes referred to as strong collision resistance . It requires a hash value at least twice as long as that required for pre-image resistance; otherwise collisions may be found by a birthday attack . Collision resistance implies second pre-image resistance , but does not imply pre-image resistance . Informally, these properties mean that a malicious adversary cannot replace or modify the input data without changing its digest . Thus, if two strings have the same digest, one can be very confident that they are identical. Second pre-image resistance prevents an attacker from crafting a document with the same hash as a document the attacker cannot control. Collision resistance prevents an attacker from creating two distinct documents with the same hash. In practice, collision resistance is insufficient for many practical uses. In addition to collision resistance, it should be impossible for an adversary to find two messages with substantially similar digests; or to infer any useful information about the data, given only its digest. In particular, a hash function should behave as much as possible like a random function (often called a random oracle in proofs of security) while still being deterministic and efficiently computable. This rules out functions like the SWIFFT function, which can be rigorously proven to be collision resistant assuming that certain problems on ideal lattices are computationally difficult, but as a linear function, does not satisfy these additional properties. Checksum algorithms, such as CRC32 and other cyclic redundancy checks , are designed to meet much weaker requirements, and are generally unsuitable as cryptographic hash functions. For example, a CRC was used for message integrity in the WEP encryption standard, but an attack was readily discovered which exploited the linearity of the checksum. Applications # Cryptographic hash functions have many information-security applications, notably in digital signatures , message authentication codes (MACs), and other forms of authentication . They can also be used as ordinary hash functions , to index data in hash tables , for fingerprinting , to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called ( digital ) fingerprints , checksums , or just hash values , even though all these terms stand for more general functions with rather different properties and purposes. Cryptographic hash algorithms # MD5 SHA-1 RIPEMD-160 Bcrypt Whirlpool (cryptography) SHA-2 SHA-3 BLAKE2","title":"Cryptographic-hash-function"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#cryptographic_hash_function","text":"A cryptographic hash function ( CHF ) is a hash function that is suitable for use in cryptography . It is a mathematical algorithm that maps data of arbitrary size (often called the \"message\") to a bit string of a fixed size (the \"hash value\", \"hash\", or \"message digest\") and is a one-way function , that is, a function which is practically infeasible to invert. Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Cryptographic hash functions are a basic tool of modern cryptography.","title":"Cryptographic hash function"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#properties","text":"The ideal cryptographic hash function has the following main properties: it is deterministic , meaning that the same message always results in the same hash it is quick to compute the hash value for any given message Note: The above two properties are the basic properties a good hash function should satisfy, which is introduced in Hash function . The below three properties are necessary for a ideal cryptographic hash function: it is infeasible to generate a message that yields a given hash value it is infeasible to find two different messages with the same hash value a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value ( avalanche effect ) A cryptographic hash function must be able to withstand all known types of cryptanalytic attack . In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance Given a hash value h it should be difficult to find any message m such that h = hash( m ). This concept is related to that of a one-way function . Functions that lack this property are vulnerable to preimage attacks . Second pre-image resistance Given an input m 1, it should be difficult to find a different input m 2 such that hash( m 1) = hash( m 2). This property is sometimes referred to as weak collision resistance . Functions that lack this property are vulnerable to second-preimage attacks . Collision resistance It should be difficult to find two different messages m 1 and m 2 such that hash( m 1) = hash( m 2). Such a pair is called a cryptographic hash collision . This property is sometimes referred to as strong collision resistance . It requires a hash value at least twice as long as that required for pre-image resistance; otherwise collisions may be found by a birthday attack . Collision resistance implies second pre-image resistance , but does not imply pre-image resistance . Informally, these properties mean that a malicious adversary cannot replace or modify the input data without changing its digest . Thus, if two strings have the same digest, one can be very confident that they are identical. Second pre-image resistance prevents an attacker from crafting a document with the same hash as a document the attacker cannot control. Collision resistance prevents an attacker from creating two distinct documents with the same hash. In practice, collision resistance is insufficient for many practical uses. In addition to collision resistance, it should be impossible for an adversary to find two messages with substantially similar digests; or to infer any useful information about the data, given only its digest. In particular, a hash function should behave as much as possible like a random function (often called a random oracle in proofs of security) while still being deterministic and efficiently computable. This rules out functions like the SWIFFT function, which can be rigorously proven to be collision resistant assuming that certain problems on ideal lattices are computationally difficult, but as a linear function, does not satisfy these additional properties. Checksum algorithms, such as CRC32 and other cyclic redundancy checks , are designed to meet much weaker requirements, and are generally unsuitable as cryptographic hash functions. For example, a CRC was used for message integrity in the WEP encryption standard, but an attack was readily discovered which exploited the linearity of the checksum.","title":"Properties"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#applications","text":"Cryptographic hash functions have many information-security applications, notably in digital signatures , message authentication codes (MACs), and other forms of authentication . They can also be used as ordinary hash functions , to index data in hash tables , for fingerprinting , to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called ( digital ) fingerprints , checksums , or just hash values , even though all these terms stand for more general functions with rather different properties and purposes.","title":"Applications"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#cryptographic_hash_algorithms","text":"MD5 SHA-1 RIPEMD-160 Bcrypt Whirlpool (cryptography) SHA-2 SHA-3 BLAKE2","title":"Cryptographic hash algorithms"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Salt(cryptography)/","text":"Salt (cryptography) # Application # Python # Reference the python doc: object.__hash__ ( self ) \u00b6","title":"Salt(cryptography)"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Salt(cryptography)/#salt_cryptography","text":"","title":"Salt (cryptography)"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Salt(cryptography)/#application","text":"","title":"Application"},{"location":"Hash/Hash-function/Cryptographic-hash-functions/Salt(cryptography)/#python","text":"Reference the python doc: object.__hash__ ( self ) \u00b6","title":"Python"},{"location":"Hash/Hash-function/Geometric-hashing/wikipedia-Geometric-hashing/","text":"Geometric hashing #","title":"[Geometric hashing](https://en.wikipedia.org/wiki/Geometric_hashing)"},{"location":"Hash/Hash-function/Geometric-hashing/wikipedia-Geometric-hashing/#geometric_hashing","text":"","title":"Geometric hashing"},{"location":"Hash/Randomization/wikipedia-Randomization-function/","text":"Randomization function #","title":"[Randomization function](https://en.wikipedia.org/wiki/Randomization_function)"},{"location":"Hash/Randomization/wikipedia-Randomization-function/#randomization_function","text":"","title":"Randomization function"},{"location":"List/Chain/","text":"Chain # \u79d1\u5b66\u9886\u57df\u7684\u547d\u540d\u5176\u5b9e\u4e5f\u9075\u5faa\u8fd9\u4e00\u5b9a\u7684\u89c4\u8303\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u5728\u79d1\u5b66\u9886\u57df\u4f7f\u7528chain\u6765\u547d\u540d\u7684\u4e00\u4e9b\u6982\u5ff5\uff1a Chain (disambiguation) Markov chain Chain rule Hash chain Blockchain \u8fd9\u4e9b\u6240\u8868\u793a\u7684\u6982\u5ff5\uff0c\u5176\u5b9e\u90fd\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u4e0b\u9762\u8fd8\u6709\u4ee5\u4e0b\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u7684\uff1a Linked timestamping","title":"Chain"},{"location":"List/Chain/#chain","text":"\u79d1\u5b66\u9886\u57df\u7684\u547d\u540d\u5176\u5b9e\u4e5f\u9075\u5faa\u8fd9\u4e00\u5b9a\u7684\u89c4\u8303\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u5728\u79d1\u5b66\u9886\u57df\u4f7f\u7528chain\u6765\u547d\u540d\u7684\u4e00\u4e9b\u6982\u5ff5\uff1a Chain (disambiguation) Markov chain Chain rule Hash chain Blockchain \u8fd9\u4e9b\u6240\u8868\u793a\u7684\u6982\u5ff5\uff0c\u5176\u5b9e\u90fd\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u4e0b\u9762\u8fd8\u6709\u4ee5\u4e0b\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u7684\uff1a Linked timestamping","title":"Chain"},{"location":"List/List/","text":"List (abstract data type) Linked list Related data structures Related data structures Unrolled linked list List (abstract data type) # Linked list # Related data structures # Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list. Related data structures # Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list. Unrolled linked list # \u5c55\u5f00\u7684\u94fe\u8868 In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. It can dramatically increase cache performance, while decreasing the memory overhead associated with storing list metadata such as references . It is related to the B-tree .","title":"List"},{"location":"List/List/#list_abstract_data_type","text":"","title":"List (abstract data type)"},{"location":"List/List/#linked_list","text":"","title":"Linked list"},{"location":"List/List/#related_data_structures","text":"Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.","title":"Related data structures"},{"location":"List/List/#related_data_structures_1","text":"Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.","title":"Related data structures"},{"location":"List/List/#unrolled_linked_list","text":"\u5c55\u5f00\u7684\u94fe\u8868 In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. It can dramatically increase cache performance, while decreasing the memory overhead associated with storing list metadata such as references . It is related to the B-tree .","title":"Unrolled linked list"},{"location":"List/VS-Unrolled-linked list-VS-B-tree/","text":"\u5728 Unrolled linked list \u4e2d\u6709\u5982\u4e0b\u63cf\u8ff0 It is related to the B-tree","title":"VS-Unrolled-linked list-VS-B-tree"},{"location":"List/VS-double-linked-list-VS-binary-tree/","text":"double\u548cbinary\u90fd\u662f2\uff0c\u4e24\u8005\u7684\u8282\u70b9\u90fd\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0bstruct\u6765\u8868\u793a struct Node{ void *value; struct Node* next; // left struct Node* prev; // right } https://stackoverflow.com/questions/11392922/what-is-the-difference-between-node-structures-of-double-linked-list-and-binary https://cs.stackexchange.com/questions/74372/is-binary-tree-just-a-two-dimension-doubly-linked-list https://www.ritambhara.in/convert-a-binary-tree-to-a-doubly-linked-list/ \u5728https://en.wikipedia.org/wiki/Linked_list#Related_data_structures\u6709\u63d0\u53ca\u8fd9\u4e2a\u95ee\u9898 binary search tree \u9000\u5316\u4e3a linked list # binary search tree\u662f\u53ef\u80fd\u9000\u5316\u4e3alinked list\u7684\uff1b","title":"VS-double-linked-list-VS-binary-tree"},{"location":"List/VS-double-linked-list-VS-binary-tree/#binary_search_tree_linked_list","text":"binary search tree\u662f\u53ef\u80fd\u9000\u5316\u4e3alinked list\u7684\uff1b","title":"binary search tree \u9000\u5316\u4e3a linked list"},{"location":"Probabilistic-data-structures/Bloom-filter/","text":"Bloom filter #","title":"Bloom-filter"},{"location":"Probabilistic-data-structures/Bloom-filter/#bloom_filter","text":"","title":"Bloom filter"},{"location":"Probabilistic-data-structures/Random-tree/","text":"Random tree #","title":"Random-tree"},{"location":"Probabilistic-data-structures/Random-tree/#random_tree","text":"","title":"Random tree"},{"location":"Probabilistic-data-structures/Skip-list/","text":"Skip list Description Implementation details Skip list # In computer science , a skip list is a data structure that allows ${{O}}(\\log n)$ search complexity as well as $ {{O}}(\\log n)$ insertion complexity within an ordered sequence of ${n}$ elements. Thus it can get the best of array (for searching) while maintaining a linked list -like structure that allows insertion- which is not possible in an array. Fast search is made possible by maintaining a linked hierarchy of subsequences , with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right). Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically [ 2] or deterministically,[ 3] with the former being more common. SUMMARY : subsequence\u7684\u542b\u4e49\u662f\u5b50\u5e8f\u5217 Description # A skip list is built in layers . The bottom layer is an ordinary ordered linked list . Each higher layer acts as an \"express lane\"\uff08\u5feb\u8f66\u9053\uff09 for the lists below, where an element in layer ${i}$ appears in layer ${i+1}$ with some fixed probability ${p}$ (two commonly used values for ${p}$ are ${1/2}$ or ${1/4}$ ). On average, each element appears in ${ 1/(1-p)}$ lists, and the tallest element (usually a special head element at the front of the skip list) in all the lists. The skip list contains ${\\log _{1/p}n\\,}$ (i.e. logarithm base ${ 1/p}$ of ${ n}$) lists. SUMMARY : layers \u5bf9\u5e94\u7684\u662f\u4e0b\u9762\u7684Inserting elements to skip list\u56fe\u4e2d\u7684levels A schematic picture of the skip list data structure. Each box with an arrow represents a pointer and a row is a linked list giving a sparse subsequence; the numbered boxes (in yellow) at the bottom represent the ordered data sequence. Searching proceeds downwards from the sparsest subsequence at the top until consecutive elements bracketing the search element are found. Inserting elements to skip list A search for a target element begins at the head element in the top list , and proceeds horizontally\uff08\u6c34\u5e73\u7684\uff09 until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most ${\\displaystyle 1/p}$, which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total expected cost of a search is ${\\displaystyle {\\tfrac {1}{p}}\\log _{1/p}n}$ which is ${\\displaystyle {\\mathcal {O}}(\\log n)\\,}$, when ${\\displaystyle p}$ is a constant. By choosing different values of ${\\displaystyle p}$, it is possible to trade search costs against storage costs . Implementation details # The elements used for a skip list can contain more than one pointer since they can participate in more than one list. Insertions and deletions are implemented much like the corresponding linked-list operations, except that \"tall\" elements must be inserted into or deleted from more than one linked list. ${\\displaystyle {\\mathcal {O}}(n)} $operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to ${\\displaystyle {\\mathcal {O}}(\\log n)}$search time. (Choose the level of the i'th finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd. Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them. Alternatively, we could make the level structure quasi-random in the following way:","title":"Skip-list"},{"location":"Probabilistic-data-structures/Skip-list/#skip_list","text":"In computer science , a skip list is a data structure that allows ${{O}}(\\log n)$ search complexity as well as $ {{O}}(\\log n)$ insertion complexity within an ordered sequence of ${n}$ elements. Thus it can get the best of array (for searching) while maintaining a linked list -like structure that allows insertion- which is not possible in an array. Fast search is made possible by maintaining a linked hierarchy of subsequences , with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right). Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically [ 2] or deterministically,[ 3] with the former being more common. SUMMARY : subsequence\u7684\u542b\u4e49\u662f\u5b50\u5e8f\u5217","title":"Skip list"},{"location":"Probabilistic-data-structures/Skip-list/#description","text":"A skip list is built in layers . The bottom layer is an ordinary ordered linked list . Each higher layer acts as an \"express lane\"\uff08\u5feb\u8f66\u9053\uff09 for the lists below, where an element in layer ${i}$ appears in layer ${i+1}$ with some fixed probability ${p}$ (two commonly used values for ${p}$ are ${1/2}$ or ${1/4}$ ). On average, each element appears in ${ 1/(1-p)}$ lists, and the tallest element (usually a special head element at the front of the skip list) in all the lists. The skip list contains ${\\log _{1/p}n\\,}$ (i.e. logarithm base ${ 1/p}$ of ${ n}$) lists. SUMMARY : layers \u5bf9\u5e94\u7684\u662f\u4e0b\u9762\u7684Inserting elements to skip list\u56fe\u4e2d\u7684levels A schematic picture of the skip list data structure. Each box with an arrow represents a pointer and a row is a linked list giving a sparse subsequence; the numbered boxes (in yellow) at the bottom represent the ordered data sequence. Searching proceeds downwards from the sparsest subsequence at the top until consecutive elements bracketing the search element are found. Inserting elements to skip list A search for a target element begins at the head element in the top list , and proceeds horizontally\uff08\u6c34\u5e73\u7684\uff09 until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most ${\\displaystyle 1/p}$, which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total expected cost of a search is ${\\displaystyle {\\tfrac {1}{p}}\\log _{1/p}n}$ which is ${\\displaystyle {\\mathcal {O}}(\\log n)\\,}$, when ${\\displaystyle p}$ is a constant. By choosing different values of ${\\displaystyle p}$, it is possible to trade search costs against storage costs .","title":"Description"},{"location":"Probabilistic-data-structures/Skip-list/#implementation_details","text":"The elements used for a skip list can contain more than one pointer since they can participate in more than one list. Insertions and deletions are implemented much like the corresponding linked-list operations, except that \"tall\" elements must be inserted into or deleted from more than one linked list. ${\\displaystyle {\\mathcal {O}}(n)} $operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to ${\\displaystyle {\\mathcal {O}}(\\log n)}$search time. (Choose the level of the i'th finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd. Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them. Alternatively, we could make the level structure quasi-random in the following way:","title":"Implementation details"},{"location":"Set/Set(abstract-data-type)/","text":"Set (abstract data type) Implementations Set (abstract data type) # In computer science , a set is an abstract data type that can store unique values, without any particular order . It is a computer implementation of the mathematical concept of a finite set . Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set. Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements \u2014 such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets , allow also the insertion and deletion of elements from the set. A multiset is a special kind of set in which an element can figure several times. Implementations # Sets can be implemented using various data structures , which provide different time and space trade-offs for various operations. Some implementations are designed to improve the efficiency of very specialized operations, such as nearest or union . Implementations described as \"general use\" typically strive to optimize the element_of , add , and delete operations. A simple implementation is to use a list , ignoring the order of the elements and taking care to avoid repeated values. This is simple but inefficient, as operations like set membership or element deletion are O ( n ), as they require scanning the entire list.[ b] Sets are often instead implemented using more efficient data structures, particularly various flavors of trees , tries , or hash tables . As sets can be interpreted as a kind of map (by the indicator function), sets are commonly implemented in the same way as (partial) maps ( associative arrays ) \u2013 in this case in which the value of each key-value pair has the unit type or a sentinel value (like 1) \u2013 namely, a self-balancing binary search tree for sorted sets (which has O(log n) for most operations), or a hash table for unsorted sets (which has O(1) average-case, but O(n) worst-case, for most operations). A sorted linear hash table[ 8] may be used to provide deterministically ordered sets. Further, in languages that support maps but not sets, sets can be implemented in terms of maps. For example, a common programming idiom in Perl that converts an array to a hash whose values are the sentinel value 1, for use as a set, is: my %elements = map { $_ => 1 } @elements; Other popular methods include arrays . In particular a subset of the integers 1.. n can be implemented efficiently as an n -bit bit array , which also support very efficient union and intersection operations. A Bloom map implements a set probabilistically, using a very compact representation but risking a small chance of false positives on queries. The Boolean set operations can be implemented in terms of more elementary operations ( pop , clear , and add ), but specialized algorithms may yield lower asymptotic time bounds. If sets are implemented as sorted lists, for example, the naive algorithm for union(*S*,*T*) will take time proportional to the length m of S times the length n of T ; whereas a variant of the list merging algorithm will do the job in time proportional to m + n . Moreover, there are specialized set data structures (such as the union-find data structure ) that are optimized for one or more of these operations, at the expense of others. How is set() implemented in python ? What are the underlying data structures used for Redis? How is the Redis sorted set implemented?","title":"Set"},{"location":"Set/Set(abstract-data-type)/#set_abstract_data_type","text":"In computer science , a set is an abstract data type that can store unique values, without any particular order . It is a computer implementation of the mathematical concept of a finite set . Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set. Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements \u2014 such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets , allow also the insertion and deletion of elements from the set. A multiset is a special kind of set in which an element can figure several times.","title":"Set (abstract data type)"},{"location":"Set/Set(abstract-data-type)/#implementations","text":"Sets can be implemented using various data structures , which provide different time and space trade-offs for various operations. Some implementations are designed to improve the efficiency of very specialized operations, such as nearest or union . Implementations described as \"general use\" typically strive to optimize the element_of , add , and delete operations. A simple implementation is to use a list , ignoring the order of the elements and taking care to avoid repeated values. This is simple but inefficient, as operations like set membership or element deletion are O ( n ), as they require scanning the entire list.[ b] Sets are often instead implemented using more efficient data structures, particularly various flavors of trees , tries , or hash tables . As sets can be interpreted as a kind of map (by the indicator function), sets are commonly implemented in the same way as (partial) maps ( associative arrays ) \u2013 in this case in which the value of each key-value pair has the unit type or a sentinel value (like 1) \u2013 namely, a self-balancing binary search tree for sorted sets (which has O(log n) for most operations), or a hash table for unsorted sets (which has O(1) average-case, but O(n) worst-case, for most operations). A sorted linear hash table[ 8] may be used to provide deterministically ordered sets. Further, in languages that support maps but not sets, sets can be implemented in terms of maps. For example, a common programming idiom in Perl that converts an array to a hash whose values are the sentinel value 1, for use as a set, is: my %elements = map { $_ => 1 } @elements; Other popular methods include arrays . In particular a subset of the integers 1.. n can be implemented efficiently as an n -bit bit array , which also support very efficient union and intersection operations. A Bloom map implements a set probabilistically, using a very compact representation but risking a small chance of false positives on queries. The Boolean set operations can be implemented in terms of more elementary operations ( pop , clear , and add ), but specialized algorithms may yield lower asymptotic time bounds. If sets are implemented as sorted lists, for example, the naive algorithm for union(*S*,*T*) will take time proportional to the length m of S times the length n of T ; whereas a variant of the list merging algorithm will do the job in time proportional to m + n . Moreover, there are specialized set data structures (such as the union-find data structure ) that are optimized for one or more of these operations, at the expense of others. How is set() implemented in python ? What are the underlying data structures used for Redis? How is the Redis sorted set implemented?","title":"Implementations"},{"location":"Stack/Application-of-stack/","text":"\u524d\u8a00 \u6808\u7279\u6027\uff1a\u5148\u8fdb\u540e\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09 \u6808\u7279\u6027\uff1a\u6709\u8fdb\u6709\u51fa \u6808\u7279\u6027\uff1a\u7ebf\u6027 All nearest smaller values \u62ec\u53f7\u5339\u914d\u95ee\u9898 stack in automata theory \u663e\u793a\u6808\u548c\u9690\u5f0f\u6808 \u524d\u8a00 # \u6808\u6709\u7740\u975e\u5e38\u591a\u7684\u5e94\u7528\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u8fd9\u4e9b\u7eb7\u7e41\u590d\u6742\u7684\u5e94\u7528\u4e3b\u8981\u4f7f\u7528\u4e86\u6808\u7684\u54ea\u4e9b\u7279\u6027\u3002 \u6808\u7279\u6027\uff1a\u5148\u8fdb\u540e\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09 # \u6808\u7684\u540e\u8fdb\u5148\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09\u7279\u6027\u5176\u5b9e\u5b58\u5728\u7740\u4e00\u79cd\u5929\u7136\u7684 \u9006\u5e8f \uff08\u4e0e\u6b64\u76f8\u53cd\u7684\u662fqueue\u7684\u5148\u8fdb\u5148\u51fa\u5219\u662f\u5929\u7136\u7684 \u987a\u5e8f \uff09 - \u9006\u5e8f \u6808\u7279\u6027\uff1a\u6709\u8fdb\u6709\u51fa # \u62ec\u53f7\u5339\u914d\uff08\u6b63\u62ec\u53f7\u8fdb\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff09\uff0c\u51fd\u6570\u6267\u884c\uff08\u8c03\u7528\u51fd\u6570\u8fdb\u6808\uff0c\u51fd\u6570\u8fd4\u56de\u51fa\u6808\uff09 \u6808\u7279\u6027\uff1a\u7ebf\u6027 # \u6808\u662f\u4e00\u79cd \u7ebf\u6027 \u7684\u6570\u636e\u7ed3\u6784\uff0c\u4e00\u4e9b \u7ebf\u6027\u5e8f\u5217 \u53ef\u4ee5\u57fa\u4e8e\u6808\u6765\u505a\u4e00\u4e9b \u76f8\u90bb\u5143\u7d20 \u7684\u57fa\u4e8e \u67d0\u79cd\u5173\u7cfb \u7684 \u805a\u5408 \u3002 - \u76f8\u90bb - \u57fa\u4e8e\u67d0\u79cd\u5173\u7cfb\u7684\u805a\u5408 All nearest smaller values # \u4f7f\u7528\u6808\u6765\u8868\u793a \u6700\u8fd1 \u62ec\u53f7\u5339\u914d\u95ee\u9898 # \u5176\u5b9e\u62ec\u53f7\u5339\u914d\u95ee\u9898\u4e5f\u53ef\u4ee5\u4f7f\u7528\u8fd9\u79cd\u601d\u8def\u6765\u8fdb\u884c\u5206\u6790\uff1a\u5305\u542b\u6709\u62ec\u53f7\u5bf9\u7684\u5b57\u7b26\u4e32\u662f \u7ebf\u6027 \u7684\uff0c\u5728\u8fdb\u884c\u62ec\u53f7\u5339\u914d\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4ec5\u4ec5\u5173\u6ce8\u7684\u662f\u62ec\u53f7\u5b57\u7b26\u800c\u5ffd\u89c6\u6240\u6709\u7684\u5176\u4ed6\u5b57\u7b26\uff0c\u6240\u4ee5\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u5b57\u7b26\u4e32\u4ec5\u4ec5\u5305\u542b\u4e86\u62ec\u53f7\u5bf9\uff0c\u6240\u4ee5\u5176\u4e2d\u7684\u5143\u7d20\u90fd\u662f\u76f8\u90bb\u7684\uff1b \u6b63\u62ec\u53f7 \u4e0e \u53cd\u62ec\u53f7 \u4e4b\u95f4\u5b58\u5728\u7684\u5173\u7cfb\u662f \u5339\u914d\u5173\u7cfb \uff0c\u53ea\u6709\u5f53\u76f8\u90bb\u4e24\u4e2a\u62ec\u53f7\u5b57\u7b26\u4e4b\u95f4\u5b58\u5728\u7740 \u5339\u914d\u5173\u7cfb \u7684\u65f6\u5019\uff0c\u6211\u4eec\u624d\u5c06\u5b83\u4eec \u805a\u5408 \uff08\u5bf9\u5e94\u8fd9\u5c06\u51fa\u6808\uff09\uff1b\u5176\u5b9e\u8fd9\u79cd\u6846\u67b6\u662f\u80fd\u591f\u89e3\u51b3\u975e\u5e38\u591a\u7684\u7c7b\u4f3c\u8fd9\u6837\u7684\u95ee\u9898\u7684 stack in automata theory # Deterministic pushdown automaton Pushdown automaton \u663e\u793a\u6808\u548c\u9690\u5f0f\u6808 # \u9690\u5f0f\u6808\u662f\u6307\u4f7f\u7528call stack\uff0c\u6bd4\u5982\uff1a python\u7684 pgen \u4f7f\u7528\u9690\u5f0f\u6808\u6765\u5b9e\u73b0\u62ec\u53f7\u7684\u5339\u914d","title":"Application-of-stack"},{"location":"Stack/Application-of-stack/#_1","text":"\u6808\u6709\u7740\u975e\u5e38\u591a\u7684\u5e94\u7528\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u8fd9\u4e9b\u7eb7\u7e41\u590d\u6742\u7684\u5e94\u7528\u4e3b\u8981\u4f7f\u7528\u4e86\u6808\u7684\u54ea\u4e9b\u7279\u6027\u3002","title":"\u524d\u8a00"},{"location":"Stack/Application-of-stack/#_2","text":"\u6808\u7684\u540e\u8fdb\u5148\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09\u7279\u6027\u5176\u5b9e\u5b58\u5728\u7740\u4e00\u79cd\u5929\u7136\u7684 \u9006\u5e8f \uff08\u4e0e\u6b64\u76f8\u53cd\u7684\u662fqueue\u7684\u5148\u8fdb\u5148\u51fa\u5219\u662f\u5929\u7136\u7684 \u987a\u5e8f \uff09 - \u9006\u5e8f","title":"\u6808\u7279\u6027\uff1a\u5148\u8fdb\u540e\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09"},{"location":"Stack/Application-of-stack/#_3","text":"\u62ec\u53f7\u5339\u914d\uff08\u6b63\u62ec\u53f7\u8fdb\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff09\uff0c\u51fd\u6570\u6267\u884c\uff08\u8c03\u7528\u51fd\u6570\u8fdb\u6808\uff0c\u51fd\u6570\u8fd4\u56de\u51fa\u6808\uff09","title":"\u6808\u7279\u6027\uff1a\u6709\u8fdb\u6709\u51fa"},{"location":"Stack/Application-of-stack/#_4","text":"\u6808\u662f\u4e00\u79cd \u7ebf\u6027 \u7684\u6570\u636e\u7ed3\u6784\uff0c\u4e00\u4e9b \u7ebf\u6027\u5e8f\u5217 \u53ef\u4ee5\u57fa\u4e8e\u6808\u6765\u505a\u4e00\u4e9b \u76f8\u90bb\u5143\u7d20 \u7684\u57fa\u4e8e \u67d0\u79cd\u5173\u7cfb \u7684 \u805a\u5408 \u3002 - \u76f8\u90bb - \u57fa\u4e8e\u67d0\u79cd\u5173\u7cfb\u7684\u805a\u5408","title":"\u6808\u7279\u6027\uff1a\u7ebf\u6027"},{"location":"Stack/Application-of-stack/#all_nearest_smaller_values","text":"\u4f7f\u7528\u6808\u6765\u8868\u793a \u6700\u8fd1","title":"All nearest smaller values"},{"location":"Stack/Application-of-stack/#_5","text":"\u5176\u5b9e\u62ec\u53f7\u5339\u914d\u95ee\u9898\u4e5f\u53ef\u4ee5\u4f7f\u7528\u8fd9\u79cd\u601d\u8def\u6765\u8fdb\u884c\u5206\u6790\uff1a\u5305\u542b\u6709\u62ec\u53f7\u5bf9\u7684\u5b57\u7b26\u4e32\u662f \u7ebf\u6027 \u7684\uff0c\u5728\u8fdb\u884c\u62ec\u53f7\u5339\u914d\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4ec5\u4ec5\u5173\u6ce8\u7684\u662f\u62ec\u53f7\u5b57\u7b26\u800c\u5ffd\u89c6\u6240\u6709\u7684\u5176\u4ed6\u5b57\u7b26\uff0c\u6240\u4ee5\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u5b57\u7b26\u4e32\u4ec5\u4ec5\u5305\u542b\u4e86\u62ec\u53f7\u5bf9\uff0c\u6240\u4ee5\u5176\u4e2d\u7684\u5143\u7d20\u90fd\u662f\u76f8\u90bb\u7684\uff1b \u6b63\u62ec\u53f7 \u4e0e \u53cd\u62ec\u53f7 \u4e4b\u95f4\u5b58\u5728\u7684\u5173\u7cfb\u662f \u5339\u914d\u5173\u7cfb \uff0c\u53ea\u6709\u5f53\u76f8\u90bb\u4e24\u4e2a\u62ec\u53f7\u5b57\u7b26\u4e4b\u95f4\u5b58\u5728\u7740 \u5339\u914d\u5173\u7cfb \u7684\u65f6\u5019\uff0c\u6211\u4eec\u624d\u5c06\u5b83\u4eec \u805a\u5408 \uff08\u5bf9\u5e94\u8fd9\u5c06\u51fa\u6808\uff09\uff1b\u5176\u5b9e\u8fd9\u79cd\u6846\u67b6\u662f\u80fd\u591f\u89e3\u51b3\u975e\u5e38\u591a\u7684\u7c7b\u4f3c\u8fd9\u6837\u7684\u95ee\u9898\u7684","title":"\u62ec\u53f7\u5339\u914d\u95ee\u9898"},{"location":"Stack/Application-of-stack/#stack_in_automata_theory","text":"Deterministic pushdown automaton Pushdown automaton","title":"stack in automata theory"},{"location":"Stack/Application-of-stack/#_6","text":"\u9690\u5f0f\u6808\u662f\u6307\u4f7f\u7528call stack\uff0c\u6bd4\u5982\uff1a python\u7684 pgen \u4f7f\u7528\u9690\u5f0f\u6808\u6765\u5b9e\u73b0\u62ec\u53f7\u7684\u5339\u914d","title":"\u663e\u793a\u6808\u548c\u9690\u5f0f\u6808"},{"location":"Stack/Stack(abstract-data-type)/","text":"Stack (abstract data type) # In computer science , a stack is an abstract data type that serves as a collection of elements, with two principal operations: push , which adds an element to the collection, and pop , which removes the most recently added element that was not yet removed. The order in which elements come off a stack gives rise to its alternative name, LIFO ( last in, first out ). Additionally, a peek operation may give access to the top without modifying the stack. The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first. Considered as a linear data structure , or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search . Applications of stacks # Expression evaluation and syntax parsing # Calculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low level code. Most programming languages are context-free languages , allowing them to be parsed with stack based machines. Backtracking # Main article: Backtracking Another important application of stacks is backtracking . Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking. The prototypical example of a backtracking algorithm is depth-first search , which finds all vertices of a graph that can be reached from a specified starting vertex. Other applications of backtracking involve searching through spaces that represent potential solutions to an optimization problem. Branch and bound is a technique for performing such backtracking searches without exhaustively searching all of the potential solutions in such a space. Compile time memory management # Main articles: Stack-based memory allocation and Stack machine A number of programming languages are stack-oriented , meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine . Almost all calling conventions \u200d\u2014\u200cthe ways in which subroutines receive their parameters and return results\u200d\u2014\u200cuse a special stack (the \" call stack \") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer. Some programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program. Efficient algorithms # Several algorithms use a stack (separate from the usual function call stack of most programming languages) as the principle data structure with which they organize their information. These include: Graham scan , an algorithm for the convex hull of a two-dimensional system of points. A convex hull of a subset of the input is maintained in a stack, which is used to find and remove concavities in the boundary when a new point is added to the hull.[ 10] Part of the SMAWK algorithm for finding the row minima of a monotone matrix uses stacks in a similar way to Graham scan.[ 11] All nearest smaller values , the problem of finding, for each number in an array, the closest preceding number that is smaller than it. One algorithm for this problem uses a stack to maintain a collection of candidates for the nearest smaller value. For each position in the array, the stack is popped until a smaller value is found on its top, and then the value in the new position is pushed onto the stack.[ 12] The nearest-neighbor chain algorithm , a method for agglomerative hierarchical clustering based on maintaining a stack of clusters, each of which is the nearest neighbor of its predecessor on the stack. When this method finds a pair of clusters that are mutual nearest neighbors, they are popped and merged.[ 13] SUMMARY : \u6808\u7684\u540e\u8fdb\u5148\u51fa\u7684\u7279\u6027\u662f\u4e00\u79cd\u5012\u5e8f\u7684\u7279\u6027\uff0c\u5b83\u975e\u5e38\u9002\u5408\u4e8e\u5b9e\u73b0\u4e00\u4e9b\u9700\u8981\u6267\u884c\u9006\u8f6c\u64cd\u4f5c\u7684\u7b97\u6cd5\uff1b Security # Some computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations. For example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail. Malicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations. This type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.","title":"Stack"},{"location":"Stack/Stack(abstract-data-type)/#stack_abstract_data_type","text":"In computer science , a stack is an abstract data type that serves as a collection of elements, with two principal operations: push , which adds an element to the collection, and pop , which removes the most recently added element that was not yet removed. The order in which elements come off a stack gives rise to its alternative name, LIFO ( last in, first out ). Additionally, a peek operation may give access to the top without modifying the stack. The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first. Considered as a linear data structure , or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search .","title":"Stack (abstract data type)"},{"location":"Stack/Stack(abstract-data-type)/#applications_of_stacks","text":"","title":"Applications of stacks"},{"location":"Stack/Stack(abstract-data-type)/#expression_evaluation_and_syntax_parsing","text":"Calculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low level code. Most programming languages are context-free languages , allowing them to be parsed with stack based machines.","title":"Expression evaluation and syntax parsing"},{"location":"Stack/Stack(abstract-data-type)/#backtracking","text":"Main article: Backtracking Another important application of stacks is backtracking . Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking. The prototypical example of a backtracking algorithm is depth-first search , which finds all vertices of a graph that can be reached from a specified starting vertex. Other applications of backtracking involve searching through spaces that represent potential solutions to an optimization problem. Branch and bound is a technique for performing such backtracking searches without exhaustively searching all of the potential solutions in such a space.","title":"Backtracking"},{"location":"Stack/Stack(abstract-data-type)/#compile_time_memory_management","text":"Main articles: Stack-based memory allocation and Stack machine A number of programming languages are stack-oriented , meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine . Almost all calling conventions \u200d\u2014\u200cthe ways in which subroutines receive their parameters and return results\u200d\u2014\u200cuse a special stack (the \" call stack \") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer. Some programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program.","title":"Compile time memory management"},{"location":"Stack/Stack(abstract-data-type)/#efficient_algorithms","text":"Several algorithms use a stack (separate from the usual function call stack of most programming languages) as the principle data structure with which they organize their information. These include: Graham scan , an algorithm for the convex hull of a two-dimensional system of points. A convex hull of a subset of the input is maintained in a stack, which is used to find and remove concavities in the boundary when a new point is added to the hull.[ 10] Part of the SMAWK algorithm for finding the row minima of a monotone matrix uses stacks in a similar way to Graham scan.[ 11] All nearest smaller values , the problem of finding, for each number in an array, the closest preceding number that is smaller than it. One algorithm for this problem uses a stack to maintain a collection of candidates for the nearest smaller value. For each position in the array, the stack is popped until a smaller value is found on its top, and then the value in the new position is pushed onto the stack.[ 12] The nearest-neighbor chain algorithm , a method for agglomerative hierarchical clustering based on maintaining a stack of clusters, each of which is the nearest neighbor of its predecessor on the stack. When this method finds a pair of clusters that are mutual nearest neighbors, they are popped and merged.[ 13] SUMMARY : \u6808\u7684\u540e\u8fdb\u5148\u51fa\u7684\u7279\u6027\u662f\u4e00\u79cd\u5012\u5e8f\u7684\u7279\u6027\uff0c\u5b83\u975e\u5e38\u9002\u5408\u4e8e\u5b9e\u73b0\u4e00\u4e9b\u9700\u8981\u6267\u884c\u9006\u8f6c\u64cd\u4f5c\u7684\u7b97\u6cd5\uff1b","title":"Efficient algorithms"},{"location":"Stack/Stack(abstract-data-type)/#security","text":"Some computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations. For example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail. Malicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations. This type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.","title":"Security"},{"location":"Structure/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u8ba8\u8bba structure \uff0c\u4ee5\u53ca\u7531\u5b83\u800c\u884d\u751f\u7684\u5173\u4e8erelation\u7684\u8ba8\u8bba\u3002","title":"Introduction"},{"location":"Structure/#_1","text":"\u672c\u7ae0\u8ba8\u8bba structure \uff0c\u4ee5\u53ca\u7531\u5b83\u800c\u884d\u751f\u7684\u5173\u4e8erelation\u7684\u8ba8\u8bba\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Structure/Hierarchy/","text":"Hierarchy # \u672c\u6587\u4e3b\u8981\u662f\u60f3\u641e\u6e05\u695ahierarchy\u548ctree structure\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u56e0\u4e3a\u5728\u6b64\u4e4b\u524d\u6211\u4e00\u76f4\u5c06hierarchy structure\u770b\u505a\u662ftree structure\u7684\u540c\u4e49\u8bcd\uff0c\u5373hierarchy structure\u5c31\u4e00\u5b9a\u662ftree structure\uff0c\u8fd9\u4e2a\u89c2\u5ff5\u662f\u6e90\u81ea\u4e8e Tree structure \u4e2d\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. \u4fc3\u4f7f\u6211\u5bf9\u8fd9\u4e2a\u89c2\u5ff5\u4ea7\u751f\u6000\u7591\u7684\u662f\u8fd9\u6837\u7684\u4e00\u4e2a\u95ee\u9898\uff1a \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u56fe \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u652f\u6301\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u5b83\u4f9d\u7136\u662fhierarchy\u7ed3\u6784\uff0c\u4f46\u662f\u5b83\u4e0d\u80fd\u662ftree\u4e86\uff0c\u56e0\u4e3a\u5b83\u6210\u73af\u4e86\uff08\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1 Discrete Mathematics and Its Applications \u4e2dTree\u7ae0\u8282\uff09\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a |---Class 2-------------| | | class 1---| |-class 4 | | |---Class 3-------------| \u4e0a\u56fe\u4ece\u5de6\u81f3\u53f3\u8868\u793a\u7ee7\u627f\u5173\u7cfb\uff0c\u4e0a\u56fe\u4e0d\u662f\u4e00\u4e2atree\uff0c\u800c\u662fgraph\u3002\u4f46\u662f\u4e0a\u56fe\u662f\u6ee1\u8db3hierarchy\u7ed3\u679c\u7684\u3002 \u5982\u4e0b\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u7684\u7b14\u8bb0\u3002 A hierarchy (from the Greek hierarkhia , \"rule of a high priest\", from hierarkhes , \"president of sacred rites\") is an arrangement of items (objects, names, values, categories, etc.) in which the items are represented as being \"above\", \"below\", or \"at the same level as\" one another. A hierarchy can link entities either directly or indirectly, and either vertically or diagonally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior\uff08\u4e0a\u7ea7\uff09 or to one of one's subordinates\uff08\u4e0b\u5c5e\uff09, although a system that is largely hierarchical can also incorporate alternative hierarchies. Hierarchical links can extend \"vertically\" upwards or downwards via multiple links in the same direction, following a path . All parts of the hierarchy that are not linked vertically to one another nevertheless can be \"horizontally\" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues \uff08\u540c\u4e00\u5c42\uff09; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy is one such form. \u201chierarchy\u201d\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u5c42\u7ea7\u201d\uff0c\u6211\u4eec\u5e73\u65f6\u5e38\u5e38\u6240\u8bf4\u7684\u201c\u7b49\u7ea7\u201d\u4e0e\u5b83\u7684\u542b\u4e49\u7c7b\u4f3c\u3002Hierarchy\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662flevel\u4ee5\u53calevel\u4e4b\u95f4\u7684\u5173\u7cfb\uff08above-below\u5173\u7cfb\u6216superior-subordinates\u5173\u7cfb\uff09\u3002\u53ef\u4ee5\u5c06Hierarchy\u770b\u505a\u662f\u4e00\u79cd\u7ed3\u6784\uff08\u5143\u7d20\u548c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff09\uff0cHierarchy\u7684\u5173\u7cfb\u4e3aabove-below\u5173\u7cfb\u3001superior-subordinates\u5173\u7cfb\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528computer science \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8egraph structure\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528math \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8e partially ordered set \uff0c\u53c2\u89c1\u539f\u6587 Mathematical representation \u7ae0\u8282\u3002 Hierachy\u7ed3\u6784\u5e76\u6ca1\u6709\u9650\u5236\u4e24\u5c42\u5143\u7d20\u4e4b\u95f4\u7684link\uff0c\u6309\u7167\u4e0a\u8ff0\u7684\u63cf\u8ff0\uff0c\u5b83\u662f\u5141\u8bb8\u67d0\u4e00\u5c42\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20\u540c\u65f6\u6709\u4e24\u4e2asuperior\uff0c\u8fd9\u79cd\u60c5\u51b5\u5c31\u662f\u672c\u6587\u5f00\u5934\u6240\u5217\u4e3e\u7684\u591a\u7ee7\u627f\u3002 \u601d\u8003\uff1a\u65e2\u7136hierarchy\u662f\u4e00\u79cd\u7ed3\u6784\uff0c\u90a3\u4e48\u6309\u7167\u54ea\u79cdrelation\u6765\u7ec4\u7ec7\u5143\u7d20\u65e0\u6cd5\u5f62\u6210hierarchy\uff1f \u8fd9\u4e2a\u95ee\u9898\u539f\u6587\u4e2d\u5e76\u6ca1\u6709\u7ed9\u51fa\u4e25\u683c\u7684\u8bf4\u660e\uff0c\u5728\u539f\u6587\u7684 Informal representation \u4e2d\u6709\u975e\u4e25\u683c\u7684\u63cf\u8ff0\u3002 Nomenclature \u547d\u540d\u6cd5 # \u7a0d\u5fae\u6d4f\u89c8\u4e86\u4e00\u4e0b\uff0c\u539f\u6587\u8fd9\u4e00\u6bb5\u4e2d\u7ed9\u51fa\u7684\u4e00\u4e9b\u672f\u8bed\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\u3002 Informal representation # In plain English, a hierarchy can be thought of as a set in which: No element is superior to itself, and One element, the hierarch \uff08\u6559\u4e3b\uff09, is superior to all of the other elements in the set. The first requirement is also interpreted to mean that a hierarchy can have no circular relationships ; the association between two objects is always transitive . The second requirement asserts that a hierarchy must have a leader or root that is common to all of the objects. \u5173\u4e8e\u7b2c\u4e00\u70b9\uff0c\u539f\u6587\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u201ca hierarchy can have no circular relationships \u201d\uff0c\u201c circular relationships \"\u6307\u5411\u7684\u662f\u56fe\u8bba\u4e2d\u7684 circle \uff0c\u4f5c\u8005\u6240\u60f3\u8981\u8868\u8fbe\u7684\u662f\uff0c\u4e00\u4e2ahierarchy\u662f\u4e0d\u80fd\u591f\u5b58\u5728\u73af\u7684\uff0c\u5426\u5219\u65e0\u6cd5\u4e25\u683c\u5730\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002\u5c31\u7b2c\u4e00\u70b9\u7684\u539f\u8bdd\uff0c\u6211\u89c9\u5f97\u4f7f\u7528relation\u7684\u7406\u8bba\u6765\u7406\u89e3\u7684\u8bdd\uff0c\u5b83\u8868\u793a\u7684\u662f\u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u591f\u662f\u4e00\u4e2a reflexive relation \uff0c\u5426\u5219\u5c31\u4f1a\u51fa\u73b0\u73af\u800c\u65e0\u6cd5\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002 Mathematical representation # Mathematically, in its most general form, a hierarchy is a partially ordered set or poset . Subtypes # \u201csubtype\u201d\u5373\u5b50\u7c7b\uff0c\u6240\u4ee5\u539f\u6587\u7684\u8fd9\u4e00\u8282\u6240\u63cf\u8ff0\u7684\u662f\u7279\u6b8a\u7c7b\u578b\u7684hierarchy\u3002 Nested hierarchy # A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . The concept of nesting is exemplified in Russian matryoshka dolls \uff08\u4fc4\u7f57\u65af\u5957\u5a03\uff09. Each doll is encompassed by another doll, all the way to the outer doll. The outer doll holds all of the inner dolls, the next outer doll holds all the remaining inner dolls, and so on. Matryoshkas represent a nested hierarchy where each level contains only one object, i.e., there is only one of each size of doll; a generalized nested hierarchy allows for multiple objects within levels but with each object having only one parent at each level . Nested hierarchies are the organizational schemes behind taxonomies \uff08\u5206\u7c7b\u5b66\uff09and systematic classifications. In many programming taxonomies and syntax models (as well as fractals in mathematics), nested hierarchies, including Russian dolls, are also used to illustrate the properties of self-similarity and recursion . Recursion itself is included as a subset of hierarchical programming, and recursive thinking can be synonymous with a form of hierarchical thinking and logic. Nested hierarchy structure\u5728hierarchy structure\u4e0a\u6dfb\u52a0\u7684\u9650\u5236\u662fnested \uff0c\u5b83\u4fdd\u8bc1\u4e86\u201c each object having only one parent at each level \"\uff0c\u663e\u7136\uff0c\u8fd9\u4e2a\u9650\u5236\u5c31\u662fgraph\u4e2d\u4e0d\u518d\u53ef\u80fd\u4ea7\u751f\u73af\u4e86\uff0c\u4e00\u4e2a\u4e0d\u5e26\u73af\u7684\u56fe\u5c31\u662ftree\uff0c\u6240\u4ee5nested hierarchy structure\u662ftree structure\uff0c\u76f8\u6bd4\u4e8ehierarchy\u800c\u8a00\uff0cnesting\u66f4\u52a0\u80fd\u591f\u4f53\u73b0tree\u7684\u7ed3\u6784\u7279\u5f81\u3002 nested hierarchy structure\u662f\u4e00\u79cd\u7279\u6b8a\u7684hierarchy structure\uff0ctree\u662f\u4e00\u79cd\u7279\u6b8a\u7684graph\u3002 \u601d\u8003\uff1a\u600e\u6837\u7684\u5173\u7cfb\u624d\u80fd\u591f\u4ea7\u751fnested hierarchy structure\uff1f\u8fd9\u4e2a\u95ee\u9898\u5728 Tree-structure \u4e2d\u4f1a\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba\u3002 \u539f\u6587\u8fd9\u4e00\u8282\u540e\u9762\u7684\u5185\u5bb9pass\u6389\u4e86\u3002 Hierarchy and data structure # Hierarchy structure\u53ef\u4ee5\u4f7f\u7528graph\u6765\u8fdb\u884c\u8868\u793a\u3002 Nested hierarchy structure\u53ef\u4ee5\u662f\u8981\u5f04tree\u6765\u8fdb\u884c\u8868\u793a\u4f60\u3002 Hierarchy structure and recursion # \u5177\u5907 hierarchical \u7ed3\u6784\u7684\u5f80\u5f80\u5177\u5907recursive\u7684\u7279\u5f81\u3002 \u4ece\u6570\u5b66\u7684\u89d2\u5ea6\u6765\u770b\uff0c Hierarchy structure\u662f\u4e00\u4e2a partially ordered set \uff0c\u6240\u4ee5\u8ba8\u8bbahierarchy structure\u7684\u9012\u5f52\u6027\u6700\u7ec8\u5e94\u8be5\u8fd8\u662f\u5f52\u4e8e\u8ba8\u8bbaposet\u7684\u9012\u5f52\u6027\u3002\u6211\u89c9\u5f97\u662f\u53ef\u4ee5\u4f7f\u7528tree\u7684\u9012\u5f52\u6027\u6765\u601d\u8003poset\u7684\u9012\u5f52\u6027\u7684\uff0c\u4e00\u4e2aposet\u7684subset\u4e5f\u5e94\u8be5\u662f\u4e00\u4e2aposet\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fposet\u5404\u5c42\u4f7f\u7528\u7684\u662f\u540c\u4e00\u4e2arelation\u3002","title":"Hierarchy"},{"location":"Structure/Hierarchy/#hierarchy","text":"\u672c\u6587\u4e3b\u8981\u662f\u60f3\u641e\u6e05\u695ahierarchy\u548ctree structure\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u56e0\u4e3a\u5728\u6b64\u4e4b\u524d\u6211\u4e00\u76f4\u5c06hierarchy structure\u770b\u505a\u662ftree structure\u7684\u540c\u4e49\u8bcd\uff0c\u5373hierarchy structure\u5c31\u4e00\u5b9a\u662ftree structure\uff0c\u8fd9\u4e2a\u89c2\u5ff5\u662f\u6e90\u81ea\u4e8e Tree structure \u4e2d\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. \u4fc3\u4f7f\u6211\u5bf9\u8fd9\u4e2a\u89c2\u5ff5\u4ea7\u751f\u6000\u7591\u7684\u662f\u8fd9\u6837\u7684\u4e00\u4e2a\u95ee\u9898\uff1a \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u56fe \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u652f\u6301\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u5b83\u4f9d\u7136\u662fhierarchy\u7ed3\u6784\uff0c\u4f46\u662f\u5b83\u4e0d\u80fd\u662ftree\u4e86\uff0c\u56e0\u4e3a\u5b83\u6210\u73af\u4e86\uff08\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1 Discrete Mathematics and Its Applications \u4e2dTree\u7ae0\u8282\uff09\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a |---Class 2-------------| | | class 1---| |-class 4 | | |---Class 3-------------| \u4e0a\u56fe\u4ece\u5de6\u81f3\u53f3\u8868\u793a\u7ee7\u627f\u5173\u7cfb\uff0c\u4e0a\u56fe\u4e0d\u662f\u4e00\u4e2atree\uff0c\u800c\u662fgraph\u3002\u4f46\u662f\u4e0a\u56fe\u662f\u6ee1\u8db3hierarchy\u7ed3\u679c\u7684\u3002 \u5982\u4e0b\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u7684\u7b14\u8bb0\u3002 A hierarchy (from the Greek hierarkhia , \"rule of a high priest\", from hierarkhes , \"president of sacred rites\") is an arrangement of items (objects, names, values, categories, etc.) in which the items are represented as being \"above\", \"below\", or \"at the same level as\" one another. A hierarchy can link entities either directly or indirectly, and either vertically or diagonally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior\uff08\u4e0a\u7ea7\uff09 or to one of one's subordinates\uff08\u4e0b\u5c5e\uff09, although a system that is largely hierarchical can also incorporate alternative hierarchies. Hierarchical links can extend \"vertically\" upwards or downwards via multiple links in the same direction, following a path . All parts of the hierarchy that are not linked vertically to one another nevertheless can be \"horizontally\" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues \uff08\u540c\u4e00\u5c42\uff09; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy is one such form. \u201chierarchy\u201d\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u5c42\u7ea7\u201d\uff0c\u6211\u4eec\u5e73\u65f6\u5e38\u5e38\u6240\u8bf4\u7684\u201c\u7b49\u7ea7\u201d\u4e0e\u5b83\u7684\u542b\u4e49\u7c7b\u4f3c\u3002Hierarchy\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662flevel\u4ee5\u53calevel\u4e4b\u95f4\u7684\u5173\u7cfb\uff08above-below\u5173\u7cfb\u6216superior-subordinates\u5173\u7cfb\uff09\u3002\u53ef\u4ee5\u5c06Hierarchy\u770b\u505a\u662f\u4e00\u79cd\u7ed3\u6784\uff08\u5143\u7d20\u548c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff09\uff0cHierarchy\u7684\u5173\u7cfb\u4e3aabove-below\u5173\u7cfb\u3001superior-subordinates\u5173\u7cfb\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528computer science \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8egraph structure\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528math \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8e partially ordered set \uff0c\u53c2\u89c1\u539f\u6587 Mathematical representation \u7ae0\u8282\u3002 Hierachy\u7ed3\u6784\u5e76\u6ca1\u6709\u9650\u5236\u4e24\u5c42\u5143\u7d20\u4e4b\u95f4\u7684link\uff0c\u6309\u7167\u4e0a\u8ff0\u7684\u63cf\u8ff0\uff0c\u5b83\u662f\u5141\u8bb8\u67d0\u4e00\u5c42\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20\u540c\u65f6\u6709\u4e24\u4e2asuperior\uff0c\u8fd9\u79cd\u60c5\u51b5\u5c31\u662f\u672c\u6587\u5f00\u5934\u6240\u5217\u4e3e\u7684\u591a\u7ee7\u627f\u3002 \u601d\u8003\uff1a\u65e2\u7136hierarchy\u662f\u4e00\u79cd\u7ed3\u6784\uff0c\u90a3\u4e48\u6309\u7167\u54ea\u79cdrelation\u6765\u7ec4\u7ec7\u5143\u7d20\u65e0\u6cd5\u5f62\u6210hierarchy\uff1f \u8fd9\u4e2a\u95ee\u9898\u539f\u6587\u4e2d\u5e76\u6ca1\u6709\u7ed9\u51fa\u4e25\u683c\u7684\u8bf4\u660e\uff0c\u5728\u539f\u6587\u7684 Informal representation \u4e2d\u6709\u975e\u4e25\u683c\u7684\u63cf\u8ff0\u3002","title":"Hierarchy"},{"location":"Structure/Hierarchy/#nomenclature","text":"\u7a0d\u5fae\u6d4f\u89c8\u4e86\u4e00\u4e0b\uff0c\u539f\u6587\u8fd9\u4e00\u6bb5\u4e2d\u7ed9\u51fa\u7684\u4e00\u4e9b\u672f\u8bed\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\u3002","title":"Nomenclature \u547d\u540d\u6cd5"},{"location":"Structure/Hierarchy/#informal_representation","text":"In plain English, a hierarchy can be thought of as a set in which: No element is superior to itself, and One element, the hierarch \uff08\u6559\u4e3b\uff09, is superior to all of the other elements in the set. The first requirement is also interpreted to mean that a hierarchy can have no circular relationships ; the association between two objects is always transitive . The second requirement asserts that a hierarchy must have a leader or root that is common to all of the objects. \u5173\u4e8e\u7b2c\u4e00\u70b9\uff0c\u539f\u6587\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u201ca hierarchy can have no circular relationships \u201d\uff0c\u201c circular relationships \"\u6307\u5411\u7684\u662f\u56fe\u8bba\u4e2d\u7684 circle \uff0c\u4f5c\u8005\u6240\u60f3\u8981\u8868\u8fbe\u7684\u662f\uff0c\u4e00\u4e2ahierarchy\u662f\u4e0d\u80fd\u591f\u5b58\u5728\u73af\u7684\uff0c\u5426\u5219\u65e0\u6cd5\u4e25\u683c\u5730\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002\u5c31\u7b2c\u4e00\u70b9\u7684\u539f\u8bdd\uff0c\u6211\u89c9\u5f97\u4f7f\u7528relation\u7684\u7406\u8bba\u6765\u7406\u89e3\u7684\u8bdd\uff0c\u5b83\u8868\u793a\u7684\u662f\u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u591f\u662f\u4e00\u4e2a reflexive relation \uff0c\u5426\u5219\u5c31\u4f1a\u51fa\u73b0\u73af\u800c\u65e0\u6cd5\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002","title":"Informal representation"},{"location":"Structure/Hierarchy/#mathematical_representation","text":"Mathematically, in its most general form, a hierarchy is a partially ordered set or poset .","title":"Mathematical representation"},{"location":"Structure/Hierarchy/#subtypes","text":"\u201csubtype\u201d\u5373\u5b50\u7c7b\uff0c\u6240\u4ee5\u539f\u6587\u7684\u8fd9\u4e00\u8282\u6240\u63cf\u8ff0\u7684\u662f\u7279\u6b8a\u7c7b\u578b\u7684hierarchy\u3002","title":"Subtypes"},{"location":"Structure/Hierarchy/#nested_hierarchy","text":"A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . The concept of nesting is exemplified in Russian matryoshka dolls \uff08\u4fc4\u7f57\u65af\u5957\u5a03\uff09. Each doll is encompassed by another doll, all the way to the outer doll. The outer doll holds all of the inner dolls, the next outer doll holds all the remaining inner dolls, and so on. Matryoshkas represent a nested hierarchy where each level contains only one object, i.e., there is only one of each size of doll; a generalized nested hierarchy allows for multiple objects within levels but with each object having only one parent at each level . Nested hierarchies are the organizational schemes behind taxonomies \uff08\u5206\u7c7b\u5b66\uff09and systematic classifications. In many programming taxonomies and syntax models (as well as fractals in mathematics), nested hierarchies, including Russian dolls, are also used to illustrate the properties of self-similarity and recursion . Recursion itself is included as a subset of hierarchical programming, and recursive thinking can be synonymous with a form of hierarchical thinking and logic. Nested hierarchy structure\u5728hierarchy structure\u4e0a\u6dfb\u52a0\u7684\u9650\u5236\u662fnested \uff0c\u5b83\u4fdd\u8bc1\u4e86\u201c each object having only one parent at each level \"\uff0c\u663e\u7136\uff0c\u8fd9\u4e2a\u9650\u5236\u5c31\u662fgraph\u4e2d\u4e0d\u518d\u53ef\u80fd\u4ea7\u751f\u73af\u4e86\uff0c\u4e00\u4e2a\u4e0d\u5e26\u73af\u7684\u56fe\u5c31\u662ftree\uff0c\u6240\u4ee5nested hierarchy structure\u662ftree structure\uff0c\u76f8\u6bd4\u4e8ehierarchy\u800c\u8a00\uff0cnesting\u66f4\u52a0\u80fd\u591f\u4f53\u73b0tree\u7684\u7ed3\u6784\u7279\u5f81\u3002 nested hierarchy structure\u662f\u4e00\u79cd\u7279\u6b8a\u7684hierarchy structure\uff0ctree\u662f\u4e00\u79cd\u7279\u6b8a\u7684graph\u3002 \u601d\u8003\uff1a\u600e\u6837\u7684\u5173\u7cfb\u624d\u80fd\u591f\u4ea7\u751fnested hierarchy structure\uff1f\u8fd9\u4e2a\u95ee\u9898\u5728 Tree-structure \u4e2d\u4f1a\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba\u3002 \u539f\u6587\u8fd9\u4e00\u8282\u540e\u9762\u7684\u5185\u5bb9pass\u6389\u4e86\u3002","title":"Nested hierarchy"},{"location":"Structure/Hierarchy/#hierarchy_and_data_structure","text":"Hierarchy structure\u53ef\u4ee5\u4f7f\u7528graph\u6765\u8fdb\u884c\u8868\u793a\u3002 Nested hierarchy structure\u53ef\u4ee5\u662f\u8981\u5f04tree\u6765\u8fdb\u884c\u8868\u793a\u4f60\u3002","title":"Hierarchy and data structure"},{"location":"Structure/Hierarchy/#hierarchy_structure_and_recursion","text":"\u5177\u5907 hierarchical \u7ed3\u6784\u7684\u5f80\u5f80\u5177\u5907recursive\u7684\u7279\u5f81\u3002 \u4ece\u6570\u5b66\u7684\u89d2\u5ea6\u6765\u770b\uff0c Hierarchy structure\u662f\u4e00\u4e2a partially ordered set \uff0c\u6240\u4ee5\u8ba8\u8bbahierarchy structure\u7684\u9012\u5f52\u6027\u6700\u7ec8\u5e94\u8be5\u8fd8\u662f\u5f52\u4e8e\u8ba8\u8bbaposet\u7684\u9012\u5f52\u6027\u3002\u6211\u89c9\u5f97\u662f\u53ef\u4ee5\u4f7f\u7528tree\u7684\u9012\u5f52\u6027\u6765\u601d\u8003poset\u7684\u9012\u5f52\u6027\u7684\uff0c\u4e00\u4e2aposet\u7684subset\u4e5f\u5e94\u8be5\u662f\u4e00\u4e2aposet\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fposet\u5404\u5c42\u4f7f\u7528\u7684\u662f\u540c\u4e00\u4e2arelation\u3002","title":"Hierarchy structure and recursion"},{"location":"Structure/Relation/","text":"Relation # \u4e86\u89e3\u4e00\u4e9b\u5173\u4e8erelation\u7684\u7406\u8bba\uff0c\u5bf9\u4e8e\u7406\u89e3structure\u5927\u6709\u88e8\u76ca\u3002 \u201crelation\u201d\u5373\u5173\u7cfb\uff0c\u8fd9\u4e2a\u8bcd\u662f\u6211\u4eec\u7ecf\u5e38\u8bf4\u8d77\u7684\uff0c\u5728\u6570\u5b66\u4e2d\uff0cRelation\u662f\u4e13\u95e8\u7814\u7a76\u5b83\u7684\u4e00\u4e2a\u6570\u5b66\u5206\u652f\u3002 \u672c\u6587\u5feb\u901f\u68b3\u7406Relation\u76f8\u5173\u77e5\u8bc6\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Finitary relation \u3002\u53e6\u5916\u5728\u7ecf\u5178\u6559\u6750 Discrete Mathematics and Its Applications \u4e2d\u4e5f\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u8ba8\u8bbaRelation\u3002 \u73b0\u4ee3\u6570\u5b66\u7684\u5f88\u591a\u6982\u5ff5\u90fd\u662f\u5efa\u7acb\u5728 Set \u7684\u57fa\u7840\u4e4b\u4e0a\uff0c\u6bd4\u5982\u6211\u4eec\u719f\u77e5\u7684 Function \u6982\u5ff5\uff0cRelation\u4e5f\u662f\u5982\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u4e2d\u4ecb\u7ecdRelation\u7684\u662f Finitary relation \u3002 Finitary relation # Finitary relation \u7684\u542b\u4e49\u662f\u201c\u6709\u9650\u5143\u5173\u7cfb\u201d\uff0c\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u7684\u662f Binary relation \uff08\u4e8c\u5143\u5173\u7cfb\uff09\u3002 Binary relation # In mathematics , a binary relation over two sets A and B is a set of ordered pairs ( a , b ), consisting of elements a of A and elements b of B . That is, it is a subset of the Cartesian product A \u00d7 B . It encodes the information of relation: an element a is related to an element b , if and only if the pair ( a , b ) belongs to the set. relation\u7684\u672c\u8d28\u4e0a\u662fset\u3002 \u67e5\u770b Binary relation \u548c Function \u53ef\u77e5\uff0c Binary relation \u662f\u4e00\u4e2a\u6bd4 Function \u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\uff1a Function \u662f\u4e00\u79cd Binary relation \uff0c\u4f46\u662f\u53cd\u4e4b\u5219\u4e0d\u4e00\u5b9a\u6210\u7acb\u3002 \u5173\u7cfb\u7684\u6027\u8d28 # Transitive relation # \"transitive\"\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u4f20\u9012\u6027\u201d Transitivity is a key property of both partial orders and equivalence relations . Reflexive relation # \u201creflexive\u201d\u5373\u201c\u53cd\u5c04\u6027\u201d\u3002 Symmetric relation # \u201csymmetric\u201d\u5373\u201c\u5bf9\u79f0\u201d relation and order # \u6309\u7167relation\u6765\u7ec4\u7ec7set\u4e2d\u7684\u5143\u7d20\u3002 Partially ordered set # Total order # Preorder # Relational algebra #","title":"Relation"},{"location":"Structure/Relation/#relation","text":"\u4e86\u89e3\u4e00\u4e9b\u5173\u4e8erelation\u7684\u7406\u8bba\uff0c\u5bf9\u4e8e\u7406\u89e3structure\u5927\u6709\u88e8\u76ca\u3002 \u201crelation\u201d\u5373\u5173\u7cfb\uff0c\u8fd9\u4e2a\u8bcd\u662f\u6211\u4eec\u7ecf\u5e38\u8bf4\u8d77\u7684\uff0c\u5728\u6570\u5b66\u4e2d\uff0cRelation\u662f\u4e13\u95e8\u7814\u7a76\u5b83\u7684\u4e00\u4e2a\u6570\u5b66\u5206\u652f\u3002 \u672c\u6587\u5feb\u901f\u68b3\u7406Relation\u76f8\u5173\u77e5\u8bc6\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Finitary relation \u3002\u53e6\u5916\u5728\u7ecf\u5178\u6559\u6750 Discrete Mathematics and Its Applications \u4e2d\u4e5f\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u8ba8\u8bbaRelation\u3002 \u73b0\u4ee3\u6570\u5b66\u7684\u5f88\u591a\u6982\u5ff5\u90fd\u662f\u5efa\u7acb\u5728 Set \u7684\u57fa\u7840\u4e4b\u4e0a\uff0c\u6bd4\u5982\u6211\u4eec\u719f\u77e5\u7684 Function \u6982\u5ff5\uff0cRelation\u4e5f\u662f\u5982\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u4e2d\u4ecb\u7ecdRelation\u7684\u662f Finitary relation \u3002","title":"Relation"},{"location":"Structure/Relation/#finitary_relation","text":"Finitary relation \u7684\u542b\u4e49\u662f\u201c\u6709\u9650\u5143\u5173\u7cfb\u201d\uff0c\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u7684\u662f Binary relation \uff08\u4e8c\u5143\u5173\u7cfb\uff09\u3002","title":"Finitary relation"},{"location":"Structure/Relation/#binary_relation","text":"In mathematics , a binary relation over two sets A and B is a set of ordered pairs ( a , b ), consisting of elements a of A and elements b of B . That is, it is a subset of the Cartesian product A \u00d7 B . It encodes the information of relation: an element a is related to an element b , if and only if the pair ( a , b ) belongs to the set. relation\u7684\u672c\u8d28\u4e0a\u662fset\u3002 \u67e5\u770b Binary relation \u548c Function \u53ef\u77e5\uff0c Binary relation \u662f\u4e00\u4e2a\u6bd4 Function \u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\uff1a Function \u662f\u4e00\u79cd Binary relation \uff0c\u4f46\u662f\u53cd\u4e4b\u5219\u4e0d\u4e00\u5b9a\u6210\u7acb\u3002","title":"Binary relation"},{"location":"Structure/Relation/#_1","text":"","title":"\u5173\u7cfb\u7684\u6027\u8d28"},{"location":"Structure/Relation/#transitive_relation","text":"\"transitive\"\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u4f20\u9012\u6027\u201d Transitivity is a key property of both partial orders and equivalence relations .","title":"Transitive relation"},{"location":"Structure/Relation/#reflexive_relation","text":"\u201creflexive\u201d\u5373\u201c\u53cd\u5c04\u6027\u201d\u3002","title":"Reflexive relation"},{"location":"Structure/Relation/#symmetric_relation","text":"\u201csymmetric\u201d\u5373\u201c\u5bf9\u79f0\u201d","title":"Symmetric relation"},{"location":"Structure/Relation/#relation_and_order","text":"\u6309\u7167relation\u6765\u7ec4\u7ec7set\u4e2d\u7684\u5143\u7d20\u3002","title":"relation and order"},{"location":"Structure/Relation/#partially_ordered_set","text":"","title":"Partially ordered set"},{"location":"Structure/Relation/#total_order","text":"","title":"Total order"},{"location":"Structure/Relation/#preorder","text":"","title":"Preorder"},{"location":"Structure/Relation/#relational_algebra","text":"","title":"Relational algebra"},{"location":"Structure/Structure/","text":"\u7ed3\u6784 # \u5728computer science\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528data structure\u6765\u5b58\u50a8\u6570\u636e\uff0c\u90a3\u4e48\u662f\u5426\u60f3\u8fc7\u8fd9\u6837\u7684\u95ee\u9898\uff1awhat is structure\uff1f\u6570\u636e\u6709\u54ea\u4e9b\u7ed3\u6784\u5462\uff1f \u4ece\u53d1\u5c55\u5386\u7a0b\u6765\u770b\uff0c\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4eec\u5e94\u8be5\u662f\u53d7\u5230\u4e86\u5b9e\u9645\u751f\u6d3b\u4e2d\u7684\u5404\u79cd\u5404\u6837\u7684\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u7136\u540e\u4f7f\u7528programming language\u6765\u63cf\u8ff0\u8fd9\u4e9b\u7ed3\u6784\uff0c\u7531\u6b64\u800c\u53d1\u5c55\u6210\u4e3a\u4e86\u6211\u4eec\u4eca\u5929\u7684data structure\u5b66\u79d1\uff0c\u7b80\u800c\u8a00\u4e4b\uff1a\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684data structure\u5176\u5b9e\u5c31\u662f\u5bf9\u5404\u79cdstructure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\uff08\u63cf\u8ff0\uff09\u3002 \u663e\u7136\uff0c\u5bf9\u7ed3\u6784\u7684\u7814\u7a76\u6709\u52a9\u4e8e\u6211\u4eec\u9009\u62e9\u5408\u9002\u7684data structure\u6765\u4fdd\u5b58\u6211\u4eec\u7684\u6570\u636e\uff0c\u5e76\u4e14\u5f53\u6211\u4eec\u77e5\u9053\u4e8b\u7269\u7ed3\u6784\u540e\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4f7f\u7528\u8ba1\u7b97\u673a\u6765\u5904\u7406\u5b83\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7ed3\u6784\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u6211\u4eec\u8089\u773c\u53ef\u89c1\u7684\u7269\u7406\u7ed3\u6784\uff0c\u5b83\u8fd8\u5305\u62ec\u6211\u4eec\u8089\u773c\u65e0\u6cd5\u770b\u5230\u7684\u903b\u8f91\uff08\u62bd\u8c61\uff09\u7ed3\u6784\u3002 Structure\u662f\u4e00\u4e2a\u5b8f\u5927\u7684\u8bdd\u9898\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4ec5\u4ec5\u4ece computer science \u7684\u89d2\u5ea6\u6765\u63a2\u8ba8structure. What is structure? # \u4e0b\u9762\u662f\u7ef4\u57fa\u767e\u79d1 Structure \u4e2d\u7ed9\u51fa\u7684\u5b9a\u4e49\uff1a A structure is an arrangement and organization of interrelated\uff08\u76f8\u4e92\u5173\u8054\uff09 elements in a material object or system , or the object or system so organized. Material structures include man-made objects such as buildings and machines and natural objects such as biological organisms , minerals and chemicals . Abstract structures include data structures in computer science and musical form . \u4e0a\u8ff0\u5b9a\u4e49\u662fgeneral\u7684\uff0c\u4e0b\u9762\u770b\u770b\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u6700\u6700\u201d\u4eb2\u5bc6\u201c\u7684\u6570\u5b66\u4e2d\u5bf9structure\u7684\u5b9a\u4e49\uff1a Mathematical structure In mathematics , a structure is a set endowed with some additional features on the set (e.g., operation , relation , metric , topology ). Often, the additional features are attached or related to the set, so as to provide it with some additional meaning or significance. Structure (mathematical logic) \uff1a In universal algebra and in model theory , a structure consists of a set along with a collection of finitary operations and relations that are defined on it. \u6570\u5b66\u4e2d\u7684\u5b9a\u4e49\u6240\u4f7f\u7528\u7684\u662f \u6570\u5b66\u8bed\u8a00 \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201celements\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f set \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201cinterrelated\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f finitary operations and relations \u3002 \u9605\u8bfb\u4e86\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u76f4\u89c2\u611f\u53d7\u5c31\u662f\uff1a\u7ed3\u6784\u4e0d\u4ec5\u4ec5\u5173\u4e4e\u5143\u7d20\uff0c\u800c\u4e14\u5173\u4e4e\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u76f8\u540c\u7684\u5143\u7d20\uff0c\u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7684\u5173\u7cfb\u6765\u8fdb\u884c\u7ec4\u7ec7\u7684\u8bdd\uff0c\u6240\u5448\u73b0\u51fa\u7684\u7ed3\u6784\u662f\u4e0d\u540c\u7684\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u5728\u7814\u7a76\u7ed3\u6784\u7684\u65f6\u5019\uff0c\u5207\u83ab\u5ffd\u89c6\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 NOTE: \u5173\u4e8e\u201c\u5173\u7cfb\u201d\uff0c\u53c2\u89c1 Relation \u3002 \u901a\u8fc7\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u6765\u56de\u7b54\u672c\u8282\u6807\u9898\u4e2d\u7684\u95ee\u9898\uff1a\u7ed3\u6784\u662f\u6211\u4eec\u6309\u7167\u67d0\u79cd\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\u5f62\u6210\u7684\uff0c\u5b83\u53ef\u80fd\u662f\u6709\u5f62\u7684\uff08\u7269\u7406\u7684\uff09\uff0c\u4e5f\u53ef\u80fd\u662f\u662f\u62bd\u8c61\u7684\uff08\u903b\u8f91\u7684\uff09\u3002 \u201c\u7ed3\u6784\u201d\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\u672f\u8bed\uff0c\u540e\u9762\u5f53\u6211\u4eec\u60f3\u8981\u8868\u8fbe\u7684\u662f\u7ed3\u6784\u7684\u5f62\u6001\uff08\u8089\u773c\u53ef\u89c1\u7684\uff09\u65f6\uff0c\u6211\u4eec\u4f7f\u7528\u201c\u5f62\u72b6\u201d\u8fd9\u4e2a\u8bcd\u3002 \u7ed3\u6784\u7684\u5f62\u72b6 # \u5f53\u6211\u4eec\u6309\u7167\u4e00\u5b9a\u7684\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\uff0c\u5982\u679c\u80fd\u591f\u5c06\u5b83\u4eec\u7ed9\u201c\u753b\u201d\u51fa\u6765\uff0c\u53ef\u80fd\u4f1a\u5448\u73b0\u4e00\u5b9a\u7684\u201c\u5f62\u72b6\u201d\uff0c\u6b64\u5904\u4f7f\u7528\u201c\u5f62\u72b6\u201d\u8fd9\u4e2a\u8bcd\uff0c\u662f\u4e3a\u4e86\u4e0e\"\u7ed3\u6784\"\"\u8fd9\u4e2a\u8bcd\u6709\u6240\u533a\u5206\uff0c\u5b83\u5f3a\u8c03\u7684\u662f\uff0c\u6211\u4eec\u8089\u773c\u53ef\u4ee5\u770b\u5230\u7684\u5f62\u6001\uff0c\u4f46\u662f\uff0c\u5e73\u65f6\uff0c\u6211\u4eec\u66f4\u591a\u7684\u8fd8\u662f\u4f7f\u7528\u7ed3\u6784\u8fd9\u4e2a\u8bcd\u3002 \u6bd4\u8f83\u5e38\u89c1\u6709\uff1a Chain \uff0c\u7ebf\u6027\u7684 Hierarchy \u3001 tree structure \uff0c\u975e\u7ebf\u6027\u7684\uff0c\u5448\u73b0\u51fa\u5c42\u6b21\u7684\u7ed3\u6784 Network \uff0c graph \uff0c\u975e\u7ebf\u6027\u7684 Lattice In computer science , tree (data structure) is often used to describe hierarchy structure, graph(data structure) is often used to describe network structure. The two kind of structures is very importan in computer science. Structure and relation # \u901a\u8fc7\u524d\u9762\u7684\u8bba\u8ff0\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\uff1a\u7ed3\u6784\u80fd\u591f\u8868\u8fbe\u5143\u7d20\u95f4\u7684\u5173\u7cfb\uff0c\u6216\u8005\u8bf4\uff0c\u6211\u4eec\u6309\u7167\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u6700\u7ec8\u53ef\u4ee5\u5f62\u6210\u67d0\u79cd\u7279\u5b9a\u7684\u5f62\u72b6\uff0c\u6bd4\u5982\uff1a \u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\uff0c\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811 \u6309\u7167inheritance\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u56fe \u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u6811\u7ed3\u6784\uff08\u4e0d\u4f1a\u6210\u73af\uff09\uff0c\u6bd4\u5982parent-children\u5173\u7cfb\uff1b\u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u56fe\uff08\u4f1a\u6210\u73af\uff09\u3002\u90a3\u662f\u5173\u7cfb\u7684\u4ec0\u4e48\u7279\u6548\u51b3\u5b9a\u4e86\u8fd9\u79cd\u7ed3\u679c\u5462\uff1f\u8981\u641e\u6e05\u695a\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5bf9\u201c\u5173\u7cfb\u201d\u7406\u8bba\u6709\u4e00\u5b9a\u7684\u4e86\u89e3\uff08\u53c2\u89c1 Relation \uff09\uff0c\u8fd9\u4e2a\u95ee\u9898\u662f\u672c\u8282\u9700\u8981\u8fdb\u884c\u63a2\u8ba8\u7684\u4e00\u4e2a\u8bdd\u9898\u3002 \u5982\u4f55\u6765\u63cf\u8ff0\u5173\u7cfb\u5462\uff1f\u4e0d\u540c\u7684\u5b66\u79d1\u6709\u4e0d\u540c\u7684\u8bed\u8a00\uff0c\u6bd4\u5982\u6570\u5b66\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528\u51fd\u6570\u6765\u63cf\u8ff0\u5173\u7cfb\uff0c\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5462\uff1f\u53c2\u89c1 Structure in computer science \u3002 \u8fc7\u7a0b\u7684\u7ed3\u6784 # \u4e00\u4e9b\u52a8\u6001\u8fc7\u7a0b\uff0c\u6bd4\u5982\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\u3001\u63a8\u5bfc\u8fc7\u7a0b\u7b49\uff0c\u90fd\u5448\u73b0\u51fa\u4e00\u5b9a\u7684\u7ed3\u6784\uff0c\u672c\u8282\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff0c\u663e\u7136\u8fd9\u79cd\u7ed3\u6784\u5c31\u662f\u524d\u9762\u63d0\u5230\u7684\u903b\u8f91\u7ed3\u6784\u3002 Proof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784 # \u5728\u9605\u8bfb Proof theory \u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\uff1a Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees , which are constructed according to the axioms and rules of inference of the logical system. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\uff0c\u5982\u679c\u5c06\u63a8\u5bfc\u7684\u8fc7\u7a0b\u53ef\u4ee5\u5c55\u793a\u4e3a\u4e00\u79cd\u6570\u636e\u7ed3\u6784\uff0c\u6bd4\u5982\u5217\u8868\u3001\u6811\u3002 Parse tree \u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff0c\u5728\u81ea\u9876\u5411\u4e0b parsing \u7684\u8fc7\u7a0b\u4e2d\uff0cparser\u4e0d\u65ad\u5730\u4f7f\u7528production\u8fdb\u884c\u63a8\u5bfc\uff08expand\uff09\uff0c\u6700\u7ec8\u751f\u6210\u4e86\u4e00\u68f5parse tree\u3002 Parsing\u8fc7\u7a0b\u4ea7\u751f Parse tree # \u5728\u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282\u7684 Summary-of-theory \u6587\u7ae0\u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\u751f\u6210parse tree\u7684\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u8fdb\u884cProof\uff0c\u6240\u4ee5\u5c06\u672c\u8282\u7f6e\u4e8e\u201cProof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784\u201d\u4e2d\u3002 \u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u5448\u73b0tree\u7ed3\u6784 # \u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002 \u5404\u4e2a\u5b66\u79d1\u4e2d\u7684Structure # \u524d\u9762\u6211\u4eec\u63a5\u89e6\u4e86Structure in math\uff0c\u4e0b\u9762\u770b\u770b\u5728\u5176\u4ed6\u5b66\u79d1\u4e2d\u5173\u4e8eStructure\u7684\u7814\u7a76\u5185\u5bb9\u3002 Structure in computer science # Data structure # \u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684data structure\u5b66\u79d1\u548c\u524d\u9762\u6240\u8ff0Structure\u662f\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u5728\u672c\u6587\u7684\u5f00\u5934\u5df2\u7ecf\u5bf9\u4e24\u8005\u8fdb\u884c\u8bf4\u660e\uff1a\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684data structure\u5176\u5b9e\u5c31\u662f\u5bf9\u5404\u79cdstructure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\uff08\u63cf\u8ff0\uff09\u3002 \u5b9e\u73b0\u4e2d\u7684\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u662f\uff1a\u5982\u4f55\u6765\u5b9e\u73b0\uff08\u63cf\u8ff0\uff09\u7ed3\u6784\u7684\u5173\u7cfb\uff1f\u4e00\u822c\u4f7f\u7528link\u6216\u53d8\u6765\u8868\u793a\u5173\u7cfb\uff0c\u6bd4\u5982\u5728tree\u3001graph\u4e2d\u3002 Software architecture # \u7565 Linguistics # \u5728\u8bed\u8a00\u5b66\u4e2d\u4f7f\u7528 Grammar \u3001 Syntax \u6765\u8868\u793a\u8bed\u8a00\u7684\u7ed3\u6784\uff0c\u6700\u6700\u5178\u578b\u7684\u5c31\u662f Phrase structure grammar \u3002","title":"Structure"},{"location":"Structure/Structure/#_1","text":"\u5728computer science\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528data structure\u6765\u5b58\u50a8\u6570\u636e\uff0c\u90a3\u4e48\u662f\u5426\u60f3\u8fc7\u8fd9\u6837\u7684\u95ee\u9898\uff1awhat is structure\uff1f\u6570\u636e\u6709\u54ea\u4e9b\u7ed3\u6784\u5462\uff1f \u4ece\u53d1\u5c55\u5386\u7a0b\u6765\u770b\uff0c\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4eec\u5e94\u8be5\u662f\u53d7\u5230\u4e86\u5b9e\u9645\u751f\u6d3b\u4e2d\u7684\u5404\u79cd\u5404\u6837\u7684\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u7136\u540e\u4f7f\u7528programming language\u6765\u63cf\u8ff0\u8fd9\u4e9b\u7ed3\u6784\uff0c\u7531\u6b64\u800c\u53d1\u5c55\u6210\u4e3a\u4e86\u6211\u4eec\u4eca\u5929\u7684data structure\u5b66\u79d1\uff0c\u7b80\u800c\u8a00\u4e4b\uff1a\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684data structure\u5176\u5b9e\u5c31\u662f\u5bf9\u5404\u79cdstructure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\uff08\u63cf\u8ff0\uff09\u3002 \u663e\u7136\uff0c\u5bf9\u7ed3\u6784\u7684\u7814\u7a76\u6709\u52a9\u4e8e\u6211\u4eec\u9009\u62e9\u5408\u9002\u7684data structure\u6765\u4fdd\u5b58\u6211\u4eec\u7684\u6570\u636e\uff0c\u5e76\u4e14\u5f53\u6211\u4eec\u77e5\u9053\u4e8b\u7269\u7ed3\u6784\u540e\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4f7f\u7528\u8ba1\u7b97\u673a\u6765\u5904\u7406\u5b83\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7ed3\u6784\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u6211\u4eec\u8089\u773c\u53ef\u89c1\u7684\u7269\u7406\u7ed3\u6784\uff0c\u5b83\u8fd8\u5305\u62ec\u6211\u4eec\u8089\u773c\u65e0\u6cd5\u770b\u5230\u7684\u903b\u8f91\uff08\u62bd\u8c61\uff09\u7ed3\u6784\u3002 Structure\u662f\u4e00\u4e2a\u5b8f\u5927\u7684\u8bdd\u9898\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4ec5\u4ec5\u4ece computer science \u7684\u89d2\u5ea6\u6765\u63a2\u8ba8structure.","title":"\u7ed3\u6784"},{"location":"Structure/Structure/#what_is_structure","text":"\u4e0b\u9762\u662f\u7ef4\u57fa\u767e\u79d1 Structure \u4e2d\u7ed9\u51fa\u7684\u5b9a\u4e49\uff1a A structure is an arrangement and organization of interrelated\uff08\u76f8\u4e92\u5173\u8054\uff09 elements in a material object or system , or the object or system so organized. Material structures include man-made objects such as buildings and machines and natural objects such as biological organisms , minerals and chemicals . Abstract structures include data structures in computer science and musical form . \u4e0a\u8ff0\u5b9a\u4e49\u662fgeneral\u7684\uff0c\u4e0b\u9762\u770b\u770b\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u6700\u6700\u201d\u4eb2\u5bc6\u201c\u7684\u6570\u5b66\u4e2d\u5bf9structure\u7684\u5b9a\u4e49\uff1a Mathematical structure In mathematics , a structure is a set endowed with some additional features on the set (e.g., operation , relation , metric , topology ). Often, the additional features are attached or related to the set, so as to provide it with some additional meaning or significance. Structure (mathematical logic) \uff1a In universal algebra and in model theory , a structure consists of a set along with a collection of finitary operations and relations that are defined on it. \u6570\u5b66\u4e2d\u7684\u5b9a\u4e49\u6240\u4f7f\u7528\u7684\u662f \u6570\u5b66\u8bed\u8a00 \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201celements\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f set \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201cinterrelated\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f finitary operations and relations \u3002 \u9605\u8bfb\u4e86\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u76f4\u89c2\u611f\u53d7\u5c31\u662f\uff1a\u7ed3\u6784\u4e0d\u4ec5\u4ec5\u5173\u4e4e\u5143\u7d20\uff0c\u800c\u4e14\u5173\u4e4e\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u76f8\u540c\u7684\u5143\u7d20\uff0c\u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7684\u5173\u7cfb\u6765\u8fdb\u884c\u7ec4\u7ec7\u7684\u8bdd\uff0c\u6240\u5448\u73b0\u51fa\u7684\u7ed3\u6784\u662f\u4e0d\u540c\u7684\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u5728\u7814\u7a76\u7ed3\u6784\u7684\u65f6\u5019\uff0c\u5207\u83ab\u5ffd\u89c6\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 NOTE: \u5173\u4e8e\u201c\u5173\u7cfb\u201d\uff0c\u53c2\u89c1 Relation \u3002 \u901a\u8fc7\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u6765\u56de\u7b54\u672c\u8282\u6807\u9898\u4e2d\u7684\u95ee\u9898\uff1a\u7ed3\u6784\u662f\u6211\u4eec\u6309\u7167\u67d0\u79cd\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\u5f62\u6210\u7684\uff0c\u5b83\u53ef\u80fd\u662f\u6709\u5f62\u7684\uff08\u7269\u7406\u7684\uff09\uff0c\u4e5f\u53ef\u80fd\u662f\u662f\u62bd\u8c61\u7684\uff08\u903b\u8f91\u7684\uff09\u3002 \u201c\u7ed3\u6784\u201d\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\u672f\u8bed\uff0c\u540e\u9762\u5f53\u6211\u4eec\u60f3\u8981\u8868\u8fbe\u7684\u662f\u7ed3\u6784\u7684\u5f62\u6001\uff08\u8089\u773c\u53ef\u89c1\u7684\uff09\u65f6\uff0c\u6211\u4eec\u4f7f\u7528\u201c\u5f62\u72b6\u201d\u8fd9\u4e2a\u8bcd\u3002","title":"What is structure?"},{"location":"Structure/Structure/#_2","text":"\u5f53\u6211\u4eec\u6309\u7167\u4e00\u5b9a\u7684\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\uff0c\u5982\u679c\u80fd\u591f\u5c06\u5b83\u4eec\u7ed9\u201c\u753b\u201d\u51fa\u6765\uff0c\u53ef\u80fd\u4f1a\u5448\u73b0\u4e00\u5b9a\u7684\u201c\u5f62\u72b6\u201d\uff0c\u6b64\u5904\u4f7f\u7528\u201c\u5f62\u72b6\u201d\u8fd9\u4e2a\u8bcd\uff0c\u662f\u4e3a\u4e86\u4e0e\"\u7ed3\u6784\"\"\u8fd9\u4e2a\u8bcd\u6709\u6240\u533a\u5206\uff0c\u5b83\u5f3a\u8c03\u7684\u662f\uff0c\u6211\u4eec\u8089\u773c\u53ef\u4ee5\u770b\u5230\u7684\u5f62\u6001\uff0c\u4f46\u662f\uff0c\u5e73\u65f6\uff0c\u6211\u4eec\u66f4\u591a\u7684\u8fd8\u662f\u4f7f\u7528\u7ed3\u6784\u8fd9\u4e2a\u8bcd\u3002 \u6bd4\u8f83\u5e38\u89c1\u6709\uff1a Chain \uff0c\u7ebf\u6027\u7684 Hierarchy \u3001 tree structure \uff0c\u975e\u7ebf\u6027\u7684\uff0c\u5448\u73b0\u51fa\u5c42\u6b21\u7684\u7ed3\u6784 Network \uff0c graph \uff0c\u975e\u7ebf\u6027\u7684 Lattice In computer science , tree (data structure) is often used to describe hierarchy structure, graph(data structure) is often used to describe network structure. The two kind of structures is very importan in computer science.","title":"\u7ed3\u6784\u7684\u5f62\u72b6"},{"location":"Structure/Structure/#structure_and_relation","text":"\u901a\u8fc7\u524d\u9762\u7684\u8bba\u8ff0\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\uff1a\u7ed3\u6784\u80fd\u591f\u8868\u8fbe\u5143\u7d20\u95f4\u7684\u5173\u7cfb\uff0c\u6216\u8005\u8bf4\uff0c\u6211\u4eec\u6309\u7167\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u6700\u7ec8\u53ef\u4ee5\u5f62\u6210\u67d0\u79cd\u7279\u5b9a\u7684\u5f62\u72b6\uff0c\u6bd4\u5982\uff1a \u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\uff0c\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811 \u6309\u7167inheritance\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u56fe \u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u6811\u7ed3\u6784\uff08\u4e0d\u4f1a\u6210\u73af\uff09\uff0c\u6bd4\u5982parent-children\u5173\u7cfb\uff1b\u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u56fe\uff08\u4f1a\u6210\u73af\uff09\u3002\u90a3\u662f\u5173\u7cfb\u7684\u4ec0\u4e48\u7279\u6548\u51b3\u5b9a\u4e86\u8fd9\u79cd\u7ed3\u679c\u5462\uff1f\u8981\u641e\u6e05\u695a\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5bf9\u201c\u5173\u7cfb\u201d\u7406\u8bba\u6709\u4e00\u5b9a\u7684\u4e86\u89e3\uff08\u53c2\u89c1 Relation \uff09\uff0c\u8fd9\u4e2a\u95ee\u9898\u662f\u672c\u8282\u9700\u8981\u8fdb\u884c\u63a2\u8ba8\u7684\u4e00\u4e2a\u8bdd\u9898\u3002 \u5982\u4f55\u6765\u63cf\u8ff0\u5173\u7cfb\u5462\uff1f\u4e0d\u540c\u7684\u5b66\u79d1\u6709\u4e0d\u540c\u7684\u8bed\u8a00\uff0c\u6bd4\u5982\u6570\u5b66\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528\u51fd\u6570\u6765\u63cf\u8ff0\u5173\u7cfb\uff0c\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5462\uff1f\u53c2\u89c1 Structure in computer science \u3002","title":"Structure and relation"},{"location":"Structure/Structure/#_3","text":"\u4e00\u4e9b\u52a8\u6001\u8fc7\u7a0b\uff0c\u6bd4\u5982\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\u3001\u63a8\u5bfc\u8fc7\u7a0b\u7b49\uff0c\u90fd\u5448\u73b0\u51fa\u4e00\u5b9a\u7684\u7ed3\u6784\uff0c\u672c\u8282\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff0c\u663e\u7136\u8fd9\u79cd\u7ed3\u6784\u5c31\u662f\u524d\u9762\u63d0\u5230\u7684\u903b\u8f91\u7ed3\u6784\u3002","title":"\u8fc7\u7a0b\u7684\u7ed3\u6784"},{"location":"Structure/Structure/#prooflisttree","text":"\u5728\u9605\u8bfb Proof theory \u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\uff1a Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees , which are constructed according to the axioms and rules of inference of the logical system. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\uff0c\u5982\u679c\u5c06\u63a8\u5bfc\u7684\u8fc7\u7a0b\u53ef\u4ee5\u5c55\u793a\u4e3a\u4e00\u79cd\u6570\u636e\u7ed3\u6784\uff0c\u6bd4\u5982\u5217\u8868\u3001\u6811\u3002 Parse tree \u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff0c\u5728\u81ea\u9876\u5411\u4e0b parsing \u7684\u8fc7\u7a0b\u4e2d\uff0cparser\u4e0d\u65ad\u5730\u4f7f\u7528production\u8fdb\u884c\u63a8\u5bfc\uff08expand\uff09\uff0c\u6700\u7ec8\u751f\u6210\u4e86\u4e00\u68f5parse tree\u3002","title":"Proof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784"},{"location":"Structure/Structure/#parsingparse_tree","text":"\u5728\u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282\u7684 Summary-of-theory \u6587\u7ae0\u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\u751f\u6210parse tree\u7684\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u8fdb\u884cProof\uff0c\u6240\u4ee5\u5c06\u672c\u8282\u7f6e\u4e8e\u201cProof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784\u201d\u4e2d\u3002","title":"Parsing\u8fc7\u7a0b\u4ea7\u751fParse tree"},{"location":"Structure/Structure/#tree","text":"\u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u5448\u73b0tree\u7ed3\u6784"},{"location":"Structure/Structure/#structure","text":"\u524d\u9762\u6211\u4eec\u63a5\u89e6\u4e86Structure in math\uff0c\u4e0b\u9762\u770b\u770b\u5728\u5176\u4ed6\u5b66\u79d1\u4e2d\u5173\u4e8eStructure\u7684\u7814\u7a76\u5185\u5bb9\u3002","title":"\u5404\u4e2a\u5b66\u79d1\u4e2d\u7684Structure"},{"location":"Structure/Structure/#structure_in_computer_science","text":"","title":"Structure in computer science"},{"location":"Structure/Structure/#data_structure","text":"\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684data structure\u5b66\u79d1\u548c\u524d\u9762\u6240\u8ff0Structure\u662f\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u5728\u672c\u6587\u7684\u5f00\u5934\u5df2\u7ecf\u5bf9\u4e24\u8005\u8fdb\u884c\u8bf4\u660e\uff1a\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684data structure\u5176\u5b9e\u5c31\u662f\u5bf9\u5404\u79cdstructure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\uff08\u63cf\u8ff0\uff09\u3002 \u5b9e\u73b0\u4e2d\u7684\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u662f\uff1a\u5982\u4f55\u6765\u5b9e\u73b0\uff08\u63cf\u8ff0\uff09\u7ed3\u6784\u7684\u5173\u7cfb\uff1f\u4e00\u822c\u4f7f\u7528link\u6216\u53d8\u6765\u8868\u793a\u5173\u7cfb\uff0c\u6bd4\u5982\u5728tree\u3001graph\u4e2d\u3002","title":"Data structure"},{"location":"Structure/Structure/#software_architecture","text":"\u7565","title":"Software architecture"},{"location":"Structure/Structure/#linguistics","text":"\u5728\u8bed\u8a00\u5b66\u4e2d\u4f7f\u7528 Grammar \u3001 Syntax \u6765\u8868\u793a\u8bed\u8a00\u7684\u7ed3\u6784\uff0c\u6700\u6700\u5178\u578b\u7684\u5c31\u662f Phrase structure grammar \u3002","title":"Linguistics"},{"location":"Summary-of-data-structure/","text":"\u524d\u8a00 # So how to choose data structure ? It is an art and worth learning. \u6700\u540e\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4\u4ee5\u7a81\u51fa\u5176\u7279\u6027\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002 \u6bd4\u8f83\u5404\u79cddata structure\uff0c\u627e\u51fa\u5b83\u4eec\u7684\u5171\u6027\u4e0e\u4e2a\u6027\uff0c\u627e\u51fa\u5b83\u4eec\u7684\u5185\u5728\u5173\u8054 Trade off # \u4e0b\u9762\u8fd9\u6bb5\u8bdd\u6458\u53d6\u81ea\u7ef4\u57fa\u767e\u79d1 Hash function \uff1a In data storage and retrieval applications, use of a hash function is a trade off between search time and data storage space . If search time were unbounded, a very compact unordered linear list would be the best medium; if storage space were unbounded, a randomly accessible structure indexable by the key value would be very large, very sparse, but very fast. A hash function takes a finite amount of time to map a potentially large key space to a feasible amount of storage space searchable in a bounded amount of time regardless of the number of keys. \u5728\u9009\u62e9data structure\u7684\u65f6\u5019\uff0c\u6211\u4eec\u603b\u662f\u5728time\u548cspace\u4e0a\u8fdb\u884c\u6743\u8861\u3002","title":"Introduction"},{"location":"Summary-of-data-structure/#_1","text":"So how to choose data structure ? It is an art and worth learning. \u6700\u540e\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4\u4ee5\u7a81\u51fa\u5176\u7279\u6027\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002 \u6bd4\u8f83\u5404\u79cddata structure\uff0c\u627e\u51fa\u5b83\u4eec\u7684\u5171\u6027\u4e0e\u4e2a\u6027\uff0c\u627e\u51fa\u5b83\u4eec\u7684\u5185\u5728\u5173\u8054","title":"\u524d\u8a00"},{"location":"Summary-of-data-structure/#trade_off","text":"\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u6458\u53d6\u81ea\u7ef4\u57fa\u767e\u79d1 Hash function \uff1a In data storage and retrieval applications, use of a hash function is a trade off between search time and data storage space . If search time were unbounded, a very compact unordered linear list would be the best medium; if storage space were unbounded, a randomly accessible structure indexable by the key value would be very large, very sparse, but very fast. A hash function takes a finite amount of time to map a potentially large key space to a feasible amount of storage space searchable in a bounded amount of time regardless of the number of keys. \u5728\u9009\u62e9data structure\u7684\u65f6\u5019\uff0c\u6211\u4eec\u603b\u662f\u5728time\u548cspace\u4e0a\u8fdb\u884c\u6743\u8861\u3002","title":"Trade off"},{"location":"Summary-of-data-structure/List-of-data-structure/","text":"List of data structures # linear structure VS nonlinear structure # linear nonlinear Arrays \u3001 Lists Trees \u3001 Graphs \u3001 Hash-based structures","title":"List-of-data-structure"},{"location":"Summary-of-data-structure/List-of-data-structure/#list_of_data_structures","text":"","title":"List of data structures"},{"location":"Summary-of-data-structure/List-of-data-structure/#linear_structure_vs_nonlinear_structure","text":"linear nonlinear Arrays \u3001 Lists Trees \u3001 Graphs \u3001 Hash-based structures","title":"linear structure VS nonlinear structure"},{"location":"Summary-of-data-structure/TODO/","text":"","title":"TODO"},{"location":"Summary-of-data-structure/VS-of-graph-and-tree-and-list/","text":"graph\u3001tree\u3001list # \u8fdb\u5316\uff1a\u4ecelist\u5230tree\uff0c\u4ecetree\u5230graph\uff1b \u9000\u5316\uff1agraph\u9000\u5316\u4e3atree\uff0ctree\u9000\u5316\u4e3alist\uff1b \u6811\u548c\u56fe\u90fd\u53ef\u80fd\u9000\u5316\u6210\u94fe\uff0c\u6240\u4ee5\u5176\u5b9e\u94fe\u4e5f\u5177\u5907\u90e8\u5206tree\u548cgraph\u7684\u5173\u7cfb\u3002\u53cd\u8fc7\u6765\u8bf4\u5176\u5b9e\u5c31\u662f\u6811\u548c\u56fe\u662f\u94fe\u7684\u6cdb\u5316\u3002 \u5176\u5b9e\u53ef\u4ee5\u770b\u5230\uff0c\u5f88\u591a\u57fa\u4e8egraph\u3001tree\u7684\u7ed3\u6784\u6216\u8bbe\u8ba1\u90fd\u6709\u5bf9\u5e94\u7684\u7ebf\u6027\u7248\u672c\uff0c\u6bd4\u5982 Merkle tree \uff0c\u5b83\u7684\u7ebf\u6027\u7248\u672c\u5c31\u662f Hash chain \u3002 Difference between Trees and Graphs | Trees vs. Graphs # Trees Graphs Path Tree is special form of graph i.e. minimally connected graph and having only one path between any two vertices. In graph there can be more than one path i.e. graph can have uni-directional or bi-directional paths (edges) between nodes Loops Tree is a special case of graph having no loops , no circuits and no self-loops. Graph can have loops, circuits as well as can have self-loops . Root Node In tree there is exactly one root node and every child have only one parent . In graph there is no such concept of root node. NOTE: In graph, a node can be specified as the root node. Parent Child relationship In trees, there is parent child relationship so flow can be there with direction top to bottom or vice versa. In Graph there is no such parent child relationship. Complexity Trees are less complex then graphs as having no cycles, no self-loops and still connected. Graphs are more complex in compare to trees as it can have cycles, loops etc Types of Traversal Tree traversal is a kind of special case of traversal of graph. Tree is traversed in Pre-Order , In-Order and Post-Order (all three in DFS or in BFS algorithm) Graph is traversed by DFS: Depth First Search and in BFS : Breadth First Search algorithm Connection Rules In trees, there are many rules / restrictions for making connections between nodes through edges. In graphs no such rules/ restrictions are there for connecting the nodes through edges. DAG Trees come in the category of DAG : Directed Acyclic Graphs is a kind of directed graph that have no cycles. Graph can be Cyclic or Acyclic . Different Types Different types of trees are : Binary Tree , Binary Search Tree, AVL tree, Heaps . There are mainly two types of Graphs : Directed and Undirected graphs . Applications Tree applications : sorting and searching like Tree Traversal & Binary Search. Graph applications : Coloring of maps, in OR ( PERT & CPM ), algorithms, Graph coloring, job scheduling, etc. No. of edges Tree always has n-1 edges. In Graph, no. of edges depend on the graph. Model Tree is a hierarchical model . Graph is a network model . Figure \u6811\u4e2d\u6bcf\u4e2a\u8282\u70b9\u90fd\u53ea\u80fd\u6709\u4e00\u4e2a\u7236\u8282\u70b9\uff0c\u56fe\u4e2d\u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\u591a\u4e2a\u7236\u8282\u70b9\u3002 \u79bb\u6563\u6570\u5b66\u4e2d\u5bf9tree\u7684\u5b9a\u4e49\uff1a a tree is a connected undirected graph with no simple circuits \u8fd9\u8574\u542b\u7740 an undirected graph is a tree if and only if there is a unique simple path between any two of its vertices \u5982\u679c\u4e24\u4e2a\u70b9\u4e4b\u95f4\u6709\u591a\u6761path\u7684\u8bdd\uff0c\u5219\u5fc5\u7136\u5c31\u5f62\u6210\u4e86circuit\u4e86","title":"VS-of-graph-and-tree-and-list"},{"location":"Summary-of-data-structure/VS-of-graph-and-tree-and-list/#graphtreelist","text":"\u8fdb\u5316\uff1a\u4ecelist\u5230tree\uff0c\u4ecetree\u5230graph\uff1b \u9000\u5316\uff1agraph\u9000\u5316\u4e3atree\uff0ctree\u9000\u5316\u4e3alist\uff1b \u6811\u548c\u56fe\u90fd\u53ef\u80fd\u9000\u5316\u6210\u94fe\uff0c\u6240\u4ee5\u5176\u5b9e\u94fe\u4e5f\u5177\u5907\u90e8\u5206tree\u548cgraph\u7684\u5173\u7cfb\u3002\u53cd\u8fc7\u6765\u8bf4\u5176\u5b9e\u5c31\u662f\u6811\u548c\u56fe\u662f\u94fe\u7684\u6cdb\u5316\u3002 \u5176\u5b9e\u53ef\u4ee5\u770b\u5230\uff0c\u5f88\u591a\u57fa\u4e8egraph\u3001tree\u7684\u7ed3\u6784\u6216\u8bbe\u8ba1\u90fd\u6709\u5bf9\u5e94\u7684\u7ebf\u6027\u7248\u672c\uff0c\u6bd4\u5982 Merkle tree \uff0c\u5b83\u7684\u7ebf\u6027\u7248\u672c\u5c31\u662f Hash chain \u3002","title":"graph\u3001tree\u3001list"},{"location":"Summary-of-data-structure/VS-of-graph-and-tree-and-list/#difference_between_trees_and_graphs_trees_vs_graphs","text":"Trees Graphs Path Tree is special form of graph i.e. minimally connected graph and having only one path between any two vertices. In graph there can be more than one path i.e. graph can have uni-directional or bi-directional paths (edges) between nodes Loops Tree is a special case of graph having no loops , no circuits and no self-loops. Graph can have loops, circuits as well as can have self-loops . Root Node In tree there is exactly one root node and every child have only one parent . In graph there is no such concept of root node. NOTE: In graph, a node can be specified as the root node. Parent Child relationship In trees, there is parent child relationship so flow can be there with direction top to bottom or vice versa. In Graph there is no such parent child relationship. Complexity Trees are less complex then graphs as having no cycles, no self-loops and still connected. Graphs are more complex in compare to trees as it can have cycles, loops etc Types of Traversal Tree traversal is a kind of special case of traversal of graph. Tree is traversed in Pre-Order , In-Order and Post-Order (all three in DFS or in BFS algorithm) Graph is traversed by DFS: Depth First Search and in BFS : Breadth First Search algorithm Connection Rules In trees, there are many rules / restrictions for making connections between nodes through edges. In graphs no such rules/ restrictions are there for connecting the nodes through edges. DAG Trees come in the category of DAG : Directed Acyclic Graphs is a kind of directed graph that have no cycles. Graph can be Cyclic or Acyclic . Different Types Different types of trees are : Binary Tree , Binary Search Tree, AVL tree, Heaps . There are mainly two types of Graphs : Directed and Undirected graphs . Applications Tree applications : sorting and searching like Tree Traversal & Binary Search. Graph applications : Coloring of maps, in OR ( PERT & CPM ), algorithms, Graph coloring, job scheduling, etc. No. of edges Tree always has n-1 edges. In Graph, no. of edges depend on the graph. Model Tree is a hierarchical model . Graph is a network model . Figure \u6811\u4e2d\u6bcf\u4e2a\u8282\u70b9\u90fd\u53ea\u80fd\u6709\u4e00\u4e2a\u7236\u8282\u70b9\uff0c\u56fe\u4e2d\u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\u591a\u4e2a\u7236\u8282\u70b9\u3002 \u79bb\u6563\u6570\u5b66\u4e2d\u5bf9tree\u7684\u5b9a\u4e49\uff1a a tree is a connected undirected graph with no simple circuits \u8fd9\u8574\u542b\u7740 an undirected graph is a tree if and only if there is a unique simple path between any two of its vertices \u5982\u679c\u4e24\u4e2a\u70b9\u4e4b\u95f4\u6709\u591a\u6761path\u7684\u8bdd\uff0c\u5219\u5fc5\u7136\u5c31\u5f62\u6210\u4e86circuit\u4e86","title":"Difference between Trees and Graphs | Trees vs. Graphs"},{"location":"Summary-of-data-structure/VS-of-implementation-of-ADT/","text":"ADT\u7684\u5b9e\u73b0 Associative array\u7684\u5b9e\u73b0 set\u7684\u5b9e\u73b0 ADT\u7684\u5b9e\u73b0 # Associative array \u7684\u5b9e\u73b0 # The two major solutions to the dictionary problem are a hash table or a search tree .[ 1] [ 2] [ 4] [ 5] b set\u7684\u5b9e\u73b0 # \u4f7f\u7528tree\u6765\u5b9e\u73b0 \u4f7f\u7528skip list\u6765\u5b9e\u73b0","title":"VS-of-implementation-of-ADT"},{"location":"Summary-of-data-structure/VS-of-implementation-of-ADT/#adt","text":"","title":"ADT\u7684\u5b9e\u73b0"},{"location":"Summary-of-data-structure/VS-of-implementation-of-ADT/#associative_array","text":"The two major solutions to the dictionary problem are a hash table or a search tree .[ 1] [ 2] [ 4] [ 5] b","title":"Associative array\u7684\u5b9e\u73b0"},{"location":"Summary-of-data-structure/VS-of-implementation-of-ADT/#set","text":"\u4f7f\u7528tree\u6765\u5b9e\u73b0 \u4f7f\u7528skip list\u6765\u5b9e\u73b0","title":"set\u7684\u5b9e\u73b0"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-hash-table/","text":"binary search tree VS hash table # \u5728 Binary search tree \u4e2d\uff0c\u6709\u5982\u4e0b\u5bf9\u6bd4\uff1a Binary search trees keep their keys in sorted order, so that lookup and other operations can use the principle of binary search : when looking for a key in a tree (or a place to insert a new key), they traverse the tree from root to leaf, making comparisons to keys stored in the nodes of the tree and deciding, on the basis of the comparison, to continue searching in the left or right subtrees. On average, this means that each comparison allows the operations to skip about half of the tree, so that each lookup, insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree. This is much better than the linear time required to find items by key in an (unsorted) array, but slower than the corresponding operations on hash tables . B tree VS hash table # Introduction to the B-Tree # Why is a tree a good data structure for a database? Searching for a particular value is fast (logarithmic time) Inserting / deleting a value you\u2019ve already found is fast (constant-ish time to rebalance) Traversing a range of values is fast (unlike a hash map) Self-balancing binary search tree VS hash table # Self-balancing binary search tree # Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues . They can also be used for associative arrays ; key-value pairs are simply inserted with an ordering based on the key alone. In this capacity, self-balancing BSTs have a number of advantages and disadvantages over their main competitor, hash tables . One advantage of self-balancing BSTs is that they allow fast (indeed, asymptotically optimal) enumeration of the items in key order , which hash tables do not provide. One disadvantage is that their lookup algorithms get more complicated when there may be multiple items with the same key. Self-balancing BSTs have better worst-case lookup performance than hash tables (O(log n) compared to O(n)), but have worse average-case performance (O(log n) compared to O(1)).","title":"VS-of-search-tree-and-hash-table"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-hash-table/#binary_search_tree_vs_hash_table","text":"\u5728 Binary search tree \u4e2d\uff0c\u6709\u5982\u4e0b\u5bf9\u6bd4\uff1a Binary search trees keep their keys in sorted order, so that lookup and other operations can use the principle of binary search : when looking for a key in a tree (or a place to insert a new key), they traverse the tree from root to leaf, making comparisons to keys stored in the nodes of the tree and deciding, on the basis of the comparison, to continue searching in the left or right subtrees. On average, this means that each comparison allows the operations to skip about half of the tree, so that each lookup, insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree. This is much better than the linear time required to find items by key in an (unsorted) array, but slower than the corresponding operations on hash tables .","title":"binary search tree VS hash table"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-hash-table/#b_tree_vs_hash_table","text":"","title":"B tree VS hash table"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-hash-table/#introduction_to_the_b-tree","text":"Why is a tree a good data structure for a database? Searching for a particular value is fast (logarithmic time) Inserting / deleting a value you\u2019ve already found is fast (constant-ish time to rebalance) Traversing a range of values is fast (unlike a hash map)","title":"Introduction to the B-Tree"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-hash-table/#self-balancing_binary_search_tree_vs_hash_table","text":"","title":"Self-balancing binary search tree VS hash table"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-hash-table/#self-balancing_binary_search_tree","text":"Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues . They can also be used for associative arrays ; key-value pairs are simply inserted with an ordering based on the key alone. In this capacity, self-balancing BSTs have a number of advantages and disadvantages over their main competitor, hash tables . One advantage of self-balancing BSTs is that they allow fast (indeed, asymptotically optimal) enumeration of the items in key order , which hash tables do not provide. One disadvantage is that their lookup algorithms get more complicated when there may be multiple items with the same key. Self-balancing BSTs have better worst-case lookup performance than hash tables (O(log n) compared to O(n)), but have worse average-case performance (O(log n) compared to O(1)).","title":"Self-balancing binary search tree"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-skip-list/","text":"Skip List vs. Binary Search Tree A B-tree VS skip list Skip List vs. Binary Search Tree # I recently came across the data structure known as a skip list . It seems to have very similar behavior to a binary search tree. Why would you ever want to use a skip list over a binary search tree? A # Skip lists are more amenable to concurrent access/modification. Herb Sutter wrote an article about data structure in concurrent environments. It has more indepth information. The most frequently used implementation of a binary search tree is a red-black tree . The concurrent problems come in when the tree is modified it often needs to rebalance. The rebalance operation can affect large portions of the tree, which would require a mutex lock on many of the tree nodes. Inserting a node into a skip list is far more localized, only nodes directly linked to the affected node need to be locked. Update from Jon Harrops comments I read Fraser and Harris's latest paper Concurrent programming without locks . Really good stuff if you're interested in lock-free data structures. The paper focuses on Transactional Memory and a theoretical operation multiword-compare-and-swap MCAS. Both of these are simulated in software as no hardware supports them yet. I'm fairly impressed that they were able to build MCAS in software at all. I didn't find the transactional memory stuff particularly compelling as it requires a garbage collector. Also software transactional memory is plagued with performance issues. However, I'd be very excited if hardware transactional memory ever becomes common. In the end it's still research and won't be of use for production code for another decade or so. In section 8.2 they compare the performance of several concurrent tree implementations. I'll summarize their findings. It's worth it to download the pdf as it has some very informative graphs on pages 50, 53, and 54. Locking skip lists are insanely fast. They scale incredibly well with the number of concurrent accesses. This is what makes skip lists special, other lock based data structures tend to croak under pressure. Lock-free skip lists are consistently faster than locking skip lists but only barely. transactional skip lists are consistently 2-3 times slower than the locking and non-locking versions. locking red-black trees croak under concurrent access. Their performance degrades linearly with each new concurrent user. Of the two known locking red-black tree implementations, one essentially has a global lock during tree rebalancing. The other uses fancy (and complicated) lock escalation but still doesn't significantly out perform the global lock version. lock-free red-black trees don't exist (no longer true, see Update). transactional red-black trees are comparable with transactional skip-lists. That was very surprising and very promising. Transactional memory, though slower if far easier to write. It can be as easy as quick search and replace on the non-concurrent version. Update Here is paper about lock-free trees: Lock-Free Red-Black Trees Using CAS . I haven't looked into it deeply, but on the surface it seems solid. COMMENTS : 3 Not to mention that in a non-degenerate skiplist, about 50% of the nodes should only have a single link which makes insert and delete remarkably efficient. \u2013 Adisak Oct 30 '09 at 3:44 2 Rebalancing does not require a mutex lock. See cl.cam.ac.uk/research/srg/netos/lock-free \u2013 Jon Harrop May 20 '10 at 21:00 3 @Jon , yes and no. There are no known lock-free red-black tree implementations. Fraser and Harris show how a transactional memory based red-black tree is implemented and its performance. Transactional memory is still very much in the research arena, so in production code, a red-black tree will still need to lock large portions of the tree. \u2013 deft_code May 21 '10 at 16:20 1 I wanted to update this answer. There are currently two lock based efficient binary search trees. One is based on AVL trees ( dl.acm.org/citation.cfm?id=1693488 ) and the other (Warning! shameless plug) is based on red black trees. See actapress.com/Abstract.aspx?paperId=453069 \u2013 Juan Besa Mar 2 '12 at 20:01 @JuanBesa , \"14% better than the best known concurrent dictionary solutions\" . Are you comparing against skip-lists? The other paper inadvertently points out that lock based trees are O(n) for n < 90, whereas skiplists (also a dictionary) are O(1) ! 14% doesn't seem to be enough when the O is that disparate. \u2013 deft_code Mar 2 '12 at 22:08 That RB tree paper looks bloody good! \u2013 user82238 Jun 19 '12 at 10:22 Think I'm going to try to apply that basic mechanism to AVL. As I see it from a quick read, the basic solution to rotation (which is the fundamental problem) is to have a retry-block which is the raising of flags in the elements you need to control - if you can raise them all, then you're safe to proceed as other threads will fail and be retrying to get those flags. Simple genius! \u2013 user82238 Jun 19 '12 at 10:29 @BlankXavier , Hmmm, that sounds suspiciously like using a spinlock instead of a mutex for a regular lock based tree. It may be more performant, but I want to see some benchmarks. In particular against the a lock-free skiplist and a locking skiplist. \u2013 deft_code Jun 20 '12 at 23:01 All helper mechanisms are essentially spinning mechanisms - it's just that rather than dumbly spinning, which performs no work, by spinning on a helper mechanism which if it completes permits you to continue your own work , then you're doing something useful - you're lock-free, in fact... \u2013 user82238 Jun 21 '12 at 6:53 4 @deft_code : Intel recently announced an implementation of Transactional Memory via TSX on Haswell. This may prove interesting w.r.t those lock free data structures you mentioned. \u2013 Mike Bailey Oct 3 '12 at 5:07 1 Any comment on Respawned Fluff's recent answer ? \u2013 Claudiu Feb 2 '15 at 3:06 2 I think Fizz' answer is more up-to-date (from 2015) rather than this answer (2012) and therefore should probably be the preferred answer by now. \u2013 fnl Jul 11 '17 at 10:45 B-tree VS skip list # \u770b\u4e86\u8fd9\u4e24\u79cdDS\u7684\u539f\u7406\uff0c\u53d1\u73b0\u4e24\u8005\u5176\u5b9e\u6709\u4e9b\u7c7b\u4f3c\uff1a\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5373\u901a\u8fc7\u6784\u5efa\u6570\u636e\u4e4b\u95f4\u7684\u66f4\u591a\u5173\u7cfb\u6765\u52a0\u901f\u6570\u636e\u7684access\uff0c\u663e\u7136\u8fd9\u4e9b\u5173\u7cfb\u662f\u9700\u8981\u8017\u8d39\u7a7a\u95f4\u6765\u5b58\u50a8\u7684\uff0c\u6240\u4ee5\u5c31\u662f\u524d\u9762\u6240\u8ff0\u7684\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5176\u5b9e\u5f53\u6211\u770b\u5b8c\u4e86\u4e24\u8005\u7684\u539f\u7406\u540e\uff0c\u89c9\u5f97\u5b83\u4eec\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\uff0c\u5c24\u5176\u662f\u770b\u5230 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\u540e\uff0c\u6211\u53d1\u6398\u5176\u5b9e\u5b83\u4eec\u7684\u539f\u7406\u7684\u5171\u540c\u4e4b\u5904\u5176\u5b9e\u5c31\u662findex\uff1b\u901a\u8fc7\u91cd\u5efaindex\u6765\u52a0\u901f\u6570\u636e\u7684access\uff1b \u4e8e\u662f\u6211\u5c31\u60f3\uff0c\u65e2\u7136 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\uff0c\u90a3\u4e48 skip list \u662f\u5426\u4e5f\u80fd\u591f\u5462\uff1f \u68c0\u7d22\u4e86\u4e00\u756a\u540e\uff0c\u53d1\u73b0\u5176\u5b9e\u662f\u6709\u5e94\u7528\u6848\u4f8b\u7684\uff1a MemSQL uses skip lists as its prime indexing structure for its database technology. The Story Behind MemSQL\u2019s Skiplist Indexes \u5176\u5b9e\u53d1\u73b0\uff0c\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u591a\u4f7f\u7528B-tree\uff0c\u800c\u5728\u5185\u5b58\u4e2d\u5219\u591a\u4f7f\u7528skip list","title":"VS-of-search-tree-and-skip-list"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-skip-list/#skip_list_vs_binary_search_tree","text":"I recently came across the data structure known as a skip list . It seems to have very similar behavior to a binary search tree. Why would you ever want to use a skip list over a binary search tree?","title":"Skip List vs. Binary Search Tree"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-skip-list/#a","text":"Skip lists are more amenable to concurrent access/modification. Herb Sutter wrote an article about data structure in concurrent environments. It has more indepth information. The most frequently used implementation of a binary search tree is a red-black tree . The concurrent problems come in when the tree is modified it often needs to rebalance. The rebalance operation can affect large portions of the tree, which would require a mutex lock on many of the tree nodes. Inserting a node into a skip list is far more localized, only nodes directly linked to the affected node need to be locked. Update from Jon Harrops comments I read Fraser and Harris's latest paper Concurrent programming without locks . Really good stuff if you're interested in lock-free data structures. The paper focuses on Transactional Memory and a theoretical operation multiword-compare-and-swap MCAS. Both of these are simulated in software as no hardware supports them yet. I'm fairly impressed that they were able to build MCAS in software at all. I didn't find the transactional memory stuff particularly compelling as it requires a garbage collector. Also software transactional memory is plagued with performance issues. However, I'd be very excited if hardware transactional memory ever becomes common. In the end it's still research and won't be of use for production code for another decade or so. In section 8.2 they compare the performance of several concurrent tree implementations. I'll summarize their findings. It's worth it to download the pdf as it has some very informative graphs on pages 50, 53, and 54. Locking skip lists are insanely fast. They scale incredibly well with the number of concurrent accesses. This is what makes skip lists special, other lock based data structures tend to croak under pressure. Lock-free skip lists are consistently faster than locking skip lists but only barely. transactional skip lists are consistently 2-3 times slower than the locking and non-locking versions. locking red-black trees croak under concurrent access. Their performance degrades linearly with each new concurrent user. Of the two known locking red-black tree implementations, one essentially has a global lock during tree rebalancing. The other uses fancy (and complicated) lock escalation but still doesn't significantly out perform the global lock version. lock-free red-black trees don't exist (no longer true, see Update). transactional red-black trees are comparable with transactional skip-lists. That was very surprising and very promising. Transactional memory, though slower if far easier to write. It can be as easy as quick search and replace on the non-concurrent version. Update Here is paper about lock-free trees: Lock-Free Red-Black Trees Using CAS . I haven't looked into it deeply, but on the surface it seems solid. COMMENTS : 3 Not to mention that in a non-degenerate skiplist, about 50% of the nodes should only have a single link which makes insert and delete remarkably efficient. \u2013 Adisak Oct 30 '09 at 3:44 2 Rebalancing does not require a mutex lock. See cl.cam.ac.uk/research/srg/netos/lock-free \u2013 Jon Harrop May 20 '10 at 21:00 3 @Jon , yes and no. There are no known lock-free red-black tree implementations. Fraser and Harris show how a transactional memory based red-black tree is implemented and its performance. Transactional memory is still very much in the research arena, so in production code, a red-black tree will still need to lock large portions of the tree. \u2013 deft_code May 21 '10 at 16:20 1 I wanted to update this answer. There are currently two lock based efficient binary search trees. One is based on AVL trees ( dl.acm.org/citation.cfm?id=1693488 ) and the other (Warning! shameless plug) is based on red black trees. See actapress.com/Abstract.aspx?paperId=453069 \u2013 Juan Besa Mar 2 '12 at 20:01 @JuanBesa , \"14% better than the best known concurrent dictionary solutions\" . Are you comparing against skip-lists? The other paper inadvertently points out that lock based trees are O(n) for n < 90, whereas skiplists (also a dictionary) are O(1) ! 14% doesn't seem to be enough when the O is that disparate. \u2013 deft_code Mar 2 '12 at 22:08 That RB tree paper looks bloody good! \u2013 user82238 Jun 19 '12 at 10:22 Think I'm going to try to apply that basic mechanism to AVL. As I see it from a quick read, the basic solution to rotation (which is the fundamental problem) is to have a retry-block which is the raising of flags in the elements you need to control - if you can raise them all, then you're safe to proceed as other threads will fail and be retrying to get those flags. Simple genius! \u2013 user82238 Jun 19 '12 at 10:29 @BlankXavier , Hmmm, that sounds suspiciously like using a spinlock instead of a mutex for a regular lock based tree. It may be more performant, but I want to see some benchmarks. In particular against the a lock-free skiplist and a locking skiplist. \u2013 deft_code Jun 20 '12 at 23:01 All helper mechanisms are essentially spinning mechanisms - it's just that rather than dumbly spinning, which performs no work, by spinning on a helper mechanism which if it completes permits you to continue your own work , then you're doing something useful - you're lock-free, in fact... \u2013 user82238 Jun 21 '12 at 6:53 4 @deft_code : Intel recently announced an implementation of Transactional Memory via TSX on Haswell. This may prove interesting w.r.t those lock free data structures you mentioned. \u2013 Mike Bailey Oct 3 '12 at 5:07 1 Any comment on Respawned Fluff's recent answer ? \u2013 Claudiu Feb 2 '15 at 3:06 2 I think Fizz' answer is more up-to-date (from 2015) rather than this answer (2012) and therefore should probably be the preferred answer by now. \u2013 fnl Jul 11 '17 at 10:45","title":"A"},{"location":"Summary-of-data-structure/VS-of-search-tree-and-skip-list/#b-tree_vs_skip_list","text":"\u770b\u4e86\u8fd9\u4e24\u79cdDS\u7684\u539f\u7406\uff0c\u53d1\u73b0\u4e24\u8005\u5176\u5b9e\u6709\u4e9b\u7c7b\u4f3c\uff1a\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5373\u901a\u8fc7\u6784\u5efa\u6570\u636e\u4e4b\u95f4\u7684\u66f4\u591a\u5173\u7cfb\u6765\u52a0\u901f\u6570\u636e\u7684access\uff0c\u663e\u7136\u8fd9\u4e9b\u5173\u7cfb\u662f\u9700\u8981\u8017\u8d39\u7a7a\u95f4\u6765\u5b58\u50a8\u7684\uff0c\u6240\u4ee5\u5c31\u662f\u524d\u9762\u6240\u8ff0\u7684\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5176\u5b9e\u5f53\u6211\u770b\u5b8c\u4e86\u4e24\u8005\u7684\u539f\u7406\u540e\uff0c\u89c9\u5f97\u5b83\u4eec\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\uff0c\u5c24\u5176\u662f\u770b\u5230 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\u540e\uff0c\u6211\u53d1\u6398\u5176\u5b9e\u5b83\u4eec\u7684\u539f\u7406\u7684\u5171\u540c\u4e4b\u5904\u5176\u5b9e\u5c31\u662findex\uff1b\u901a\u8fc7\u91cd\u5efaindex\u6765\u52a0\u901f\u6570\u636e\u7684access\uff1b \u4e8e\u662f\u6211\u5c31\u60f3\uff0c\u65e2\u7136 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\uff0c\u90a3\u4e48 skip list \u662f\u5426\u4e5f\u80fd\u591f\u5462\uff1f \u68c0\u7d22\u4e86\u4e00\u756a\u540e\uff0c\u53d1\u73b0\u5176\u5b9e\u662f\u6709\u5e94\u7528\u6848\u4f8b\u7684\uff1a MemSQL uses skip lists as its prime indexing structure for its database technology. The Story Behind MemSQL\u2019s Skiplist Indexes \u5176\u5b9e\u53d1\u73b0\uff0c\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u591a\u4f7f\u7528B-tree\uff0c\u800c\u5728\u5185\u5b58\u4e2d\u5219\u591a\u4f7f\u7528skip list","title":"B-tree VS skip list"},{"location":"Summary-of-data-structure/VS-of-tree/","text":"B-tree VS self-balancing trees # \u5728 Search tree \u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u8bdd\uff1a B-trees are generalizations of binary search trees in that they can have a variable number of subtrees at each node. While child-nodes have a pre-defined range, they will not necessarily be filled with data, meaning B-trees can potentially waste some space. The advantage is that B-trees do not need to be re-balanced as frequently as other self-balancing trees .","title":"VS-of-trees"},{"location":"Summary-of-data-structure/VS-of-tree/#b-tree_vs_self-balancing_trees","text":"\u5728 Search tree \u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u8bdd\uff1a B-trees are generalizations of binary search trees in that they can have a variable number of subtrees at each node. While child-nodes have a pre-defined range, they will not necessarily be filled with data, meaning B-trees can potentially waste some space. The advantage is that B-trees do not need to be re-balanced as frequently as other self-balancing trees .","title":"B-tree VS self-balancing trees"},{"location":"Tree/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u63cf\u8ff0tree\uff0c\u5b83\u5728computer science\u548c\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u4f9d\u7167\u672c\u5de5\u7a0b\u7684\u60ef\u4f8b\uff0c\u4ecestructure\u5165\u624b\uff0c\u5373\u5148\u63cf\u8ff0tree structure\uff08\u6811\u5f62\u7ed3\u6784\uff09\uff0c\u7136\u540e\u63cf\u8ff0tree in data structure\uff08\u6570\u636e\u7ed3\u6784\u4e2d\u7684\u6811\uff09\uff0c\u56e0\u4e3atree in data structure\u672c\u8d28\u4e0a\u662f\u5bf9tree structure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\u3002","title":"Introduction"},{"location":"Tree/#_1","text":"\u672c\u7ae0\u63cf\u8ff0tree\uff0c\u5b83\u5728computer science\u548c\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u4f9d\u7167\u672c\u5de5\u7a0b\u7684\u60ef\u4f8b\uff0c\u4ecestructure\u5165\u624b\uff0c\u5373\u5148\u63cf\u8ff0tree structure\uff08\u6811\u5f62\u7ed3\u6784\uff09\uff0c\u7136\u540e\u63cf\u8ff0tree in data structure\uff08\u6570\u636e\u7ed3\u6784\u4e2d\u7684\u6811\uff09\uff0c\u56e0\u4e3atree in data structure\u672c\u8d28\u4e0a\u662f\u5bf9tree structure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Tree/Application-of-tree-data-structure/","text":"Application of tree data structure # NOTE: The content is concluded from the following post: Tree structure Tree (data structure) Representing nesting structure # Decision tree # Representing sorted lists of data # Tree automaton #","title":"Application-of-tree-data-structure"},{"location":"Tree/Application-of-tree-data-structure/#application_of_tree_data_structure","text":"NOTE: The content is concluded from the following post: Tree structure Tree (data structure)","title":"Application of tree data structure"},{"location":"Tree/Application-of-tree-data-structure/#representing_nesting_structure","text":"","title":"Representing  nesting structure"},{"location":"Tree/Application-of-tree-data-structure/#decision_tree","text":"","title":"Decision tree"},{"location":"Tree/Application-of-tree-data-structure/#representing_sorted_lists_of_data","text":"","title":"Representing sorted lists of data"},{"location":"Tree/Application-of-tree-data-structure/#tree_automaton","text":"","title":"Tree automaton"},{"location":"Tree/Implicit-tree/","text":"Implicit tree # https://en.wikipedia.org/wiki/Bottom-up_parsing Tree is then merely implicit in the parser's actions. \u7c7b\u4f3c\u7684\u6709\u9012\u5f52\u8c03\u7528\u6811\u3002 https://opendatastructures.org/ods-cpp/10_1_Implicit_Binary_Tree.html An Implicit Binary Tree Implicit data structure \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4f7f\u7528\u7684\u662f\u6808\uff0c\u6b63\u5982\u5728 Implicit data structure \u4e2d\u6240\u4ecb\u7ecd\u7684\uff0c\u53ef\u4ee5\u5c06tree\u4fdd\u5b58\u5230\u4e00\u4e2aarray\u4e2d\uff0c\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u4e2d\u6240\u4f7f\u7528\u7684call stack\u5176\u5b9e\u5c31\u7c7b\u4f3c\u4e8e\u5c06\u4e00\u68f5\u6811\u4fdd\u5b58\u5230\u6808\u4e2d\u3002 Implicit k -d tree","title":"Implicit-tree"},{"location":"Tree/Implicit-tree/#implicit_tree","text":"https://en.wikipedia.org/wiki/Bottom-up_parsing Tree is then merely implicit in the parser's actions. \u7c7b\u4f3c\u7684\u6709\u9012\u5f52\u8c03\u7528\u6811\u3002 https://opendatastructures.org/ods-cpp/10_1_Implicit_Binary_Tree.html An Implicit Binary Tree Implicit data structure \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4f7f\u7528\u7684\u662f\u6808\uff0c\u6b63\u5982\u5728 Implicit data structure \u4e2d\u6240\u4ecb\u7ecd\u7684\uff0c\u53ef\u4ee5\u5c06tree\u4fdd\u5b58\u5230\u4e00\u4e2aarray\u4e2d\uff0c\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u4e2d\u6240\u4f7f\u7528\u7684call stack\u5176\u5b9e\u5c31\u7c7b\u4f3c\u4e8e\u5c06\u4e00\u68f5\u6811\u4fdd\u5b58\u5230\u6808\u4e2d\u3002 Implicit k -d tree","title":"Implicit  tree"},{"location":"Tree/Represent-tree/","text":"Represent tree # Visually Representing Trees # There are many ways of visually representing tree structures. Almost always, these boil down to variations, or combinations, of a few basic styles: Classical node-link diagrams # Classical node-link diagrams, that connect nodes together with line segments: encyclopedia / \\ culture science / \\ art craft Nested sets # Nested sets that use enclosure/containment to show parenthood, examples include TreeMaps and fractal maps : Layered \"icicle\" diagrams # Layered \"icicle\" diagrams that use alignment/adjacency. Outlines and tree views # Lists or diagrams that use indentation, sometimes called \" outlines \" or \" tree views \". A tree view: encyclopedia culture art craft science Nested parentheses # See also: Newick format and Dyck language A correspondence to nested parentheses was first noticed by Sir Arthur Cayley : ((art,craft)culture,science)encyclopedia or encyclopedia(culture(art,craft),science) Radial trees # See also: Radial tree Trees can also be represented radially : art craft \\ / culture | encyclopedia | science Note: The following content comes from the the dragon book 6.2 Three-Address Code: Three-address code is a linearized representation of a syntax tree or a DAG in which explicit names correspond to the interior nodes of the graph. Three-address code # Reverse Polish notation # Representing Tree in Memory/Implementation # The following is a partial enumeration: represent the nodes as dynamically allocated records with pointers to their children, their parents, or both represent the nodes as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). For a more complete enumeration, click the link above.","title":"Represent-tree"},{"location":"Tree/Represent-tree/#represent_tree","text":"","title":"Represent tree"},{"location":"Tree/Represent-tree/#visually_representing_trees","text":"There are many ways of visually representing tree structures. Almost always, these boil down to variations, or combinations, of a few basic styles:","title":"Visually  Representing Trees"},{"location":"Tree/Represent-tree/#classical_node-link_diagrams","text":"Classical node-link diagrams, that connect nodes together with line segments: encyclopedia / \\ culture science / \\ art craft","title":"Classical node-link diagrams"},{"location":"Tree/Represent-tree/#nested_sets","text":"Nested sets that use enclosure/containment to show parenthood, examples include TreeMaps and fractal maps :","title":"Nested sets"},{"location":"Tree/Represent-tree/#layered_icicle_diagrams","text":"Layered \"icicle\" diagrams that use alignment/adjacency.","title":"Layered \"icicle\" diagrams"},{"location":"Tree/Represent-tree/#outlines_and_tree_views","text":"Lists or diagrams that use indentation, sometimes called \" outlines \" or \" tree views \". A tree view: encyclopedia culture art craft science","title":"Outlines and tree views"},{"location":"Tree/Represent-tree/#nested_parentheses","text":"See also: Newick format and Dyck language A correspondence to nested parentheses was first noticed by Sir Arthur Cayley : ((art,craft)culture,science)encyclopedia or encyclopedia(culture(art,craft),science)","title":"Nested parentheses"},{"location":"Tree/Represent-tree/#radial_trees","text":"See also: Radial tree Trees can also be represented radially : art craft \\ / culture | encyclopedia | science Note: The following content comes from the the dragon book 6.2 Three-Address Code: Three-address code is a linearized representation of a syntax tree or a DAG in which explicit names correspond to the interior nodes of the graph.","title":"Radial trees"},{"location":"Tree/Represent-tree/#three-address_code","text":"","title":"Three-address code"},{"location":"Tree/Represent-tree/#reverse_polish_notation","text":"","title":"Reverse Polish notation"},{"location":"Tree/Represent-tree/#representing_tree_in_memoryimplementation","text":"The following is a partial enumeration: represent the nodes as dynamically allocated records with pointers to their children, their parents, or both represent the nodes as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). For a more complete enumeration, click the link above.","title":"Representing Tree in Memory/Implementation"},{"location":"Tree/TODO/","text":"radix tree vs trie # https://stackoverflow.com/questions/14708134/what-is-the-difference-between-trie-and-radix-trie-data-structures","title":"TODO"},{"location":"Tree/TODO/#radix_tree_vs_trie","text":"https://stackoverflow.com/questions/14708134/what-is-the-difference-between-trie-and-radix-trie-data-structures","title":"radix tree vs trie"},{"location":"Tree/Tree(data-structure)/","text":"Tree (data structure) # In computer science , a tree is a widely used abstract data type (ADT)\u2014or data structure implementing this ADT\u2014that simulates a hierarchical tree structure , with a root value and subtrees of children with a parent node , represented as a set of linked nodes . A tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the \"children\"), with the constraints that no reference is duplicated, and none points to the root. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002 Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree , with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a set of nodes and an adjacency list of edges between nodes, as one may represent a digraph , for instance). For example, looking at a tree as a whole, one can talk about \"the parent node\" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any). Preliminary definition # A tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002 Mathematical definition # NOTE: \u8fd9\u4e00\u8282\u662f\u539f\u6587\u4e2d\u6700\u6700\u6666\u6da9\u96be\u61c2\u7684\u7ae0\u8282\u4e86\uff0c\u5b83\u9700\u8981set theory\u7684\u77e5\u8bc6\u4f5c\u4e3a\u57fa\u7840\u3002\u90a3\u8fd9\u5c31 \u5176\u5b9e\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\uff0c\u4f7f\u7528tree\u6765\u8868\u793a\u96c6\u5408\u7684\u5305\u542b\u5173\u7cfb\uff0c\u8fd9\u5c31\u597d\u6bd4\u662f\u62ec\u53f7\u4e86\u3002 Unordered tree # Mathematically, an unordered tree [ 1] (or \"algebraic tree\"[ 2] ) can be defined as an algebraic structure $ (X,parent) $ where X is the non-empty carrier set of nodes and parent is a function on X which assigns each node x its \"parent\" node, parent ( x ). The structure is subject to the condition that every non-empty subalgebra must have the same fixed point . That is, there must be a unique \"root\" node r , such that parent ( r ) = r and for every node x , some iterative application parent ( parent (\u2026 parent ( x )\u2026)) equals r . NOTE :\u6570\u5b66\u7684\u5b9a\u4e49\u5f3a\u8c03\u7684\u662f\u6bcf\u4e2anode\u9700\u8981\u6709\u4e00\u4e2a\u4e14\u53ea\u6709\u4e00\u4e2aparent\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u662f\u6570\u5b66\u4e0a\u7684\u5b9a\u4e49\uff0c\u8ba1\u7b97\u673a\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u4e0d\u662f\u5b9a\u4e49parent\uff0c\u800c\u662f\u53cd\u5411\u5b9a\u4e49children\u3002 There are several equivalent definitions. As the closest alternative, one can define unordered trees as partial algebras ( X , parent ) which are obtained from the total algebras described above by letting parent ( r ) be undefined. That is, the root r is the only node on which the parent function is not defined and for every node x , the root is reachable from x in the directed graph ( X , parent ). This definition is in fact coincident with that of an anti-arborescence . The TAoCP book uses the term oriented tree .[ 3] Another equivalent definition is that of a set-theoretic tree that is singly-rooted and whose height is at most \u03c9 (a finite-ish tree[ 4] ). That is, the algebraic structures ( X , parent ) are equivalent to partial orders $ (X,\\leq ) $ that have a top element r and whose every principal upset (aka principal filter ) is a finite chain . To be precise, we should speak about an inverse set-theoretic tree since the set-theoretic definition usually employs opposite ordering. The correspondence between ( X , parent ) and ( X , \u2264) is established via reflexive transitive closure / reduction , with the reduction resulting in the \"partial\" version without the root cycle. We can refer to the four equivalent characterizations as to tree as an algebra , tree as a partial algebra , tree as a partial order , and tree as a prefix order . There is also a fifth equivalent definition \u2013 that of a graph-theoretic rooted tree which is just a connected acyclic rooted graph . Ordered tree # The structures introduced in the previous subsection form just the core \"hierarchical\" part of tree data structures that appear in computing. In most cases, there is also an additional \"horizontal\" ordering between siblings. In search trees the order is commonly established by the \"key\" or value associated with each sibling, but in many trees that is not the case. For example, XML documents, lists within JSON files, and many other structures have order that does not depend on the values in the nodes, but is itself data \u2014 sorting the paragraphs of a novel alphabetically would lose information. The correspondent expansion of the previously described tree structures ( X , \u2264) can be defined by endowing each sibling set with a linear order as follows. An alternative definition according to Kuboyama is presented in the next subsection. Terminology used in trees # NOTE: \u539f\u6587\u672c\u8282\u63cf\u8ff0tree\u4e2d\u7684\u5404\u79cd\u672f\u8bed\u3002 Representations # There are many different ways to represent trees; common representations represent the nodes as dynamically allocated records with pointers to their children, their parents, or both, or as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp S-expressions , where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child. In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a threaded binary tree . Common operations # Enumerating all the items Enumerating a section of a tree Searching for an item Adding a new item at a certain position on the tree Deleting an item Pruning : Removing a whole section of a tree Grafting : Adding a whole section to a tree Finding the root for any node Finding the lowest common ancestor of two nodes Common uses # Representing hierarchical data such as syntax trees Storing data in a way that makes it efficiently searchable (see binary search tree and tree traversal ) Representing sorted lists of data As a workflow for compositing digital images for visual effects Storing Barnes-Hut trees used to simulate galaxies. Recursive definition of tree # \u672c\u6587\u4e2d\uff0c\u5173\u4e8e\u6811\u7684Recursive definition\u90fd\u6709\u6807\u6ce8\u51fa\u6765\u4e86\u3002 https://cgi.csc.liv.ac.uk/~michele/TEACHING/COMP102/2006/5.4.pdf http://www.montefiore.ulg.ac.be/~piater/Cours/INFO0902/notes/tree/foil04.xhtml \u6811\u7684\u7ed3\u6784\u662f\u5177\u5907\u9012\u5f52\u6027\u7684\uff1a\u4e00\u4e2a\u8282\u70b9\u7684\u5de6\u8282\u70b9\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u53f3\u8282\u70b9\u4e5f\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u663e\u7136\u6811\u7684\u5b9a\u4e49\u662f\u7531\u5b83\u81ea\u8eab\u5b9a\u4e49\u7684\uff1b\u6240\u4ee5\u5bf9\u6811\u7684\u64cd\u4f5c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6811\u7ed3\u6784\u7684\u9012\u5f52\u6027\u800c\u5199\u51fa\u9012\u5f52\u51fd\u6570\uff1b","title":"Tree(data-structure)"},{"location":"Tree/Tree(data-structure)/#tree_data_structure","text":"In computer science , a tree is a widely used abstract data type (ADT)\u2014or data structure implementing this ADT\u2014that simulates a hierarchical tree structure , with a root value and subtrees of children with a parent node , represented as a set of linked nodes . A tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the \"children\"), with the constraints that no reference is duplicated, and none points to the root. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002 Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree , with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a set of nodes and an adjacency list of edges between nodes, as one may represent a digraph , for instance). For example, looking at a tree as a whole, one can talk about \"the parent node\" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).","title":"Tree (data structure)"},{"location":"Tree/Tree(data-structure)/#preliminary_definition","text":"A tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002","title":"Preliminary definition"},{"location":"Tree/Tree(data-structure)/#mathematical_definition","text":"NOTE: \u8fd9\u4e00\u8282\u662f\u539f\u6587\u4e2d\u6700\u6700\u6666\u6da9\u96be\u61c2\u7684\u7ae0\u8282\u4e86\uff0c\u5b83\u9700\u8981set theory\u7684\u77e5\u8bc6\u4f5c\u4e3a\u57fa\u7840\u3002\u90a3\u8fd9\u5c31 \u5176\u5b9e\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\uff0c\u4f7f\u7528tree\u6765\u8868\u793a\u96c6\u5408\u7684\u5305\u542b\u5173\u7cfb\uff0c\u8fd9\u5c31\u597d\u6bd4\u662f\u62ec\u53f7\u4e86\u3002","title":"Mathematical definition"},{"location":"Tree/Tree(data-structure)/#unordered_tree","text":"Mathematically, an unordered tree [ 1] (or \"algebraic tree\"[ 2] ) can be defined as an algebraic structure $ (X,parent) $ where X is the non-empty carrier set of nodes and parent is a function on X which assigns each node x its \"parent\" node, parent ( x ). The structure is subject to the condition that every non-empty subalgebra must have the same fixed point . That is, there must be a unique \"root\" node r , such that parent ( r ) = r and for every node x , some iterative application parent ( parent (\u2026 parent ( x )\u2026)) equals r . NOTE :\u6570\u5b66\u7684\u5b9a\u4e49\u5f3a\u8c03\u7684\u662f\u6bcf\u4e2anode\u9700\u8981\u6709\u4e00\u4e2a\u4e14\u53ea\u6709\u4e00\u4e2aparent\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u662f\u6570\u5b66\u4e0a\u7684\u5b9a\u4e49\uff0c\u8ba1\u7b97\u673a\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u4e0d\u662f\u5b9a\u4e49parent\uff0c\u800c\u662f\u53cd\u5411\u5b9a\u4e49children\u3002 There are several equivalent definitions. As the closest alternative, one can define unordered trees as partial algebras ( X , parent ) which are obtained from the total algebras described above by letting parent ( r ) be undefined. That is, the root r is the only node on which the parent function is not defined and for every node x , the root is reachable from x in the directed graph ( X , parent ). This definition is in fact coincident with that of an anti-arborescence . The TAoCP book uses the term oriented tree .[ 3] Another equivalent definition is that of a set-theoretic tree that is singly-rooted and whose height is at most \u03c9 (a finite-ish tree[ 4] ). That is, the algebraic structures ( X , parent ) are equivalent to partial orders $ (X,\\leq ) $ that have a top element r and whose every principal upset (aka principal filter ) is a finite chain . To be precise, we should speak about an inverse set-theoretic tree since the set-theoretic definition usually employs opposite ordering. The correspondence between ( X , parent ) and ( X , \u2264) is established via reflexive transitive closure / reduction , with the reduction resulting in the \"partial\" version without the root cycle. We can refer to the four equivalent characterizations as to tree as an algebra , tree as a partial algebra , tree as a partial order , and tree as a prefix order . There is also a fifth equivalent definition \u2013 that of a graph-theoretic rooted tree which is just a connected acyclic rooted graph .","title":"Unordered tree"},{"location":"Tree/Tree(data-structure)/#ordered_tree","text":"The structures introduced in the previous subsection form just the core \"hierarchical\" part of tree data structures that appear in computing. In most cases, there is also an additional \"horizontal\" ordering between siblings. In search trees the order is commonly established by the \"key\" or value associated with each sibling, but in many trees that is not the case. For example, XML documents, lists within JSON files, and many other structures have order that does not depend on the values in the nodes, but is itself data \u2014 sorting the paragraphs of a novel alphabetically would lose information. The correspondent expansion of the previously described tree structures ( X , \u2264) can be defined by endowing each sibling set with a linear order as follows. An alternative definition according to Kuboyama is presented in the next subsection.","title":"Ordered tree"},{"location":"Tree/Tree(data-structure)/#terminology_used_in_trees","text":"NOTE: \u539f\u6587\u672c\u8282\u63cf\u8ff0tree\u4e2d\u7684\u5404\u79cd\u672f\u8bed\u3002","title":"Terminology used in trees"},{"location":"Tree/Tree(data-structure)/#representations","text":"There are many different ways to represent trees; common representations represent the nodes as dynamically allocated records with pointers to their children, their parents, or both, or as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp S-expressions , where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child. In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a threaded binary tree .","title":"Representations"},{"location":"Tree/Tree(data-structure)/#common_operations","text":"Enumerating all the items Enumerating a section of a tree Searching for an item Adding a new item at a certain position on the tree Deleting an item Pruning : Removing a whole section of a tree Grafting : Adding a whole section to a tree Finding the root for any node Finding the lowest common ancestor of two nodes","title":"Common operations"},{"location":"Tree/Tree(data-structure)/#common_uses","text":"Representing hierarchical data such as syntax trees Storing data in a way that makes it efficiently searchable (see binary search tree and tree traversal ) Representing sorted lists of data As a workflow for compositing digital images for visual effects Storing Barnes-Hut trees used to simulate galaxies.","title":"Common uses"},{"location":"Tree/Tree(data-structure)/#recursive_definition_of_tree","text":"\u672c\u6587\u4e2d\uff0c\u5173\u4e8e\u6811\u7684Recursive definition\u90fd\u6709\u6807\u6ce8\u51fa\u6765\u4e86\u3002 https://cgi.csc.liv.ac.uk/~michele/TEACHING/COMP102/2006/5.4.pdf http://www.montefiore.ulg.ac.be/~piater/Cours/INFO0902/notes/tree/foil04.xhtml \u6811\u7684\u7ed3\u6784\u662f\u5177\u5907\u9012\u5f52\u6027\u7684\uff1a\u4e00\u4e2a\u8282\u70b9\u7684\u5de6\u8282\u70b9\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u53f3\u8282\u70b9\u4e5f\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u663e\u7136\u6811\u7684\u5b9a\u4e49\u662f\u7531\u5b83\u81ea\u8eab\u5b9a\u4e49\u7684\uff1b\u6240\u4ee5\u5bf9\u6811\u7684\u64cd\u4f5c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6811\u7ed3\u6784\u7684\u9012\u5f52\u6027\u800c\u5199\u51fa\u9012\u5f52\u51fd\u6570\uff1b","title":"Recursive definition of tree"},{"location":"Tree/Tree(set-theory)/","text":"Tree (set theory) Definition Tree (set theory) # In set theory , a tree is a partially ordered set ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. Frequently trees are assumed to have only one root (i.e. minimal element ), as the typical questions investigated in this field are easily reduced to questions about single-rooted trees. Definition # A tree is a partially ordered set (poset) ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. In particular, each well-ordered set ( T , <) is a tree. For each t \u2208 T , the order type of { s \u2208 T : s < t } is called the height of t (denoted ht( t , T )). The height of T itself is the least ordinal greater than the height of each element of T . A root of a tree T is an element of height 0. Frequently trees are assumed to have only one root. Note that trees in set theory are often defined to grow downward making the root the greatest node. Trees with a single root may be viewed as rooted trees in the sense of graph theory in one of two ways: either as a tree (graph theory) or as a trivially perfect graph . In the first case, the graph is the undirected Hasse Diagram of the partially ordered set, and in the second case, the graph is simply the underlying (undirected) graph of the partially ordered set. However, if T is a tree of height > \u03c9, then the Hasse diagram definition does not work. For example, the partially ordered set $ \\omega +1=\\left{0,1,2,\\dots ,\\omega \\right} $ does not have a Hasse Diagram, as there is no predecessor to \u03c9. Hence we require height at most omega in this case. A branch of a tree is a maximal chain in the tree (that is, any two elements of the branch are comparable, and any element of the tree not in the branch is incomparable with at least one element of the branch). The length of a branch is the ordinal that is order isomorphic to the branch. For each ordinal \u03b1, the \u03b1-th level of T is the set of all elements of T of height \u03b1. A tree is a \u03ba-tree, for an ordinal number \u03ba, if and only if it has height \u03ba and every level has size less than the cardinality of \u03ba. The width of a tree is the supremum of the cardinalities of its levels. Any single-rooted tree of height $ \\leq \\omega $ forms a meet-semilattice, where meet (common ancestor) is given by maximal element of intersection of ancestors, which exists as the set of ancestors is non-empty and finite well-ordered, hence has a maximal element. Without a single root, the intersection of parents can be empty (two elements need not have common ancestors), for example $ \\left{a,b\\right} $ where the elements are not comparable; while if there are an infinite number of ancestors there need not be a maximal element \u2013 for example, $ \\left{0,1,2,\\dots ,\\omega {0},\\omega {0}'\\right} $ where $ \\omega {0},\\omega {0}' $ are not comparable. A subtree of a tree $ (T,<) $ is a tree $ (T',<) $ where $ T'\\subseteq T $ and $ T' $ is downward closed under $ < $, i.e., if $ s,t\\in T $ and $ s<t $ then $ t\\in T'\\implies s\\in T' $.","title":"Tree(set-theory)"},{"location":"Tree/Tree(set-theory)/#tree_set_theory","text":"In set theory , a tree is a partially ordered set ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. Frequently trees are assumed to have only one root (i.e. minimal element ), as the typical questions investigated in this field are easily reduced to questions about single-rooted trees.","title":"Tree (set theory)"},{"location":"Tree/Tree(set-theory)/#definition","text":"A tree is a partially ordered set (poset) ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. In particular, each well-ordered set ( T , <) is a tree. For each t \u2208 T , the order type of { s \u2208 T : s < t } is called the height of t (denoted ht( t , T )). The height of T itself is the least ordinal greater than the height of each element of T . A root of a tree T is an element of height 0. Frequently trees are assumed to have only one root. Note that trees in set theory are often defined to grow downward making the root the greatest node. Trees with a single root may be viewed as rooted trees in the sense of graph theory in one of two ways: either as a tree (graph theory) or as a trivially perfect graph . In the first case, the graph is the undirected Hasse Diagram of the partially ordered set, and in the second case, the graph is simply the underlying (undirected) graph of the partially ordered set. However, if T is a tree of height > \u03c9, then the Hasse diagram definition does not work. For example, the partially ordered set $ \\omega +1=\\left{0,1,2,\\dots ,\\omega \\right} $ does not have a Hasse Diagram, as there is no predecessor to \u03c9. Hence we require height at most omega in this case. A branch of a tree is a maximal chain in the tree (that is, any two elements of the branch are comparable, and any element of the tree not in the branch is incomparable with at least one element of the branch). The length of a branch is the ordinal that is order isomorphic to the branch. For each ordinal \u03b1, the \u03b1-th level of T is the set of all elements of T of height \u03b1. A tree is a \u03ba-tree, for an ordinal number \u03ba, if and only if it has height \u03ba and every level has size less than the cardinality of \u03ba. The width of a tree is the supremum of the cardinalities of its levels. Any single-rooted tree of height $ \\leq \\omega $ forms a meet-semilattice, where meet (common ancestor) is given by maximal element of intersection of ancestors, which exists as the set of ancestors is non-empty and finite well-ordered, hence has a maximal element. Without a single root, the intersection of parents can be empty (two elements need not have common ancestors), for example $ \\left{a,b\\right} $ where the elements are not comparable; while if there are an infinite number of ancestors there need not be a maximal element \u2013 for example, $ \\left{0,1,2,\\dots ,\\omega {0},\\omega {0}'\\right} $ where $ \\omega {0},\\omega {0}' $ are not comparable. A subtree of a tree $ (T,<) $ is a tree $ (T',<) $ where $ T'\\subseteq T $ and $ T' $ is downward closed under $ < $, i.e., if $ s,t\\in T $ and $ s<t $ then $ t\\in T'\\implies s\\in T' $.","title":"Definition"},{"location":"Tree/Tree-structure/","text":"Tree structure # \u672c\u6587\u6240\u8981\u8ba8\u8bba\u7684\u662f\u201c\u6811\u5f62\u5f62\u72b6\u201d\uff0c\u6807\u9898\u4e2d\u7684\u201cstructure\u201d\u5728\u6240\u8981\u8868\u8fbe\u7684\u542b\u4e49\u662f\u201c\u5f62\u72b6\u201d\u3002 \u672c\u6587\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Tree structure \u3002 A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. It is named a \"tree structure\" because the classic representation resembles a tree . \u5728 Hierarchy \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86 tree structure \u5bf9\u5e94\u7684\u662f nested hierarchy structure\u3002 tree structure \u7684\u6700\u6700\u5178\u578b\u7684\u7279\u6027\u662f\uff1a\u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\uff08\u5305\u542b\uff09\u591a\u4e2a\u5b50\u8282\u70b9\uff0c\u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9\uff0croot\u8282\u70b9\u6ca1\u6709\u7236\u8282\u70b9\u3002 tree structure \u7684 \u201c\u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9\u201d \u7684\u8981\u6c42\uff0c\u5c06\u5b83\u548cgraph\u533a\u5206\u5f00\u6765\u4e86\uff08\u53c2\u89c1 Discrete Mathematics and Its Applications \u7684Tree\u7ae0\u8282\uff09\u3002 \u63cf\u8ff0 tree structure \u7684\u8fd9\u4e2a\u6700\u6700\u5178\u578b\u7279\u6027\u7684\u8bcd\u662f\uff1anesting \uff0cnesting\u8fd9\u4e2a\u8bcd\u7684\u542b\u4e49\u662f\u4e30\u5bcc\u7684\uff0c\u5b83\u7684\u8868\u9762\u610f\u601d\u662f\u201c\u5d4c\u5957\u201d\uff0c\u540c\u65f6\u5b83\u8574\u542b\u7740\u201c\u5305\u542b\u201d\u7684\u542b\u4e49\uff1b\u201c\u5d4c\u5957\u201d\u8574\u542b\u7740\u201c\u9012\u5f52\u201d\uff1b\u540e\u9762\u6211\u4eec\u5c06\u7edf\u4e00\u201dnesting\u201c\u89e3\u91ca\u4e3a\u201c\u5d4c\u5957\u5305\u542b\u201d\u3002 \u6211\u7b2c\u4e00\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u8282\u65f6\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time . The following example illustrates nesting of procedure calls. \u8fd9\u662factivations of procedures\u7684 nest in time \u7279\u6027\uff0c\u4f7f\u5f97\u201cStack allocation\u201d\u53d8\u5f97\u53ef\u884c\uff0c\u5e76\u4e14activations of procedures\u7684\u8fc7\u7a0b\u662ftree structure\u7684\u3002 \u7b2c\u4e8c\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Hierarchy \u7684\u201cNested hierarchy\u201d\u8282\u65f6\uff0c\u81f3\u6b64\u624d\u66f4\u52a0\u89c9\u5f97\u5b83\u975e\u5e38\u80fd\u591f\u4f53\u73b0 tree structure \u7684\u672c\u8d28\u3002 \u66f4\u591a\u5173\u4e8enesting\u7684\u63cf\u8ff0\uff0c\u53c2\u89c1\uff1a Nesting (computing) \u548c Nested sets \uff09\u3002 \u663e\u7136\u5177\u5907nesting\u7279\u6027\uff0c\u5c31\u5177\u5907\u4e86\u5982\u4e0b\u7279\u6027\uff1a hierarchical \uff0c\u5373\u6811\u7ed3\u6784\u662f\u5c42\u6b21\u7684 recursive \uff0c\u5373\u6811\u7ed3\u6784\u662f\u5177\u5907\u9012\u5f52\u7279\u6027\u7684 \u54ea\u4e9b\u5173\u7cfb\u80fd\u591f\u5f62\u6210\u6811\u5f62\u72b6 # \u8fd9\u4e2a\u95ee\u9898\u5728 Hierarchy \u4e2d\u540c\u6837\u63d0\u95ee\u8fc7\uff0c\u672c\u6bb5\u4ece\u4e00\u4e9b\u5177\u4f53\u7684\u5173\u7cfb\u7684\u4f8b\u5b50\u51fa\u53d1\u6765\u8fdb\u884c\u603b\u7ed3\u3002 Example: nesting\u5173\u7cfb # \u5728\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u8bf4\u660e\u4e86tree structure\u7684\u6700\u6700\u6839\u672c\u7684\u7279\u5f81\u662fnesting\uff0c\u6240\u6709\u7684\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\uff0c\u6309\u7167\u8be5\u5173\u7cfb\u8fdb\u884c\u7ec4\u7ec7\uff0c\u90fd\u80fd\u591f\u5f62\u6210tree structure\u3002\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5b58\u5728\u7740\u592a\u591a\u592a\u591a\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\u4e86\uff0c\u5728\u4e0b\u9762\u7684 Examples of tree structures \u4f1a\u679a\u4e3e\u5177\u5907\u8fd9\u79cd\u5173\u7cfb\u7684\u7ed3\u6784\u3002 \u5176\u5b9e\u6709\u5f88\u591a\u7684\u5173\u7cfb\u672c\u8d28\u4e0a\u90fd\u662fnesting\u5173\u7cfb\uff1a \u63a8\u5bfc\u5173\u7cfb # Formal grammar\u7684production\u7684head\u53ef\u4ee5derive\u5f97\u5230body\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8eexpand\uff0c\u5176\u5b9eexpand\u5c31\u662f\u5305\u542b\uff0c\u4e5f\u5c31\u662fnesting \u5173\u7cfb\u3002\u66f4\u591a\u5173\u4e8e\u63a8\u5bfc\u5173\u7cfb\u53c2\u89c1 \u63a8\u5bfc\u5173\u7cfb\u5206\u6790 \u3002 Example: Parent-child\u5173\u7cfb # \u4e00\u4e2aparent\u53ef\u4ee5\u6709\u591a\u4e2achildren\uff0c\u4e00\u4e2achild\u53ea\u80fd\u591f\u6709\u4e00\u4e2aparent\u3002\u5176\u5b9eParent-child\u5173\u7cfb\u4e5f\u53ef\u4ee5\u5f52\u5165\u5230nesting\u5173\u7cfb\u4e2d\u3002 \u6700\u7ec8\u7b54\u6848 # \u8fd9\u79cd\u5173\u7cfb\u9700\u8981\u662fN:1\u7684\uff08\u8bb0\u5f97\u5728\u5927\u5b66\u7684\u6570\u636e\u5e93\u8bfe\u7a0b\u6240\u4f7f\u7528\u7684\u6559\u6750\u4e2d\u6709\u8fc7\u8fd9\u6837\u7684\u7406\u8bba\uff0c\u8fd9\u4e2a\u7406\u8bba\u5e94\u8be5\u662f\u5c5e\u4e8e Relational algebra \uff0c\u53c2\u89c1\uff1a Relational model \u3001 Database normalization \uff09\u3002 \u8fd9\u79cd\u5173\u7cfb\u5e94\u8be5\u662f Transitive relation \u3002 \u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u662f Reflexive relation \u3002 \u53ef\u4ee5\u8fd9\u6837\u4e0d\u4e25\u8c28\u5730\u8fdb\u884c\u63cf\u8ff0\uff1a \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb \u3002 Examples of tree structures # Directory structure ( directory ) # nesting\u5173\u7cfb\u3002 See also: Tree (command) Path (computing) Process tree # parent-children\u5173\u7cfb\u3002 File format # nesting\u5173\u7cfb\u3002 Document Object Model \uff08 XML \uff09 json yaml Namespace # nesting\u5173\u7cfb\u3002 Namespace\u7684\u5e94\u7528\u573a\u666f\u5b9e\u5728\u592a\u591a\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u5b83\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002\u5728\u5bf9\u5b83\u8fdb\u884c\u601d\u8003\u7684\u65f6\u5019\uff0c\u53d1\u89c9\u4f7f\u7528namespace\u6765\u7ec4\u7ec7\u7684\u6570\u636e\u6700\u7ec8\u5c31\u662fhierarchy\u7ed3\u6784\u3002\u5176\u5b9e\u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u5c06namespace\u770b\u505a\u662f\u62ec\u53f7\u3002 Expression # binary expression tree Source code # Parse tree \u3001 Abstract syntax tree \u3002 nesting\u5173\u7cfb\u3002 Activation tree # nesting\u5173\u7cfb\u3002 \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4e5f\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u53c2\u89c1 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u3002 Linguistics # \u5728\u8bed\u8a00\u5b66\u4e2d\uff0c\u57fa\u672c\u4e0a\u662f\u4f7f\u7528tree\u6765\u63cf\u8ff0\u8bed\u8a00\u7684\u7ed3\u6784\u3002 If you have read book describing the compiler technology , for example the classic definitive book Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman, you will be aware how important the tree structure is to the compiler. As described in chapter 2.2.3 Parse Trees: Tree data structures figure prominently in compiling. There are some many tree in the book, such as Parse tree , abstract syntax tree , activation tree(in chapter 7.2.1 Activation Trees), expression tree . Essentially Speaking, a programming language is a formal language , what have been concluded in article structure is that Tree can be used to describe the structure of sentences that is syntax and formal grammar can be convert to tree . So it's natural to use trees in the compiling. Regular expression , algebraic expression can be described using formal grammar , so given an expression, it can be converted to an equivalent parse tree . Nesting \u4e0e hierarchy # Nesting\u7ed3\u6784\u5177\u5907hierarchy\u7279\u6027\uff0c\u53ef\u4ee5\u8fd9\u6837\u6765\u8fdb\u884c\u89e3\u91ca\uff1a \u6700\u5916\u5c42\u662f\u662f\u7b2c\u4e00\u5c42\u3001\u5b83\u6240\u76f4\u63a5\u5305\u542b\u7684\u5143\u7d20\u90fd\u5c5e\u4e8e\u7b2c\u4e8c\u5c42\u3001\u4f9d\u6b21\u9012\u5f52\u3002 \u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811 # Nesting \u7ed3\u6784\u5728computer science\u662f\u975e\u5e38\u5e38\u89c1\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684hierarchy\u7ed3\u6784\uff0cnesting\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u8868\u793a\uff1a ( () () ( ( ) ) ) Nesting\u7684\u4e2d\u6587\u542b\u4e49\u662f\u201c\u5d4c\u5957\u201d\uff0c\u663e\u7136\uff0c\u5b83\u80fd\u591f\u63cf\u8ff0\u5143\u7d20\u4e4b\u95f4\u7684\u5d4c\u5957\u5173\u7cfb\uff1b\u4e0a\u9762\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793anesting\u7ed3\u6784\uff0c\u56e0\u4e3a\u62ec\u53f7\u6240\u80fd\u591f\u8868\u8fbe\u7684\u201c\u5305\u542b\u201d\u5173\u7cfb\u548c\u201c\u5d4c\u5957\u201d\u5173\u7cfb\u662f\u57fa\u672c\u7c7b\u4f3c\u7684\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u4e0b\u9762\u7684 Structure and relation \u3002 \u4e0a\u8ff0\u7ed3\u6784\u662f\u53ef\u4ee5\u8868\u793a\u6210\u6811\u7684\uff0c\u5982\u4e0b\uff1a ( ) ( ) ( ) ( ) ( ) \u4f8b\u5b50\u5305\u62ec\uff1a C\u548cC++\u4e2d\uff0c\u4f7f\u7528 {} \u6765\u5b9a\u4e49block\uff0cblock\u4e2d\u53ef\u4ee5\u518d\u5305\u542bblock\uff0c\u4ece\u800c\u5f62\u6210nesting\u7ed3\u6784 \u9f99\u4e667.2.1 Activation Trees\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time. \u5373\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u4ece\u65f6\u95f4\u4e0a\u6765\u770b\u4e5f\u662f\u5d4c\u5957\u7684\u3002 \u63a8\u5bfc\u5173\u7cfb\u5206\u6790 # \u6269\u5c55\u4e00\u4e2a\u4f7f\u7528tree\u63cf\u8ff0\u7684\u5173\u7cfb\u7684\u6700\u7ec8\u76ee\u6807\u662f\u83b7\u5f97\u6240\u6709\u7684\u53f6\u5b50\u8282\u70b9\uff0c\u5b83\u7684\u57fa\u672c\u7b97\u6cd5\u662f\uff1a\u4e00\u4e2a\u8282\u70b9\uff0c\u53ea\u8981\u662fnon-terminal\u5143\u7d20\uff0c\u5c31\u9700\u8981\u5bf9\u5b83\u8fdb\u884cexpand\uff0c\u5176\u5b9e\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\uff1b \u6240\u4ee5\u5176\u5b9e\uff0c\u6211\u4e0a\u8ff0\u6240\u63cf\u8ff0\u7684\u90fd\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\u8fc7\u7a0b\uff1b \u4e0b\u9762\u662f\u4e00\u6bb5\u63cf\u8ff0\u4e0a\u8ff0 \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb \u7684\u83b7\u53d6\u6240\u6709\u7684\u53ef\u80fd\u7684\u53f6\u5b50\u8282\u70b9\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u5b83\u9700\u8981\u5c06\u6240\u6709\u7684\u5185\u8282\u70b9\u8fdb\u884c\u6269\u5c55\uff0c\u6700\u7ec8\u7684\u7ed3\u679c\u53ea\u80fd\u591f\u5305\u542b\u53f6\u5b50\u8282\u70b9\u800c\u4e0d\u80fd\u5305\u542b\u53f6\u5b50\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info] = list() to_expand_words = list(retriever_context.fen_zi_detail_dict[fen_zi_word_str]) # \u5f85\u6269\u5c55\u8bcd\u5217\u8868 while len(to_expand_words): word = to_expand_words.pop() # \u4e00\u6b21\u53ea\u5904\u7406\u4e00\u4e2a\u8bcd if word in retriever_context.fen_zi_detail_dict: # \u5f53\u524d\u8bcd\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5185\u8282\u70b9 to_expand_words.extend(retriever_context.fen_zi_detail_dict[word]) # \u6269\u5c55\u5f53\u524d\u8bcd\uff0c\u5e76\u4e14\u5c06\u5b83\u6dfb\u52a0\u5230\u5f85\u6269\u5c55\u8bcd\u5217\u8868\u4e2d else: # \u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u9875\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info].append(word) # \u5c06\u8be5\u8bcd\u8fdb\u884c\u8f93\u51fa \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb\u4f8b\u5b50 # \u4e00\u4e9b\u5305\u542b\u5173\u7cfb\u5177\u6709\u4f20\u9012\u6027\uff08\u56de\u53bb\u770b\u79bb\u6563\u6570\u5b66\u5bf9\u8fd9\u7684\u63cf\u8ff0\uff09\uff0c\u6bd4\u5982A\u5305\u542bB\uff0cB\u53c8\u5305\u542bC\uff0c\u5219A\u5e94\u8be5\u5305\u542b\u6240\u6709\u7684C\u3002 \u5982\u4e0b\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a \u6709\u4ef7\u8bc1\u5238:\u80a1\u7968,\u503a\u5238,\u6743\u8bc1,\u8d44\u4ea7\u652f\u6301\u8bc1\u5238,\u4e70\u5165\u8fd4\u552e\u91d1\u878d\u8d44\u4ea7 \u503a\u5238:\u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u6709\u4ef7\u8bc1\u5238 \u5305\u542b \u503a\u5238 \uff0c\u503a\u5238\u53c8\u5305\u542b \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \uff0c\u6240\u4ee5 \u6709\u4ef7\u8bc1\u5238 \u5305\u62ec \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u3002 \u4e0a\u8ff0\u5173\u7cfb\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684 \u4e0a\u8ff0\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2adict\u6765\u8fdb\u884c\u4fdd\u5b58\uff1a\u6240\u6709\u4f5c\u4e3akey\u7684\u90fd\u662fnon-terminal\uff0c\u90fd\u9700\u8981\u8fdb\u884c\u6269\u5c55\uff1b See also # Hierarchy Nesting (computing) Nested set model Nested set Hereditary property Software engineer\u5bf9\u8fd9\u4e24\u4e2a\u8bcd\u80af\u5b9a\u4e0d\u4f1a\u964c\u751f\uff0c\u4f46\u4e0d\u77e5\u662f\u5426\u77e5\u6653\u5b83\u4eec\u90fd\u53ef\u4ee5\u4f7f\u7528tree structure\u6765\u8fdb\u884c\u8868\u793a\uff0c\u5e0c\u671b\u5728\u9605\u8bfb\u4e86\u672c\u6587\u540e\uff0c\u8bfb\u8005\u4e0b\u6b21\u5728\u9047\u5230\u8fd9\u4e24\u4e2a\u8bcd\u7684\u65f6\u5019\u80fd\u591f\u4ea7\u751f\u8fd9\u6837\u7684\u53cd\u6620\u3002","title":"Tree-structure"},{"location":"Tree/Tree-structure/#tree_structure","text":"\u672c\u6587\u6240\u8981\u8ba8\u8bba\u7684\u662f\u201c\u6811\u5f62\u5f62\u72b6\u201d\uff0c\u6807\u9898\u4e2d\u7684\u201cstructure\u201d\u5728\u6240\u8981\u8868\u8fbe\u7684\u542b\u4e49\u662f\u201c\u5f62\u72b6\u201d\u3002 \u672c\u6587\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Tree structure \u3002 A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. It is named a \"tree structure\" because the classic representation resembles a tree . \u5728 Hierarchy \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86 tree structure \u5bf9\u5e94\u7684\u662f nested hierarchy structure\u3002 tree structure \u7684\u6700\u6700\u5178\u578b\u7684\u7279\u6027\u662f\uff1a\u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\uff08\u5305\u542b\uff09\u591a\u4e2a\u5b50\u8282\u70b9\uff0c\u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9\uff0croot\u8282\u70b9\u6ca1\u6709\u7236\u8282\u70b9\u3002 tree structure \u7684 \u201c\u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9\u201d \u7684\u8981\u6c42\uff0c\u5c06\u5b83\u548cgraph\u533a\u5206\u5f00\u6765\u4e86\uff08\u53c2\u89c1 Discrete Mathematics and Its Applications \u7684Tree\u7ae0\u8282\uff09\u3002 \u63cf\u8ff0 tree structure \u7684\u8fd9\u4e2a\u6700\u6700\u5178\u578b\u7279\u6027\u7684\u8bcd\u662f\uff1anesting \uff0cnesting\u8fd9\u4e2a\u8bcd\u7684\u542b\u4e49\u662f\u4e30\u5bcc\u7684\uff0c\u5b83\u7684\u8868\u9762\u610f\u601d\u662f\u201c\u5d4c\u5957\u201d\uff0c\u540c\u65f6\u5b83\u8574\u542b\u7740\u201c\u5305\u542b\u201d\u7684\u542b\u4e49\uff1b\u201c\u5d4c\u5957\u201d\u8574\u542b\u7740\u201c\u9012\u5f52\u201d\uff1b\u540e\u9762\u6211\u4eec\u5c06\u7edf\u4e00\u201dnesting\u201c\u89e3\u91ca\u4e3a\u201c\u5d4c\u5957\u5305\u542b\u201d\u3002 \u6211\u7b2c\u4e00\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u8282\u65f6\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time . The following example illustrates nesting of procedure calls. \u8fd9\u662factivations of procedures\u7684 nest in time \u7279\u6027\uff0c\u4f7f\u5f97\u201cStack allocation\u201d\u53d8\u5f97\u53ef\u884c\uff0c\u5e76\u4e14activations of procedures\u7684\u8fc7\u7a0b\u662ftree structure\u7684\u3002 \u7b2c\u4e8c\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Hierarchy \u7684\u201cNested hierarchy\u201d\u8282\u65f6\uff0c\u81f3\u6b64\u624d\u66f4\u52a0\u89c9\u5f97\u5b83\u975e\u5e38\u80fd\u591f\u4f53\u73b0 tree structure \u7684\u672c\u8d28\u3002 \u66f4\u591a\u5173\u4e8enesting\u7684\u63cf\u8ff0\uff0c\u53c2\u89c1\uff1a Nesting (computing) \u548c Nested sets \uff09\u3002 \u663e\u7136\u5177\u5907nesting\u7279\u6027\uff0c\u5c31\u5177\u5907\u4e86\u5982\u4e0b\u7279\u6027\uff1a hierarchical \uff0c\u5373\u6811\u7ed3\u6784\u662f\u5c42\u6b21\u7684 recursive \uff0c\u5373\u6811\u7ed3\u6784\u662f\u5177\u5907\u9012\u5f52\u7279\u6027\u7684","title":"Tree structure"},{"location":"Tree/Tree-structure/#_1","text":"\u8fd9\u4e2a\u95ee\u9898\u5728 Hierarchy \u4e2d\u540c\u6837\u63d0\u95ee\u8fc7\uff0c\u672c\u6bb5\u4ece\u4e00\u4e9b\u5177\u4f53\u7684\u5173\u7cfb\u7684\u4f8b\u5b50\u51fa\u53d1\u6765\u8fdb\u884c\u603b\u7ed3\u3002","title":"\u54ea\u4e9b\u5173\u7cfb\u80fd\u591f\u5f62\u6210\u6811\u5f62\u72b6"},{"location":"Tree/Tree-structure/#example_nesting","text":"\u5728\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u8bf4\u660e\u4e86tree structure\u7684\u6700\u6700\u6839\u672c\u7684\u7279\u5f81\u662fnesting\uff0c\u6240\u6709\u7684\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\uff0c\u6309\u7167\u8be5\u5173\u7cfb\u8fdb\u884c\u7ec4\u7ec7\uff0c\u90fd\u80fd\u591f\u5f62\u6210tree structure\u3002\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5b58\u5728\u7740\u592a\u591a\u592a\u591a\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\u4e86\uff0c\u5728\u4e0b\u9762\u7684 Examples of tree structures \u4f1a\u679a\u4e3e\u5177\u5907\u8fd9\u79cd\u5173\u7cfb\u7684\u7ed3\u6784\u3002 \u5176\u5b9e\u6709\u5f88\u591a\u7684\u5173\u7cfb\u672c\u8d28\u4e0a\u90fd\u662fnesting\u5173\u7cfb\uff1a","title":"Example: nesting\u5173\u7cfb"},{"location":"Tree/Tree-structure/#_2","text":"Formal grammar\u7684production\u7684head\u53ef\u4ee5derive\u5f97\u5230body\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8eexpand\uff0c\u5176\u5b9eexpand\u5c31\u662f\u5305\u542b\uff0c\u4e5f\u5c31\u662fnesting \u5173\u7cfb\u3002\u66f4\u591a\u5173\u4e8e\u63a8\u5bfc\u5173\u7cfb\u53c2\u89c1 \u63a8\u5bfc\u5173\u7cfb\u5206\u6790 \u3002","title":"\u63a8\u5bfc\u5173\u7cfb"},{"location":"Tree/Tree-structure/#example_parent-child","text":"\u4e00\u4e2aparent\u53ef\u4ee5\u6709\u591a\u4e2achildren\uff0c\u4e00\u4e2achild\u53ea\u80fd\u591f\u6709\u4e00\u4e2aparent\u3002\u5176\u5b9eParent-child\u5173\u7cfb\u4e5f\u53ef\u4ee5\u5f52\u5165\u5230nesting\u5173\u7cfb\u4e2d\u3002","title":"Example: Parent-child\u5173\u7cfb"},{"location":"Tree/Tree-structure/#_3","text":"\u8fd9\u79cd\u5173\u7cfb\u9700\u8981\u662fN:1\u7684\uff08\u8bb0\u5f97\u5728\u5927\u5b66\u7684\u6570\u636e\u5e93\u8bfe\u7a0b\u6240\u4f7f\u7528\u7684\u6559\u6750\u4e2d\u6709\u8fc7\u8fd9\u6837\u7684\u7406\u8bba\uff0c\u8fd9\u4e2a\u7406\u8bba\u5e94\u8be5\u662f\u5c5e\u4e8e Relational algebra \uff0c\u53c2\u89c1\uff1a Relational model \u3001 Database normalization \uff09\u3002 \u8fd9\u79cd\u5173\u7cfb\u5e94\u8be5\u662f Transitive relation \u3002 \u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u662f Reflexive relation \u3002 \u53ef\u4ee5\u8fd9\u6837\u4e0d\u4e25\u8c28\u5730\u8fdb\u884c\u63cf\u8ff0\uff1a \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb \u3002","title":"\u6700\u7ec8\u7b54\u6848"},{"location":"Tree/Tree-structure/#examples_of_tree_structures","text":"","title":"Examples of tree structures"},{"location":"Tree/Tree-structure/#directory_structure_directory","text":"nesting\u5173\u7cfb\u3002 See also: Tree (command) Path (computing)","title":"Directory structure (directory)"},{"location":"Tree/Tree-structure/#process_tree","text":"parent-children\u5173\u7cfb\u3002","title":"Process tree"},{"location":"Tree/Tree-structure/#file_format","text":"nesting\u5173\u7cfb\u3002 Document Object Model \uff08 XML \uff09 json yaml","title":"File format"},{"location":"Tree/Tree-structure/#namespace","text":"nesting\u5173\u7cfb\u3002 Namespace\u7684\u5e94\u7528\u573a\u666f\u5b9e\u5728\u592a\u591a\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u5b83\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002\u5728\u5bf9\u5b83\u8fdb\u884c\u601d\u8003\u7684\u65f6\u5019\uff0c\u53d1\u89c9\u4f7f\u7528namespace\u6765\u7ec4\u7ec7\u7684\u6570\u636e\u6700\u7ec8\u5c31\u662fhierarchy\u7ed3\u6784\u3002\u5176\u5b9e\u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u5c06namespace\u770b\u505a\u662f\u62ec\u53f7\u3002","title":"Namespace"},{"location":"Tree/Tree-structure/#expression","text":"binary expression tree","title":"Expression"},{"location":"Tree/Tree-structure/#source_code","text":"Parse tree \u3001 Abstract syntax tree \u3002 nesting\u5173\u7cfb\u3002","title":"Source code"},{"location":"Tree/Tree-structure/#activation_tree","text":"nesting\u5173\u7cfb\u3002 \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4e5f\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u53c2\u89c1 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u3002","title":"Activation tree"},{"location":"Tree/Tree-structure/#linguistics","text":"\u5728\u8bed\u8a00\u5b66\u4e2d\uff0c\u57fa\u672c\u4e0a\u662f\u4f7f\u7528tree\u6765\u63cf\u8ff0\u8bed\u8a00\u7684\u7ed3\u6784\u3002 If you have read book describing the compiler technology , for example the classic definitive book Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman, you will be aware how important the tree structure is to the compiler. As described in chapter 2.2.3 Parse Trees: Tree data structures figure prominently in compiling. There are some many tree in the book, such as Parse tree , abstract syntax tree , activation tree(in chapter 7.2.1 Activation Trees), expression tree . Essentially Speaking, a programming language is a formal language , what have been concluded in article structure is that Tree can be used to describe the structure of sentences that is syntax and formal grammar can be convert to tree . So it's natural to use trees in the compiling. Regular expression , algebraic expression can be described using formal grammar , so given an expression, it can be converted to an equivalent parse tree .","title":"Linguistics"},{"location":"Tree/Tree-structure/#nesting_hierarchy","text":"Nesting\u7ed3\u6784\u5177\u5907hierarchy\u7279\u6027\uff0c\u53ef\u4ee5\u8fd9\u6837\u6765\u8fdb\u884c\u89e3\u91ca\uff1a \u6700\u5916\u5c42\u662f\u662f\u7b2c\u4e00\u5c42\u3001\u5b83\u6240\u76f4\u63a5\u5305\u542b\u7684\u5143\u7d20\u90fd\u5c5e\u4e8e\u7b2c\u4e8c\u5c42\u3001\u4f9d\u6b21\u9012\u5f52\u3002","title":"Nesting \u4e0e hierarchy"},{"location":"Tree/Tree-structure/#_4","text":"Nesting \u7ed3\u6784\u5728computer science\u662f\u975e\u5e38\u5e38\u89c1\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684hierarchy\u7ed3\u6784\uff0cnesting\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u8868\u793a\uff1a ( () () ( ( ) ) ) Nesting\u7684\u4e2d\u6587\u542b\u4e49\u662f\u201c\u5d4c\u5957\u201d\uff0c\u663e\u7136\uff0c\u5b83\u80fd\u591f\u63cf\u8ff0\u5143\u7d20\u4e4b\u95f4\u7684\u5d4c\u5957\u5173\u7cfb\uff1b\u4e0a\u9762\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793anesting\u7ed3\u6784\uff0c\u56e0\u4e3a\u62ec\u53f7\u6240\u80fd\u591f\u8868\u8fbe\u7684\u201c\u5305\u542b\u201d\u5173\u7cfb\u548c\u201c\u5d4c\u5957\u201d\u5173\u7cfb\u662f\u57fa\u672c\u7c7b\u4f3c\u7684\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u4e0b\u9762\u7684 Structure and relation \u3002 \u4e0a\u8ff0\u7ed3\u6784\u662f\u53ef\u4ee5\u8868\u793a\u6210\u6811\u7684\uff0c\u5982\u4e0b\uff1a ( ) ( ) ( ) ( ) ( ) \u4f8b\u5b50\u5305\u62ec\uff1a C\u548cC++\u4e2d\uff0c\u4f7f\u7528 {} \u6765\u5b9a\u4e49block\uff0cblock\u4e2d\u53ef\u4ee5\u518d\u5305\u542bblock\uff0c\u4ece\u800c\u5f62\u6210nesting\u7ed3\u6784 \u9f99\u4e667.2.1 Activation Trees\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time. \u5373\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u4ece\u65f6\u95f4\u4e0a\u6765\u770b\u4e5f\u662f\u5d4c\u5957\u7684\u3002","title":"\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811"},{"location":"Tree/Tree-structure/#_5","text":"\u6269\u5c55\u4e00\u4e2a\u4f7f\u7528tree\u63cf\u8ff0\u7684\u5173\u7cfb\u7684\u6700\u7ec8\u76ee\u6807\u662f\u83b7\u5f97\u6240\u6709\u7684\u53f6\u5b50\u8282\u70b9\uff0c\u5b83\u7684\u57fa\u672c\u7b97\u6cd5\u662f\uff1a\u4e00\u4e2a\u8282\u70b9\uff0c\u53ea\u8981\u662fnon-terminal\u5143\u7d20\uff0c\u5c31\u9700\u8981\u5bf9\u5b83\u8fdb\u884cexpand\uff0c\u5176\u5b9e\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\uff1b \u6240\u4ee5\u5176\u5b9e\uff0c\u6211\u4e0a\u8ff0\u6240\u63cf\u8ff0\u7684\u90fd\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\u8fc7\u7a0b\uff1b \u4e0b\u9762\u662f\u4e00\u6bb5\u63cf\u8ff0\u4e0a\u8ff0 \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb \u7684\u83b7\u53d6\u6240\u6709\u7684\u53ef\u80fd\u7684\u53f6\u5b50\u8282\u70b9\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u5b83\u9700\u8981\u5c06\u6240\u6709\u7684\u5185\u8282\u70b9\u8fdb\u884c\u6269\u5c55\uff0c\u6700\u7ec8\u7684\u7ed3\u679c\u53ea\u80fd\u591f\u5305\u542b\u53f6\u5b50\u8282\u70b9\u800c\u4e0d\u80fd\u5305\u542b\u53f6\u5b50\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info] = list() to_expand_words = list(retriever_context.fen_zi_detail_dict[fen_zi_word_str]) # \u5f85\u6269\u5c55\u8bcd\u5217\u8868 while len(to_expand_words): word = to_expand_words.pop() # \u4e00\u6b21\u53ea\u5904\u7406\u4e00\u4e2a\u8bcd if word in retriever_context.fen_zi_detail_dict: # \u5f53\u524d\u8bcd\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5185\u8282\u70b9 to_expand_words.extend(retriever_context.fen_zi_detail_dict[word]) # \u6269\u5c55\u5f53\u524d\u8bcd\uff0c\u5e76\u4e14\u5c06\u5b83\u6dfb\u52a0\u5230\u5f85\u6269\u5c55\u8bcd\u5217\u8868\u4e2d else: # \u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u9875\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info].append(word) # \u5c06\u8be5\u8bcd\u8fdb\u884c\u8f93\u51fa","title":"\u63a8\u5bfc\u5173\u7cfb\u5206\u6790"},{"location":"Tree/Tree-structure/#_6","text":"\u4e00\u4e9b\u5305\u542b\u5173\u7cfb\u5177\u6709\u4f20\u9012\u6027\uff08\u56de\u53bb\u770b\u79bb\u6563\u6570\u5b66\u5bf9\u8fd9\u7684\u63cf\u8ff0\uff09\uff0c\u6bd4\u5982A\u5305\u542bB\uff0cB\u53c8\u5305\u542bC\uff0c\u5219A\u5e94\u8be5\u5305\u542b\u6240\u6709\u7684C\u3002 \u5982\u4e0b\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a \u6709\u4ef7\u8bc1\u5238:\u80a1\u7968,\u503a\u5238,\u6743\u8bc1,\u8d44\u4ea7\u652f\u6301\u8bc1\u5238,\u4e70\u5165\u8fd4\u552e\u91d1\u878d\u8d44\u4ea7 \u503a\u5238:\u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u6709\u4ef7\u8bc1\u5238 \u5305\u542b \u503a\u5238 \uff0c\u503a\u5238\u53c8\u5305\u542b \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \uff0c\u6240\u4ee5 \u6709\u4ef7\u8bc1\u5238 \u5305\u62ec \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u3002 \u4e0a\u8ff0\u5173\u7cfb\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684 \u4e0a\u8ff0\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2adict\u6765\u8fdb\u884c\u4fdd\u5b58\uff1a\u6240\u6709\u4f5c\u4e3akey\u7684\u90fd\u662fnon-terminal\uff0c\u90fd\u9700\u8981\u8fdb\u884c\u6269\u5c55\uff1b","title":"\u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb\u4f8b\u5b50"},{"location":"Tree/Tree-structure/#see_also","text":"Hierarchy Nesting (computing) Nested set model Nested set Hereditary property Software engineer\u5bf9\u8fd9\u4e24\u4e2a\u8bcd\u80af\u5b9a\u4e0d\u4f1a\u964c\u751f\uff0c\u4f46\u4e0d\u77e5\u662f\u5426\u77e5\u6653\u5b83\u4eec\u90fd\u53ef\u4ee5\u4f7f\u7528tree structure\u6765\u8fdb\u884c\u8868\u793a\uff0c\u5e0c\u671b\u5728\u9605\u8bfb\u4e86\u672c\u6587\u540e\uff0c\u8bfb\u8005\u4e0b\u6b21\u5728\u9047\u5230\u8fd9\u4e24\u4e2a\u8bcd\u7684\u65f6\u5019\u80fd\u591f\u4ea7\u751f\u8fd9\u6837\u7684\u53cd\u6620\u3002","title":"See also"},{"location":"Tree/Tree-and-stack/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u8ba8\u8bbatree \u548c stack\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002 \u8349\u7a3f1 # \u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811\uff0c\u8fd9\u662f\u6e90\u4e8e Newick format and Dyck language \uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\uff1a [[[]]] \u5bf9\u5e94\u7684tree\u5982\u4e0b\uff1a [] / / [] / / [] \u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u5df2\u7ecf\u9000\u5316\u6210\u4e86\u4e00\u4e2alist\uff0c\u8fd9\u79cd\u5c5e\u4e8e\u4f7f\u7528tree\u6765\u63cf\u8ff0\u5177\u6709hierarchy structure\u7684\u6570\u636e\u3002 \u5bf9\u4e8e Dyck language \uff0c\u5728\u5bf9\u5176\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u9700\u8981\u4f7f\u7528stack\uff0c\u5176\u5b9e\u6211\u4eec\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u662f\u6cbf\u7740\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\uff0c\u4e00\u822c\u6211\u4eec\u8fdb\u884cparsing\u65f6\u5019\uff0c\u9047\u5230\u5f00\u62ec\u53f7 [ \u662f\u8981\u7ee7\u7eed\u538b\u6808\uff0c\u5176\u5b9e\u770b\u4e0a\u56fe\u5c31\u53ef\u4ee5\u77e5\u9053\uff0c\u538b\u6808\u5bf9\u5e94\u7740\u662f\u6cbf\u7740\u6811\u8def\u5f84\u5411\u4e0b\uff0c\u5373\u4e0d\u65ad\u5730\u5411\u4e0b\u904d\u5386\u3002\u9047\u5230\u95ed\u62ec\u53f7 ] \uff0c\u5176\u5b9e\u662f\u51fa\u6808\uff0c\u5f00\u59cb\u5411\u4e0a\u4e86\u3002 \u8349\u7a3f2 # call stack and stack # SUMMARY :\u5173\u4e8e call stack \uff0c\u5728ABI\u4e2d\u5df2\u7ecf\u6536\u5f55\u4e86 \u51fd\u6570\u8c03\u7528 \u5165\u6808 \u51fd\u6570\u8fd4\u56de \u51fa\u6808 parenthesis and stack # \u6b63\u62ec\u53f7 \u5165\u6808 \u53cd\u62ec\u53f7 \u51fa\u6808 SUMMARY : stack\u7684\u5165\u6808\u4e0e\u51fa\u6808\u662f\u4e00\u5bf9\u6237\u9006\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5stack\u975e\u5e38\u9002\u5408\u4e8e\u89e3\u51b3\u54ea\u4e9b\u5b58\u5728\u7740\u4e92\u9006\u64cd\u4f5c\u7684\u95ee\u9898\uff1b \u8349\u7a3f3 # activation tree\uff0c Parse tree \uff0c\u5b83\u4eec\u90fd\u662f\u5448\u73b0\u7684tree\u7ed3\u6784\uff0c\u4f46\u662f\u51fd\u6570\u7684\u6267\u884c\u4ec5\u4ec5\u9700\u8981\u4e00\u4e2acall stack\uff0cparsing\u7684\u8fc7\u7a0b\u4e5f\u4ec5\u4ec5\u53ea\u9700\u8981\u4e00\u4e2a pushdown automata \uff08\u672c\u8d28\u4e0a\u662f\u4e00\u4e2astack\uff09\uff0c\u4e24\u8005\u5b58\u5728\u7740\u975e\u5e38\u7c7b\u4f3c\u7684\u73b0\u8c61\uff0c\u6211\u4eec\u9700\u8981\u53d6\u601d\u8003\u73b0\u8c61\u80cc\u540e\u6240\u8574\u542b\u7684\u9053\u7406\u3002\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u5177\u6709nesting\u7279\u6027\uff0c\u6240\u4ee5\u5b83\u4eec\u7684\u8fc7\u7a0b\u90fd\u5448\u73b0tree structure\u3002\u5728 4.6 Introduction to LR Parsing: Simple LR \u4e2d\u6211\u5bf9\u6b64\u6709\u8fc7\u5206\u6790\u3002 \u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"Introduction"},{"location":"Tree/Tree-and-stack/#_1","text":"\u672c\u7ae0\u8ba8\u8bbatree \u548c stack\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Tree/Tree-and-stack/#1","text":"\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811\uff0c\u8fd9\u662f\u6e90\u4e8e Newick format and Dyck language \uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\uff1a [[[]]] \u5bf9\u5e94\u7684tree\u5982\u4e0b\uff1a [] / / [] / / [] \u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u5df2\u7ecf\u9000\u5316\u6210\u4e86\u4e00\u4e2alist\uff0c\u8fd9\u79cd\u5c5e\u4e8e\u4f7f\u7528tree\u6765\u63cf\u8ff0\u5177\u6709hierarchy structure\u7684\u6570\u636e\u3002 \u5bf9\u4e8e Dyck language \uff0c\u5728\u5bf9\u5176\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u9700\u8981\u4f7f\u7528stack\uff0c\u5176\u5b9e\u6211\u4eec\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u662f\u6cbf\u7740\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\uff0c\u4e00\u822c\u6211\u4eec\u8fdb\u884cparsing\u65f6\u5019\uff0c\u9047\u5230\u5f00\u62ec\u53f7 [ \u662f\u8981\u7ee7\u7eed\u538b\u6808\uff0c\u5176\u5b9e\u770b\u4e0a\u56fe\u5c31\u53ef\u4ee5\u77e5\u9053\uff0c\u538b\u6808\u5bf9\u5e94\u7740\u662f\u6cbf\u7740\u6811\u8def\u5f84\u5411\u4e0b\uff0c\u5373\u4e0d\u65ad\u5730\u5411\u4e0b\u904d\u5386\u3002\u9047\u5230\u95ed\u62ec\u53f7 ] \uff0c\u5176\u5b9e\u662f\u51fa\u6808\uff0c\u5f00\u59cb\u5411\u4e0a\u4e86\u3002","title":"\u8349\u7a3f1"},{"location":"Tree/Tree-and-stack/#2","text":"","title":"\u8349\u7a3f2"},{"location":"Tree/Tree-and-stack/#call_stack_and_stack","text":"SUMMARY :\u5173\u4e8e call stack \uff0c\u5728ABI\u4e2d\u5df2\u7ecf\u6536\u5f55\u4e86 \u51fd\u6570\u8c03\u7528 \u5165\u6808 \u51fd\u6570\u8fd4\u56de \u51fa\u6808","title":"call stack and stack"},{"location":"Tree/Tree-and-stack/#parenthesis_and_stack","text":"\u6b63\u62ec\u53f7 \u5165\u6808 \u53cd\u62ec\u53f7 \u51fa\u6808 SUMMARY : stack\u7684\u5165\u6808\u4e0e\u51fa\u6808\u662f\u4e00\u5bf9\u6237\u9006\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5stack\u975e\u5e38\u9002\u5408\u4e8e\u89e3\u51b3\u54ea\u4e9b\u5b58\u5728\u7740\u4e92\u9006\u64cd\u4f5c\u7684\u95ee\u9898\uff1b","title":"parenthesis and stack"},{"location":"Tree/Tree-and-stack/#3","text":"activation tree\uff0c Parse tree \uff0c\u5b83\u4eec\u90fd\u662f\u5448\u73b0\u7684tree\u7ed3\u6784\uff0c\u4f46\u662f\u51fd\u6570\u7684\u6267\u884c\u4ec5\u4ec5\u9700\u8981\u4e00\u4e2acall stack\uff0cparsing\u7684\u8fc7\u7a0b\u4e5f\u4ec5\u4ec5\u53ea\u9700\u8981\u4e00\u4e2a pushdown automata \uff08\u672c\u8d28\u4e0a\u662f\u4e00\u4e2astack\uff09\uff0c\u4e24\u8005\u5b58\u5728\u7740\u975e\u5e38\u7c7b\u4f3c\u7684\u73b0\u8c61\uff0c\u6211\u4eec\u9700\u8981\u53d6\u601d\u8003\u73b0\u8c61\u80cc\u540e\u6240\u8574\u542b\u7684\u9053\u7406\u3002\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u5177\u6709nesting\u7279\u6027\uff0c\u6240\u4ee5\u5b83\u4eec\u7684\u8fc7\u7a0b\u90fd\u5448\u73b0tree structure\u3002\u5728 4.6 Introduction to LR Parsing: Simple LR \u4e2d\u6211\u5bf9\u6b64\u6709\u8fc7\u5206\u6790\u3002 \u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"\u8349\u7a3f3"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/","text":"Bracket and tree # \u672c\u6587\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002 \u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree # \u5df2\u7ecf\u770b\u5230\u4e86\u6709\u4e24\u4e2a\u9879\u76ee\u90fd\u662f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree\u7684\uff1a nltk\u7684 Noun Phrase Chunking python\u7684 Python Language Services \u4e5f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree \u603b\u7684\u6765\u8bf4\uff0c\u5728\u5b57\u7b26\u7ec8\u7aef\u4e2d\uff0c\u4f7f\u7528\u62ec\u53f7\u662f\u975e\u5e38\u9002\u5408\u6765\u8868\u793atree\u7684\u3002 Binary tree to string with brackets # Construct Binary Tree from String with bracket representation # Parentheses Are Trees # Parentheses are at the heart of programming. Understand parentheses and you can rule the earth. No, seriously! Parentheses, trees and stacks are all interconnected in a very deep and fundamental way. Back in the early days of programming, long before Google started asking why manhole covers are round, one of the very common programming aptitude tests was to take a long mess of parentheses and ask the candidate if they were valid. Something like: Is: (((((()))((()))))))((())))))) a valid expression? So parentheses are fundamental to programming and being able to handle parentheses is a fundamental programming skill. But why? The simple reason why the humble parenthesis is so great is that it forms a tree structure as soon as you give it a chance. Hence the title - Parentheses Are Trees. The fact that parentheses are trees might not seem obvious so let's see how this works. A single parenthesis is a useless thing. It only comes into its own when paired with a parenthesis of the opposite handedness. The reason that a ( goes with a ) is simply that together they form a container . Put simply you can put something in between the parenthesis - (data) Usually the data that you put in the parenthesis has some sort of structure but this isn't really relevant to the key property of parenthesis. However even at this stage you can see that parenthesis can be used to represent an array: (first) (second) (third) (fourth) What makes parenthesis really useful is that the data that you place between a pair of parenthesis can be another pair of parenthesis. This is always the moment when a simple data structure turns into a complex one - let a one dimensional array element store another array and the result is a two dimensional and more array. Letting a data structure store other data structures is one way of getting brand new things. If you allow a parenthesis pair to store another pair of parenthesis then the result is a tree. For example: (()()) corresponds to a binary tree with two terminal nodes: | /\\ (()()) You can now construct examples as complicated as you care to make them. For example: | / \\ /\\ \\ / \\ \\ / \\ \\ /\\ /\\ \\ (((()())(()()))()) The nesting of parenthesis simply gives the structure of the tree that the parenthesis correspond to. Now you can see why many computer languages use a lot of parenthesis. Probably none more so than the infamous Lisp and at this point who could resist quoting the well known xkcd cartoon: For more xkcd cartoons click here . Lisp can get away with adding very little additional machinery to the parenthesis to create a complete and powerful language. The fact that parenthesis form a tree structure also explains the strange and arbitrary rules that you had to learn in school concerning arithmetic. The rule of arithmetic expressions form a \"little\" language the grammar of which can be expressed as a very simple tree structure with the rule that you always work out the expression by doing a left to right depth first walk. Consider the expression 3+2*5. To make sense of this and evaluate it correctly we have to invoke the idea of operator precedence. In particular we have to say that multiply has a higher precedence than addition so that the expression is 2 times 5 plus 3. However if this expression had been written as ((3)+((2)*(5))) then no operator precedence rules need to be used. The parenthesised expression corresponds to the tree: + / \\ / * / / \\ ((3)+((2)*(5))) NOTE: binary expression tree can also be used to demostrate the relavance between tree and parenthesis. and you can see that walking the tree in depth first either right to left or left to right and performing the operations indicated at each of the nodes gives you the correct evaluation. Here is a depth first left-to-right evaluation: In other words if you are prepared to put all the parenthesis needed into the expression to make the syntax tree of the expression clear and unambiguous you don't need to introduce the idea of operator precedence. Of course we prefer to leave out parenthesis and complicate things by claiming that multiplication has to be done before addition. In fact we leave out so many parenthesis that we have to use the \"My Dear Aunt Sally\" rule - i.e. Multiplication and Division before Addition and Subtraction. Perhaps the biggest use of parenthesis today is in the form of all those markup languages that generate hierarchies i.e. trees of visual objects. For example what is HTML other than a bracketing syntax - <open tab>content</close tag> Just think of each open tag as a \"(\" and each closing tag as a \")\". The same is true of XAML and all the other object instantiation languages. They all create tree structures. In a more general context XML is a bracketing system that generates general tree structures consisting of arbitrary data. For another example consider the nesting of control structures such as for , if , do while and so on. These too are a bracketing language, often using curly brackets {} , and they generate a tree structure which the compiler has to work out to successfully parse the language. Once you notice the way bracketing generates hierarchies and general tree structures you start noticing them more or less everywhere. Perhaps now you will agree that parenthesis are fundamental to programming and testing the ability to work with them is probably a good way to see if some one is going to make the grade as a programmer. We have one question remaining - what arrangements of parenthesis are legal? The simple answer is only those arrangements that correspond to complete tree structures and there are only two ways in which a set of parenthesis can fail to do so. The first is just not having the same number of opening and closing parenthesis. You can't have half a container and so all valid bracketing structures have to match numbers of opening and closing parenthesis. This is a minimal condition for legal parenthesis. The second condition is that the pairs of parenthesis always occur in the right order - that is you always have () and never )(. Put as simply as this you would think that this condition is trivial but it is very easy to hide a pair of parenthesis in the wrong order. For example, ())(()() This structure is clearly wrong but there is no single answer to exactly what is wrong with it. For example you might say that it corresponds to any of the following groupings: () )( ()() () )( ( )( ) ( ))(()( ) In other words there is no single correct way to parse an incorrect structure. So how do we check for a valid bracketing structure? There is more than one answer to this but the simplest is to see if it is possible to walk the tree that the parenthesis describe. To do this you need a stack . All you have to do is scan the expression from left-to-right. Each time you encounter an opening parenthesis push it on the stack. Each time you encounter a closing parenthesis pop the top of the stack - if there is nothing on the top of the stack to pop then you have an invalid set of parenthesis. If you reach the end of the scan without trying to pop an empty stack then the stack will be empty if the expression was valid. For example in the case of: ((()())) the stack contents are: stack remainder 0 . ((()())). 1 .( (()())). 2 .(( ()())). 3 .((( )())). 4 .(( ())). 5 .((( ))). 6 .(( )). 7 .( ). 8 . . As the stack is empty when the scan is complete the expression was valid. The real question is can you see that this algorithm works? The answer is that the stack algorithm performs a depth-first left-to-right tree walk and this can only be completed if the parenthesis really do specify a tree. Notice that we are only considering parenthesis that are unlabeled - that is any closing parenthesis will pair with any opening parenthesis . Things become a little more difficult if you allow named parenthesis as in HTML, XML or program language structures. Then there are other ways to get things wrong such as <div><span></div></span> . You can easily modify the stack algorithm to detect such errors - all you have to do is make sure that the popped parenthesis matches the closing parenthesis that caused the pop operation. Parenthesis , stacks and trees go together perfectly. The final word however should go to xkcd and the observation in its blog of a halcyon time when Wikipedia had a sense of humor. The blog quotes from the Wikipedia article on parenthesis: Parentheses may also be nested (with one set (such as this) inside another set). This is not commonly used in formal writing [though sometimes other brackets (especially parentheses) will be used for one or more inner set of parentheses (in other words, secondary {or even tertiary } phrases can be found within the main sentence)] Sadly the Wikipedia entry no longer contains this paragraph... Yes parenthesises are fundamental to programming so much so that some programmers can spot a malformed structure instinctively. It is as if their eyes had a developed a co-processor for walking the tree structure. This perhaps the source for the final final word from parenthesis obsessed xkcd: ![ ( ](https://imgs.xkcd.com/comics/(.png) Only if you are a programmer... \u5b8c\u5168\u52a0\u62ec\u53f7\u95ee\u9898\u5176\u5b9e\u662f\u53ef\u4ee5\u4f7f\u7528\u6811\u6765\u8fdb\u884c\u8868\u793a\u7684 Bracket (tournament) # A bracket or tournament bracket is a tree diagram that represents the series of games played during a knockout tournament . Different knockout tournament formats have different brackets; the simplest and most common is that of the single-elimination tournament . The name \"bracket\" is American English , derived from the resemblance of the links in the tree diagram to the bracket punctuation symbol ] or [ (called a \"square bracket\" in British English ). The closest British term is draw , although this implies an element of chance, whereas some brackets are determined entirely by seeding . Abstract syntax tree and bracket # For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. Reverse Polish Notation and bracket","title":"VS-bracket-VS-tree"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#bracket_and_tree","text":"\u672c\u6587\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002","title":"Bracket and tree"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#tree","text":"\u5df2\u7ecf\u770b\u5230\u4e86\u6709\u4e24\u4e2a\u9879\u76ee\u90fd\u662f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree\u7684\uff1a nltk\u7684 Noun Phrase Chunking python\u7684 Python Language Services \u4e5f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree \u603b\u7684\u6765\u8bf4\uff0c\u5728\u5b57\u7b26\u7ec8\u7aef\u4e2d\uff0c\u4f7f\u7528\u62ec\u53f7\u662f\u975e\u5e38\u9002\u5408\u6765\u8868\u793atree\u7684\u3002","title":"\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#binary_tree_to_string_with_brackets","text":"","title":"Binary tree to string with brackets"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#construct_binary_tree_from_string_with_bracket_representation","text":"","title":"Construct Binary Tree from String with bracket representation"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#parentheses_are_trees","text":"Parentheses are at the heart of programming. Understand parentheses and you can rule the earth. No, seriously! Parentheses, trees and stacks are all interconnected in a very deep and fundamental way. Back in the early days of programming, long before Google started asking why manhole covers are round, one of the very common programming aptitude tests was to take a long mess of parentheses and ask the candidate if they were valid. Something like: Is: (((((()))((()))))))((())))))) a valid expression? So parentheses are fundamental to programming and being able to handle parentheses is a fundamental programming skill. But why? The simple reason why the humble parenthesis is so great is that it forms a tree structure as soon as you give it a chance. Hence the title - Parentheses Are Trees. The fact that parentheses are trees might not seem obvious so let's see how this works. A single parenthesis is a useless thing. It only comes into its own when paired with a parenthesis of the opposite handedness. The reason that a ( goes with a ) is simply that together they form a container . Put simply you can put something in between the parenthesis - (data) Usually the data that you put in the parenthesis has some sort of structure but this isn't really relevant to the key property of parenthesis. However even at this stage you can see that parenthesis can be used to represent an array: (first) (second) (third) (fourth) What makes parenthesis really useful is that the data that you place between a pair of parenthesis can be another pair of parenthesis. This is always the moment when a simple data structure turns into a complex one - let a one dimensional array element store another array and the result is a two dimensional and more array. Letting a data structure store other data structures is one way of getting brand new things. If you allow a parenthesis pair to store another pair of parenthesis then the result is a tree. For example: (()()) corresponds to a binary tree with two terminal nodes: | /\\ (()()) You can now construct examples as complicated as you care to make them. For example: | / \\ /\\ \\ / \\ \\ / \\ \\ /\\ /\\ \\ (((()())(()()))()) The nesting of parenthesis simply gives the structure of the tree that the parenthesis correspond to. Now you can see why many computer languages use a lot of parenthesis. Probably none more so than the infamous Lisp and at this point who could resist quoting the well known xkcd cartoon: For more xkcd cartoons click here . Lisp can get away with adding very little additional machinery to the parenthesis to create a complete and powerful language. The fact that parenthesis form a tree structure also explains the strange and arbitrary rules that you had to learn in school concerning arithmetic. The rule of arithmetic expressions form a \"little\" language the grammar of which can be expressed as a very simple tree structure with the rule that you always work out the expression by doing a left to right depth first walk. Consider the expression 3+2*5. To make sense of this and evaluate it correctly we have to invoke the idea of operator precedence. In particular we have to say that multiply has a higher precedence than addition so that the expression is 2 times 5 plus 3. However if this expression had been written as ((3)+((2)*(5))) then no operator precedence rules need to be used. The parenthesised expression corresponds to the tree: + / \\ / * / / \\ ((3)+((2)*(5))) NOTE: binary expression tree can also be used to demostrate the relavance between tree and parenthesis. and you can see that walking the tree in depth first either right to left or left to right and performing the operations indicated at each of the nodes gives you the correct evaluation. Here is a depth first left-to-right evaluation: In other words if you are prepared to put all the parenthesis needed into the expression to make the syntax tree of the expression clear and unambiguous you don't need to introduce the idea of operator precedence. Of course we prefer to leave out parenthesis and complicate things by claiming that multiplication has to be done before addition. In fact we leave out so many parenthesis that we have to use the \"My Dear Aunt Sally\" rule - i.e. Multiplication and Division before Addition and Subtraction. Perhaps the biggest use of parenthesis today is in the form of all those markup languages that generate hierarchies i.e. trees of visual objects. For example what is HTML other than a bracketing syntax - <open tab>content</close tag> Just think of each open tag as a \"(\" and each closing tag as a \")\". The same is true of XAML and all the other object instantiation languages. They all create tree structures. In a more general context XML is a bracketing system that generates general tree structures consisting of arbitrary data. For another example consider the nesting of control structures such as for , if , do while and so on. These too are a bracketing language, often using curly brackets {} , and they generate a tree structure which the compiler has to work out to successfully parse the language. Once you notice the way bracketing generates hierarchies and general tree structures you start noticing them more or less everywhere. Perhaps now you will agree that parenthesis are fundamental to programming and testing the ability to work with them is probably a good way to see if some one is going to make the grade as a programmer. We have one question remaining - what arrangements of parenthesis are legal? The simple answer is only those arrangements that correspond to complete tree structures and there are only two ways in which a set of parenthesis can fail to do so. The first is just not having the same number of opening and closing parenthesis. You can't have half a container and so all valid bracketing structures have to match numbers of opening and closing parenthesis. This is a minimal condition for legal parenthesis. The second condition is that the pairs of parenthesis always occur in the right order - that is you always have () and never )(. Put as simply as this you would think that this condition is trivial but it is very easy to hide a pair of parenthesis in the wrong order. For example, ())(()() This structure is clearly wrong but there is no single answer to exactly what is wrong with it. For example you might say that it corresponds to any of the following groupings: () )( ()() () )( ( )( ) ( ))(()( ) In other words there is no single correct way to parse an incorrect structure. So how do we check for a valid bracketing structure? There is more than one answer to this but the simplest is to see if it is possible to walk the tree that the parenthesis describe. To do this you need a stack . All you have to do is scan the expression from left-to-right. Each time you encounter an opening parenthesis push it on the stack. Each time you encounter a closing parenthesis pop the top of the stack - if there is nothing on the top of the stack to pop then you have an invalid set of parenthesis. If you reach the end of the scan without trying to pop an empty stack then the stack will be empty if the expression was valid. For example in the case of: ((()())) the stack contents are: stack remainder 0 . ((()())). 1 .( (()())). 2 .(( ()())). 3 .((( )())). 4 .(( ())). 5 .((( ))). 6 .(( )). 7 .( ). 8 . . As the stack is empty when the scan is complete the expression was valid. The real question is can you see that this algorithm works? The answer is that the stack algorithm performs a depth-first left-to-right tree walk and this can only be completed if the parenthesis really do specify a tree. Notice that we are only considering parenthesis that are unlabeled - that is any closing parenthesis will pair with any opening parenthesis . Things become a little more difficult if you allow named parenthesis as in HTML, XML or program language structures. Then there are other ways to get things wrong such as <div><span></div></span> . You can easily modify the stack algorithm to detect such errors - all you have to do is make sure that the popped parenthesis matches the closing parenthesis that caused the pop operation. Parenthesis , stacks and trees go together perfectly. The final word however should go to xkcd and the observation in its blog of a halcyon time when Wikipedia had a sense of humor. The blog quotes from the Wikipedia article on parenthesis: Parentheses may also be nested (with one set (such as this) inside another set). This is not commonly used in formal writing [though sometimes other brackets (especially parentheses) will be used for one or more inner set of parentheses (in other words, secondary {or even tertiary } phrases can be found within the main sentence)] Sadly the Wikipedia entry no longer contains this paragraph... Yes parenthesises are fundamental to programming so much so that some programmers can spot a malformed structure instinctively. It is as if their eyes had a developed a co-processor for walking the tree structure. This perhaps the source for the final final word from parenthesis obsessed xkcd: ![ ( ](https://imgs.xkcd.com/comics/(.png) Only if you are a programmer... \u5b8c\u5168\u52a0\u62ec\u53f7\u95ee\u9898\u5176\u5b9e\u662f\u53ef\u4ee5\u4f7f\u7528\u6811\u6765\u8fdb\u884c\u8868\u793a\u7684","title":"Parentheses Are Trees"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#bracket_tournament","text":"A bracket or tournament bracket is a tree diagram that represents the series of games played during a knockout tournament . Different knockout tournament formats have different brackets; the simplest and most common is that of the single-elimination tournament . The name \"bracket\" is American English , derived from the resemblance of the links in the tree diagram to the bracket punctuation symbol ] or [ (called a \"square bracket\" in British English ). The closest British term is draw , although this implies an element of chance, whereas some brackets are determined entirely by seeding .","title":"Bracket (tournament)"},{"location":"Tree/Tree-and-stack/VS-bracket-VS-tree/#abstract_syntax_tree_and_bracket","text":"For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. Reverse Polish Notation and bracket","title":"Abstract syntax tree and bracket"},{"location":"Tree/Tree-operation/","text":"TODO","title":"Introduction"},{"location":"Tree/Trie/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u63cf\u8ff0trie\u4ee5\u53ca\u5b83\u7684\u5404\u79cd\u5e94\u7528\uff0c\u53d8\u4f53\u7b49\u3002","title":"Introduction"},{"location":"Tree/Trie/#_1","text":"\u672c\u7ae0\u63cf\u8ff0trie\u4ee5\u53ca\u5b83\u7684\u5404\u79cd\u5e94\u7528\uff0c\u53d8\u4f53\u7b49\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Tree/Trie/Trie/","text":"Trie # NOTE: trie\u4e5f\u53ef\u4ee5\u79f0\u4e4b\u4e3a\u524d\u7f00\u6811 In computer science , a trie , also called digital tree , radix tree or prefix tree , is a kind of search tree \u2014an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings . Unlike a binary search tree , no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string . Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node. For the space-optimized presentation of prefix tree, see compact prefix tree . In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a tree-shaped deterministic finite automaton \uff08\u5373DFA\uff0c\u786e\u5b9a\u6709\u7a77\u81ea\u52a8\u673a\uff09. Each finite language is generated by a trie automaton , and each trie can be compressed into a deterministic acyclic finite state automaton . SUMMARY : \u5173\u4e8efinite automata\u7684\u5185\u5bb9\u53c2\u89c1\u300aAutomata theory-formal languages and formal grammars\u300b\u76ee\u5f55\uff0c\u5176\u4e2d\u6536\u5f55\u5173\u4e8efinite automata\u7684\u77e5\u8bc6\u3002 SUMMARY : \u5176\u5b9e\u611f\u89c9trie\u66f4\u52a0\u7c7b\u4f3c\u4e8egraph\uff0c\u56e0\u4e3atrie\u7684edge\u6709label\uff0c\u7c7b\u4f3c\u4e8eweighted graph\uff0c\u6240\u4ee5\u5b83\u7684\u5b9e\u73b0\u9664\u4e86\u9700\u8981\u4fdd\u5b58\u8282\u70b9\u4e4b\u95f4\u7684edge\uff0c\u8fd8\u9700\u8981\u4fdd\u5b58\u6bcf\u6761edge\u7684label\uff0c\u5176\u5b9e\u8fd9\u5c31\u975e\u5e38\u7c7b\u4f3cfinite automata\uff1b Though tries can be keyed by character strings, they need not be. The same algorithms can be adapted to serve similar functions on ordered lists of any construct, e.g. permutations on a list of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up any fixed-length binary datum, such as an integer or memory address. A trie for keys \"A\", \"to\", \"tea\", \"ted\", \"ten\", \"i\", \"in\", and \"inn\". Note that this example does not have all the children alphabetically sorted from left to right as it should be (the root and node 't'). Applications # As a replacement for other data structures # As discussed below, a trie has a number of advantages over binary search trees. A trie can also be used to replace a hash table , over which it has the following advantages: Looking up data in a trie is faster in the worst case, O(m) time (where m is the length of a search string), compared to an imperfect hash table. An imperfect hash table can have key collisions. A key collision is the hash function mapping of different keys to the same position in a hash table. The worst-case lookup speed in an imperfect hash table is O(N) time, but far more typically is O(1), with O(m) time spent evaluating the hash. There are no collisions of different keys in a trie. Buckets in a trie, which are analogous to hash table buckets that store key collisions, are necessary only if a single key is associated with more than one value. There is no need to provide a hash function or to change hash functions as more keys are added to a trie. A trie can provide an alphabetical ordering of the entries by key. However, a trie also has some drawbacks compared to a hash table: Trie lookup can be slower than hash table lookup, especially if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to main memory.[ 7] Some keys, such as floating point numbers, can lead to long chains and prefixes that are not particularly meaningful. Nevertheless, a bitwise trie can handle standard IEEE single and double format floating point numbers.[ citation needed ] Some tries can require more space than a hash table, as memory may be allocated for each character in the search string, rather than a single chunk of memory for the whole entry, as in most hash tables. Dictionary representation # A common application of a trie is storing a predictive text or autocomplete dictionary, such as found on a mobile telephone . Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries; however, if storing dictionary words is all that is required (i.e., storage of information auxiliary to each word is not required), a minimal deterministic acyclic finite state automaton (DAFSA) would use less space than a trie. This is because a DAFSA can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored. Tries are also well suited for implementing approximate matching algorithms,[ 8] including those used in spell checking and hyphenation [ 4] software. Term indexing # A discrimination tree term index stores its information in a trie data structure.[ 9] Algorithms # The trie is a tree of nodes which supports Find and Insert operations. Find returns the value for a key string, and Insert inserts a string (the key) and a value into the trie. Both Insert and Find run in O( n ) time, where n is the length of the key. A simple Node class can be used to represent nodes in the trie: class Node(): def __init__(self): # Note that using dictionary for children (as in this implementation) would not allow lexicographic sorting mentioned in the next section (Sorting), # because ordinary dictionary would not preserve the order of the keys self.children : Dict[str, Node] = {} # mapping from character ==> Node self.value : Any = None Note that children is a dictionary of characters to a node's children; and it is said that a \"terminal\" node is one which represents a complete string. A trie's value can be looked up as follows: def find(node: Node, key: str) -> Any: for char in key: if char in node.children: node = node.children[char] else: return None return node.value Insertion proceeds by walking the trie according to the string to be inserted, then appending new nodes for the suffix of the string that is not contained in the trie: def insert(node: Node, key: str, value: Any) -> None: for char in key: if char not in node.children: node.children[char] = Node() node = node.children[char] node.value = value Sorting # Lexicographic sorting of a set of keys can be accomplished by building a trie from them, and traversing it in pre-order , printing only the leaves' values. This algorithm is a form of radix sort .[ 10] A trie forms the fundamental data structure of Burstsort , which (in 2007) was the fastest known string sorting algorithm.[ 11] However, now there are faster string sorting algorithms.[ 12] Full text search # A special kind of trie, called a suffix tree , can be used to index all suffixes in a text in order to carry out fast full text searches. SUMMARY : suffix tree \u7684\u786e\u5f88\u5f3a\u5927\uff1b Implementation strategies # There are several ways to represent tries, corresponding to different trade-offs between memory use and speed of the operations. The basic form is that of a linked set of nodes, where each node contains an array of child pointers, one for each symbol in the alphabet (so for the English alphabet , one would store 26 child pointers and for the alphabet of bytes, 256 pointers). This is simple but wasteful in terms of memory: using the alphabet of bytes (size 256) and four-byte pointers, each node requires a kilobyte of storage, and when there is little overlap in the strings' prefixes, the number of required nodes is roughly the combined length of the stored strings.[ 2] :341 Put another way, the nodes near the bottom of the tree tend to have few children and there are many of them, so the structure wastes space storing null pointers.[ 13] The storage problem can be alleviated by an implementation technique called alphabet reduction , whereby the original strings are reinterpreted as longer strings over a smaller alphabet. E.g., a string of n bytes can alternatively be regarded as a string of 2 n four-bit units and stored in a trie with sixteen pointers per node. Lookups need to visit twice as many nodes in the worst case, but the storage requirements go down by a factor of eight.[ 2] :347\u2013352 An alternative implementation represents a node as a triple (symbol, child, next) and links the children of a node together as a singly linked list : child points to the node's first child, next to the parent node's next child.[ 13] [ 14] The set of children can also be represented as a binary search tree ; one instance of this idea is the ternary search tree developed by Bentley and Sedgewick .[ 2] :353 Another alternative in order to avoid the use of an array of 256 pointers (ASCII), as suggested before, is to store the alphabet array as a bitmap of 256 bits representing the ASCII alphabet, reducing dramatically the size of the nodes.[ 15] Bitwise tries # Bitwise tries are much the same as a normal character-based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g., GCC's __builtin_clz() intrinsic). This value is then used to index a 32- or 64-entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing child[0] or child[1] appropriately until the item is found. Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact has excellent performance on modern out-of-order execution CPUs. A red-black tree for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and TLB stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory, and when it does, it does so only to read, thus avoiding SMP cache coherency overhead. Hence, it is increasingly becoming the algorithm of choice for code that performs many rapid insertions and deletions, such as memory allocators (e.g., recent versions of the famous Doug Lea's allocator (dlmalloc) and its descendants ). Compressing tries # Compressing the trie and merging the common branches can sometimes yield large performance gains. This works best under the following conditions: The trie is mostly static (key insertions to or deletions from a pre-filled trie are disabled).[ citation needed ] Only lookups are needed. The trie nodes are not keyed by node-specific data, or the nodes' data are common.[ 16] The total set of stored keys is very sparse within their representation space.[ citation needed ] For example, it may be used to represent sparse bitsets , i.e., subsets of a much larger, fixed enumerable set. In such a case, the trie is keyed by the bit element position within the full set. The key is created from the string of bits needed to encode the integral position of each element. Such tries have a very degenerate form with many missing branches. After detecting the repetition of common patterns or filling the unused gaps, the unique leaf nodes (bit strings) can be stored and compressed easily, reducing the overall size of the trie. Such compression is also used in the implementation of the various fast lookup tables for retrieving Unicode character properties. These could include case-mapping tables (e.g. for the Greek letter pi , from \u03a0 to \u03c0), or lookup tables normalizing the combination of base and combining characters (like the a- umlaut in German , \u00e4, or the dalet - patah - dagesh - ole in Biblical Hebrew , \u05d3\u05b7\u05bc\u05ab). For such applications, the representation is similar to transforming a very large, unidimensional, sparse table (e.g. Unicode code points) into a multidimensional matrix of their combinations, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie to represent the resulting character. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key. For example, to avoid storing the full, multibyte Unicode code point of each element forming a matrix column, the groupings of similar code points can be exploited. Each dimension of the hyper-matrix stores the start position of the next dimension, so that only the offset (typically a single byte) need be stored. The resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) can be compressed separately. Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually has a significant cost when compressed segments need to be split or merged. Some tradeoff has to be made between data compression and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.[ citation needed ] The result of such compression may look similar to trying to transform the trie into a directed acyclic graph (DAG), because the reverse transform from a DAG to a trie is obvious and always possible. However, the shape of the DAG is determined by the form of the key chosen to index the nodes, in turn constraining the compression possible. Another compression strategy is to \"unravel\" the data structure into a single byte array.[ 17] This approach eliminates the need for node pointers, substantially reducing the memory requirements. This in turn permits memory mapping and the use of virtual memory to efficiently load the data from disk. One more approach is to \"pack\" the trie.[ 4] Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation , in which the descendants of each node may be interleaved in memory.","title":"Trie"},{"location":"Tree/Trie/Trie/#trie","text":"NOTE: trie\u4e5f\u53ef\u4ee5\u79f0\u4e4b\u4e3a\u524d\u7f00\u6811 In computer science , a trie , also called digital tree , radix tree or prefix tree , is a kind of search tree \u2014an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings . Unlike a binary search tree , no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string . Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node. For the space-optimized presentation of prefix tree, see compact prefix tree . In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a tree-shaped deterministic finite automaton \uff08\u5373DFA\uff0c\u786e\u5b9a\u6709\u7a77\u81ea\u52a8\u673a\uff09. Each finite language is generated by a trie automaton , and each trie can be compressed into a deterministic acyclic finite state automaton . SUMMARY : \u5173\u4e8efinite automata\u7684\u5185\u5bb9\u53c2\u89c1\u300aAutomata theory-formal languages and formal grammars\u300b\u76ee\u5f55\uff0c\u5176\u4e2d\u6536\u5f55\u5173\u4e8efinite automata\u7684\u77e5\u8bc6\u3002 SUMMARY : \u5176\u5b9e\u611f\u89c9trie\u66f4\u52a0\u7c7b\u4f3c\u4e8egraph\uff0c\u56e0\u4e3atrie\u7684edge\u6709label\uff0c\u7c7b\u4f3c\u4e8eweighted graph\uff0c\u6240\u4ee5\u5b83\u7684\u5b9e\u73b0\u9664\u4e86\u9700\u8981\u4fdd\u5b58\u8282\u70b9\u4e4b\u95f4\u7684edge\uff0c\u8fd8\u9700\u8981\u4fdd\u5b58\u6bcf\u6761edge\u7684label\uff0c\u5176\u5b9e\u8fd9\u5c31\u975e\u5e38\u7c7b\u4f3cfinite automata\uff1b Though tries can be keyed by character strings, they need not be. The same algorithms can be adapted to serve similar functions on ordered lists of any construct, e.g. permutations on a list of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up any fixed-length binary datum, such as an integer or memory address. A trie for keys \"A\", \"to\", \"tea\", \"ted\", \"ten\", \"i\", \"in\", and \"inn\". Note that this example does not have all the children alphabetically sorted from left to right as it should be (the root and node 't').","title":"Trie"},{"location":"Tree/Trie/Trie/#applications","text":"","title":"Applications"},{"location":"Tree/Trie/Trie/#as_a_replacement_for_other_data_structures","text":"As discussed below, a trie has a number of advantages over binary search trees. A trie can also be used to replace a hash table , over which it has the following advantages: Looking up data in a trie is faster in the worst case, O(m) time (where m is the length of a search string), compared to an imperfect hash table. An imperfect hash table can have key collisions. A key collision is the hash function mapping of different keys to the same position in a hash table. The worst-case lookup speed in an imperfect hash table is O(N) time, but far more typically is O(1), with O(m) time spent evaluating the hash. There are no collisions of different keys in a trie. Buckets in a trie, which are analogous to hash table buckets that store key collisions, are necessary only if a single key is associated with more than one value. There is no need to provide a hash function or to change hash functions as more keys are added to a trie. A trie can provide an alphabetical ordering of the entries by key. However, a trie also has some drawbacks compared to a hash table: Trie lookup can be slower than hash table lookup, especially if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to main memory.[ 7] Some keys, such as floating point numbers, can lead to long chains and prefixes that are not particularly meaningful. Nevertheless, a bitwise trie can handle standard IEEE single and double format floating point numbers.[ citation needed ] Some tries can require more space than a hash table, as memory may be allocated for each character in the search string, rather than a single chunk of memory for the whole entry, as in most hash tables.","title":"As a replacement for other data structures"},{"location":"Tree/Trie/Trie/#dictionary_representation","text":"A common application of a trie is storing a predictive text or autocomplete dictionary, such as found on a mobile telephone . Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries; however, if storing dictionary words is all that is required (i.e., storage of information auxiliary to each word is not required), a minimal deterministic acyclic finite state automaton (DAFSA) would use less space than a trie. This is because a DAFSA can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored. Tries are also well suited for implementing approximate matching algorithms,[ 8] including those used in spell checking and hyphenation [ 4] software.","title":"Dictionary representation"},{"location":"Tree/Trie/Trie/#term_indexing","text":"A discrimination tree term index stores its information in a trie data structure.[ 9]","title":"Term indexing"},{"location":"Tree/Trie/Trie/#algorithms","text":"The trie is a tree of nodes which supports Find and Insert operations. Find returns the value for a key string, and Insert inserts a string (the key) and a value into the trie. Both Insert and Find run in O( n ) time, where n is the length of the key. A simple Node class can be used to represent nodes in the trie: class Node(): def __init__(self): # Note that using dictionary for children (as in this implementation) would not allow lexicographic sorting mentioned in the next section (Sorting), # because ordinary dictionary would not preserve the order of the keys self.children : Dict[str, Node] = {} # mapping from character ==> Node self.value : Any = None Note that children is a dictionary of characters to a node's children; and it is said that a \"terminal\" node is one which represents a complete string. A trie's value can be looked up as follows: def find(node: Node, key: str) -> Any: for char in key: if char in node.children: node = node.children[char] else: return None return node.value Insertion proceeds by walking the trie according to the string to be inserted, then appending new nodes for the suffix of the string that is not contained in the trie: def insert(node: Node, key: str, value: Any) -> None: for char in key: if char not in node.children: node.children[char] = Node() node = node.children[char] node.value = value","title":"Algorithms"},{"location":"Tree/Trie/Trie/#sorting","text":"Lexicographic sorting of a set of keys can be accomplished by building a trie from them, and traversing it in pre-order , printing only the leaves' values. This algorithm is a form of radix sort .[ 10] A trie forms the fundamental data structure of Burstsort , which (in 2007) was the fastest known string sorting algorithm.[ 11] However, now there are faster string sorting algorithms.[ 12]","title":"Sorting"},{"location":"Tree/Trie/Trie/#full_text_search","text":"A special kind of trie, called a suffix tree , can be used to index all suffixes in a text in order to carry out fast full text searches. SUMMARY : suffix tree \u7684\u786e\u5f88\u5f3a\u5927\uff1b","title":"Full text search"},{"location":"Tree/Trie/Trie/#implementation_strategies","text":"There are several ways to represent tries, corresponding to different trade-offs between memory use and speed of the operations. The basic form is that of a linked set of nodes, where each node contains an array of child pointers, one for each symbol in the alphabet (so for the English alphabet , one would store 26 child pointers and for the alphabet of bytes, 256 pointers). This is simple but wasteful in terms of memory: using the alphabet of bytes (size 256) and four-byte pointers, each node requires a kilobyte of storage, and when there is little overlap in the strings' prefixes, the number of required nodes is roughly the combined length of the stored strings.[ 2] :341 Put another way, the nodes near the bottom of the tree tend to have few children and there are many of them, so the structure wastes space storing null pointers.[ 13] The storage problem can be alleviated by an implementation technique called alphabet reduction , whereby the original strings are reinterpreted as longer strings over a smaller alphabet. E.g., a string of n bytes can alternatively be regarded as a string of 2 n four-bit units and stored in a trie with sixteen pointers per node. Lookups need to visit twice as many nodes in the worst case, but the storage requirements go down by a factor of eight.[ 2] :347\u2013352 An alternative implementation represents a node as a triple (symbol, child, next) and links the children of a node together as a singly linked list : child points to the node's first child, next to the parent node's next child.[ 13] [ 14] The set of children can also be represented as a binary search tree ; one instance of this idea is the ternary search tree developed by Bentley and Sedgewick .[ 2] :353 Another alternative in order to avoid the use of an array of 256 pointers (ASCII), as suggested before, is to store the alphabet array as a bitmap of 256 bits representing the ASCII alphabet, reducing dramatically the size of the nodes.[ 15]","title":"Implementation strategies"},{"location":"Tree/Trie/Trie/#bitwise_tries","text":"Bitwise tries are much the same as a normal character-based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g., GCC's __builtin_clz() intrinsic). This value is then used to index a 32- or 64-entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing child[0] or child[1] appropriately until the item is found. Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact has excellent performance on modern out-of-order execution CPUs. A red-black tree for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and TLB stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory, and when it does, it does so only to read, thus avoiding SMP cache coherency overhead. Hence, it is increasingly becoming the algorithm of choice for code that performs many rapid insertions and deletions, such as memory allocators (e.g., recent versions of the famous Doug Lea's allocator (dlmalloc) and its descendants ).","title":"Bitwise tries"},{"location":"Tree/Trie/Trie/#compressing_tries","text":"Compressing the trie and merging the common branches can sometimes yield large performance gains. This works best under the following conditions: The trie is mostly static (key insertions to or deletions from a pre-filled trie are disabled).[ citation needed ] Only lookups are needed. The trie nodes are not keyed by node-specific data, or the nodes' data are common.[ 16] The total set of stored keys is very sparse within their representation space.[ citation needed ] For example, it may be used to represent sparse bitsets , i.e., subsets of a much larger, fixed enumerable set. In such a case, the trie is keyed by the bit element position within the full set. The key is created from the string of bits needed to encode the integral position of each element. Such tries have a very degenerate form with many missing branches. After detecting the repetition of common patterns or filling the unused gaps, the unique leaf nodes (bit strings) can be stored and compressed easily, reducing the overall size of the trie. Such compression is also used in the implementation of the various fast lookup tables for retrieving Unicode character properties. These could include case-mapping tables (e.g. for the Greek letter pi , from \u03a0 to \u03c0), or lookup tables normalizing the combination of base and combining characters (like the a- umlaut in German , \u00e4, or the dalet - patah - dagesh - ole in Biblical Hebrew , \u05d3\u05b7\u05bc\u05ab). For such applications, the representation is similar to transforming a very large, unidimensional, sparse table (e.g. Unicode code points) into a multidimensional matrix of their combinations, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie to represent the resulting character. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key. For example, to avoid storing the full, multibyte Unicode code point of each element forming a matrix column, the groupings of similar code points can be exploited. Each dimension of the hyper-matrix stores the start position of the next dimension, so that only the offset (typically a single byte) need be stored. The resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) can be compressed separately. Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually has a significant cost when compressed segments need to be split or merged. Some tradeoff has to be made between data compression and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.[ citation needed ] The result of such compression may look similar to trying to transform the trie into a directed acyclic graph (DAG), because the reverse transform from a DAG to a trie is obvious and always possible. However, the shape of the DAG is determined by the form of the key chosen to index the nodes, in turn constraining the compression possible. Another compression strategy is to \"unravel\" the data structure into a single byte array.[ 17] This approach eliminates the need for node pointers, substantially reducing the memory requirements. This in turn permits memory mapping and the use of virtual memory to efficiently load the data from disk. One more approach is to \"pack\" the trie.[ 4] Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation , in which the descendants of each node may be interleaved in memory.","title":"Compressing tries"},{"location":"Tree/Trie/geeksforgeeks-trie/","text":"Trie | (Insert and Search) # Trie is an efficient information re Trie val data structure. Using Trie, search complexities can be brought to optimal limit (key length). If we store keys in binary search tree, a well balanced BST will need time proportional to M * log N , where M is maximum string length and N is number of keys in tree. Using Trie, we can search the key in O(M) time. However the penalty is on Trie storage requirements (Please refer Applications of Trie for more details) Every node of Trie consists of multiple branches. Each branch represents a possible character of keys. We need to mark the last node of every key as end of word node. A Trie node field isEndOfWord is used to distinguish the node as end of word node. A simple structure to represent nodes of the English alphabet can be as following, // Trie node struct TrieNode { struct TrieNode *children[ALPHABET_SIZE]; // isEndOfWord is true if the node // represents end of a word bool isEndOfWord; }; Inserting # Inserting a key into Trie is a simple approach. Every character of the input key is inserted as an individual Trie node. Note that the children is an array of pointers (or references) to next level trie nodes. The key character acts as an index into the array children . If the input key is new or an extension of the existing key, we need to construct non-existing nodes of the key, and mark end of the word for the last node. If the input key is a prefix of the existing key in Trie, we simply mark the last node of the key as the end of a word. The key length determines Trie depth. Searching # Searching for a key is similar to insert operation, however, we only compare the characters and move down. The search can terminate due to the end of a string or lack of key in the trie. In the former case, if the isEndofWord field of the last node is true, then the key exists in the trie. In the second case, the search terminates without examining all the characters of the key, since the key is not present in the trie. The following picture explains construction of trie using keys given in the example below, root / \\ \\ t a b | | | h n y | | \\ | e s y e / | | i r w | | | r e e | r In the picture, every character is of type trie_node_t . For example, the root is of type trie_node_t , and it\u2019s children a , b and t are filled, all other nodes of root will be NULL. Similarly, \u201ca\u201d at the next level is having only one child (\u201cn\u201d), all other children are NULL. The leaf nodes are in blue. Insert and search costs O(key_length) , however the memory requirements of Trie is O(ALPHABET_SIZE * key_length * N) where N is number of keys in Trie. There are efficient representation of trie nodes (e.g. compressed trie, ternary search tree , etc.) to minimize memory requirements of trie.","title":"[Trie | (Insert and Search)](https://www.geeksforgeeks.org/tag/trie/)"},{"location":"Tree/Trie/geeksforgeeks-trie/#trie_insert_and_search","text":"Trie is an efficient information re Trie val data structure. Using Trie, search complexities can be brought to optimal limit (key length). If we store keys in binary search tree, a well balanced BST will need time proportional to M * log N , where M is maximum string length and N is number of keys in tree. Using Trie, we can search the key in O(M) time. However the penalty is on Trie storage requirements (Please refer Applications of Trie for more details) Every node of Trie consists of multiple branches. Each branch represents a possible character of keys. We need to mark the last node of every key as end of word node. A Trie node field isEndOfWord is used to distinguish the node as end of word node. A simple structure to represent nodes of the English alphabet can be as following, // Trie node struct TrieNode { struct TrieNode *children[ALPHABET_SIZE]; // isEndOfWord is true if the node // represents end of a word bool isEndOfWord; };","title":"Trie | (Insert and Search)"},{"location":"Tree/Trie/geeksforgeeks-trie/#inserting","text":"Inserting a key into Trie is a simple approach. Every character of the input key is inserted as an individual Trie node. Note that the children is an array of pointers (or references) to next level trie nodes. The key character acts as an index into the array children . If the input key is new or an extension of the existing key, we need to construct non-existing nodes of the key, and mark end of the word for the last node. If the input key is a prefix of the existing key in Trie, we simply mark the last node of the key as the end of a word. The key length determines Trie depth.","title":"Inserting"},{"location":"Tree/Trie/geeksforgeeks-trie/#searching","text":"Searching for a key is similar to insert operation, however, we only compare the characters and move down. The search can terminate due to the end of a string or lack of key in the trie. In the former case, if the isEndofWord field of the last node is true, then the key exists in the trie. In the second case, the search terminates without examining all the characters of the key, since the key is not present in the trie. The following picture explains construction of trie using keys given in the example below, root / \\ \\ t a b | | | h n y | | \\ | e s y e / | | i r w | | | r e e | r In the picture, every character is of type trie_node_t . For example, the root is of type trie_node_t , and it\u2019s children a , b and t are filled, all other nodes of root will be NULL. Similarly, \u201ca\u201d at the next level is having only one child (\u201cn\u201d), all other children are NULL. The leaf nodes are in blue. Insert and search costs O(key_length) , however the memory requirements of Trie is O(ALPHABET_SIZE * key_length * N) where N is number of keys in Trie. There are efficient representation of trie nodes (e.g. compressed trie, ternary search tree , etc.) to minimize memory requirements of trie.","title":"Searching"}]}